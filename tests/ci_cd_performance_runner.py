#!/usr/bin/env python3
"""
Task 22.3: CI/CD Performance Testing Runner

Simplified CI/CD integration for performance testing framework.
Demonstrates how to integrate performance monitoring into CI/CD pipelines.
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from pathlib import Path

# Add the tests directory to the path for imports
sys.path.append(os.path.dirname(__file__))
from test_task_22_3_performance_testing_framework import (
    PerformanceBenchmarkSuite,
    ContinuousPerformanceMonitor,
    ScreenshotComplexity
)


class CICDPerformanceRunner:
    """CI/CD Performance Testing Runner"""
    
    def __init__(self, output_dir: str = "./performance_reports"):
        self.output_dir = output_dir
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        self.benchmark_suite = PerformanceBenchmarkSuite(output_dir=self.output_dir)
        self.monitor = ContinuousPerformanceMonitor()
    
    async def run_performance_tests(self) -> dict:
        """Execute CI/CD performance test suite"""
        print("🚀 Starting CI/CD Performance Tests...")
        
        all_results = []
        
        # Run browser initialization benchmarks
        print("📊 Testing browser initialization...")
        browser_results = await self.benchmark_suite.benchmark_browser_initialization(iterations=3)
        all_results.extend(browser_results)
        print(f"   ✅ Completed {len(browser_results)} browser tests")
        
        # Run screenshot capture benchmarks
        print("📊 Testing screenshot capture...")
        capture_results = await self.benchmark_suite.benchmark_screenshot_capture(
            screenshot_type="full_page", 
            complexity=ScreenshotComplexity.SIMPLE,
            iterations=5
        )
        all_results.extend(capture_results)
        print(f"   ✅ Completed {len(capture_results)} capture tests")
        
        # Add to monitoring
        self.monitor.add_performance_data(all_results)
        
        # Check for regressions
        print("🔍 Analyzing performance...")
        regression_analysis = self.benchmark_suite.detect_performance_regression(all_results)
        ci_report = self.monitor.generate_ci_cd_report()
        
        # Determine success
        has_regressions = regression_analysis.get("regression_detected", False)
        health_critical = ci_report["health_status"] == "critical"
        overall_success = not (has_regressions or health_critical)
        
        test_summary = {
            "timestamp": datetime.now().isoformat(),
            "tests_executed": len(all_results),
            "overall_success": overall_success,
            "health_status": ci_report["health_status"],
            "regressions_detected": has_regressions,
            "regression_count": len(regression_analysis.get("regression_details", [])),
            "recommendations": ci_report.get("recommendations", [])
        }
        
        # Generate reports
        self._save_json_report(test_summary, all_results)
        self._save_markdown_report(test_summary)
        
        print(f"🏁 Tests completed: {'✅ PASSED' if overall_success else '❌ FAILED'}")
        return test_summary
    
    def _save_json_report(self, summary: dict, results: list):
        """Save JSON performance report"""
        report_path = os.path.join(self.output_dir, "performance_report.json")
        
        full_report = {
            "summary": summary,
            "detailed_analysis": self.benchmark_suite.analyze_performance(results),
            "results_count": len(results)
        }
        
        with open(report_path, 'w') as f:
            json.dump(full_report, f, indent=2)
        
        print(f"📄 JSON report: {report_path}")
    
    def _save_markdown_report(self, summary: dict):
        """Save Markdown performance report"""
        report_path = os.path.join(self.output_dir, "performance_report.md")
        
        status_emoji = "✅" if summary["overall_success"] else "❌"
        health_emoji = {"healthy": "💚", "warning": "⚠️", "critical": "🔴"}.get(
            summary["health_status"], "❓"
        )
        
        content = f"""# Performance Test Report

## Summary
{status_emoji} **Status**: {'PASSED' if summary["overall_success"] else 'FAILED'}
{health_emoji} **Health**: {summary["health_status"]}

- **Tests Executed**: {summary["tests_executed"]}
- **Regressions**: {summary["regression_count"]}
- **Timestamp**: {summary["timestamp"]}

## Recommendations
"""
        
        for rec in summary["recommendations"]:
            content += f"- {rec}\n"
        
        content += "\n---\n*Generated by Task 22.3 Performance Testing Framework*"
        
        with open(report_path, 'w') as f:
            f.write(content)
        
        print(f"📄 Markdown report: {report_path}")


async def main():
    """Main entry point for CI/CD performance testing"""
    runner = CICDPerformanceRunner()
    
    try:
        test_summary = await runner.run_performance_tests()
        
        # Exit with appropriate code for CI/CD
        if test_summary["overall_success"]:
            print("🎉 Performance tests PASSED")
            sys.exit(0)
        else:
            print("💥 Performance tests FAILED")
            sys.exit(1)
            
    except Exception as e:
        print(f"💥 Error running performance tests: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 