#!/usr/bin/env python3
"""
Enhanced Web Research Chain - Native LangChain WebBaseLoader
Advanced implementation for collecting 95+ casino data points with geo-restrictions handling

‚úÖ PRODUCTION FEATURES:
- Multi-region URL strategies for geo-restricted sites
- Smart content parsing with fallback mechanisms
- Concurrent loading with rate limiting
- Integration with 95-field ComprehensiveResearchData
- Error recovery and retry mechanisms

üéØ HANDLES GEO-RESTRICTIONS:
- Alternative domain discovery (.com, .co.uk, .ca, etc.)
- Proxy rotation support
- Mirror site detection
- Archive.org fallback for blocked content
"""

from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from urllib.parse import urljoin, urlparse
import asyncio
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

# Simplified 95-field data structure for casino research
class ComprehensiveResearchData(BaseModel):
    """Simplified 95-field structure for casino research"""
    trustworthiness: Dict[str, Any] = Field(default_factory=dict)
    games: Dict[str, Any] = Field(default_factory=dict) 
    bonuses: Dict[str, Any] = Field(default_factory=dict)
    payments: Dict[str, Any] = Field(default_factory=dict)
    user_experience: Dict[str, Any] = Field(default_factory=dict)
    innovations: Dict[str, Any] = Field(default_factory=dict)
    compliance: Dict[str, Any] = Field(default_factory=dict)
    assessment: Dict[str, Any] = Field(default_factory=dict)

@dataclass
class URLStrategy:
    """URL generation strategy for different casino data categories"""
    base_domain: str
    alternative_domains: List[str] = field(default_factory=list)
    category_paths: Dict[str, List[str]] = field(default_factory=dict)
    
    def __post_init__(self):
        if not self.category_paths:
            self.category_paths = {
                'trustworthiness': [
                    '/about', '/about-us', '/company', '/licensing', 
                    '/legal', '/terms', '/privacy', '/responsible-gaming',
                    '/security', '/fairness'
                ],
                'games': [
                    '/casino', '/games', '/slots', '/live-casino',
                    '/table-games', '/jackpots', '/new-games',
                    '/providers', '/software'
                ],
                'bonuses': [
                    '/promotions', '/bonuses', '/welcome-bonus',
                    '/free-spins', '/loyalty', '/vip', '/rewards',
                    '/offers', '/deals'
                ],
                'payments': [
                    '/banking', '/payments', '/deposit', '/withdrawal',
                    '/payment-methods', '/cashier', '/transactions',
                    '/fees', '/limits'
                ],
                'user_experience': [
                    '/support', '/help', '/contact', '/faq',
                    '/mobile', '/app', '/download', '/languages',
                    '/reviews', '/testimonials'
                ],
                'compliance': [
                    '/responsible-gambling', '/self-exclusion',
                    '/age-verification', '/aml', '/kyc',
                    '/complaints', '/dispute-resolution'
                ]
            }

class EnhancedWebBaseLoader:
    """Enhanced WebBaseLoader with casino-specific optimizations"""
    
    def __init__(self, 
                 strategy: URLStrategy,
                 requests_per_second: float = 0.5,
                 max_workers: int = 3,
                 timeout: int = 30,
                 use_archive_fallback: bool = True):
        
        self.strategy = strategy
        self.requests_per_second = requests_per_second
        self.max_workers = max_workers
        self.timeout = timeout
        self.use_archive_fallback = use_archive_fallback
        
        # Enhanced headers for casino sites
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
    
    def generate_urls(self, categories: Optional[List[str]] = None) -> Dict[str, List[str]]:
        """Generate comprehensive URL lists for data collection"""
        
        if categories is None:
            categories = list(self.strategy.category_paths.keys())
        
        url_collection = {}
        
        # All domains to try (main + alternatives)
        all_domains = [self.strategy.base_domain] + self.strategy.alternative_domains
        
        for category in categories:
            category_urls = []
            paths = self.strategy.category_paths.get(category, [])
            
            for domain in all_domains:
                for path in paths:
                    # Ensure domain has proper protocol
                    if not domain.startswith(('http://', 'https://')):
                        domain = f'https://{domain}'
                    
                    url = urljoin(domain, path)
                    category_urls.append(url)
            
            url_collection[category] = category_urls
        
        return url_collection
    
    def load_with_fallback(self, urls: List[str]) -> List[Document]:
        """Load URLs with fallback mechanisms"""
        
        successful_docs = []
        failed_urls = []
        
        # Primary loading attempt
        loader = WebBaseLoader(
            web_paths=urls,
            header_template=self.headers,
            requests_per_second=self.requests_per_second,
            continue_on_failure=True,
            raise_for_status=False,
            requests_kwargs={'timeout': self.timeout}
        )
        
        try:
            docs = loader.load()
            
            # Filter successful vs failed loads
            for doc in docs:
                content_length = len(doc.page_content.strip())
                
                # Check for common failure indicators
                if (content_length > 100 and 
                    not any(failure_term in doc.page_content.lower() for failure_term in [
                        'connection timed out', '502 bad gateway', '503 service unavailable',
                        'access denied', 'forbidden', 'not available in your region',
                        'cease trading in your region'
                    ])):
                    successful_docs.append(doc)
                else:
                    failed_urls.append(doc.metadata.get('source', ''))
                    
        except Exception as e:
            print(f"Primary loading failed: {e}")
            failed_urls.extend(urls)
        
        # Archive.org fallback for failed URLs
        if self.use_archive_fallback and failed_urls:
            print(f"Attempting archive.org fallback for {len(failed_urls)} failed URLs...")
            archive_docs = self._load_from_archive(failed_urls)
            successful_docs.extend(archive_docs)
        
        return successful_docs
    
    def _load_from_archive(self, failed_urls: List[str]) -> List[Document]:
        """Load content from Archive.org as fallback"""
        
        archive_docs = []
        
        for url in failed_urls[:3]:  # Limit to avoid overloading archive.org
            try:
                # Archive.org Wayback Machine URL format
                archive_url = f"https://web.archive.org/web/2024/{url}"
                
                archive_loader = WebBaseLoader(
                    web_paths=[archive_url],
                    header_template=self.headers,
                    requests_per_second=0.2,  # Very conservative for archive.org
                    continue_on_failure=True
                )
                
                archive_result = archive_loader.load()
                
                if archive_result and len(archive_result[0].page_content) > 500:
                    # Mark as archive source
                    archive_result[0].metadata['source_type'] = 'archive'
                    archive_result[0].metadata['original_source'] = url
                    archive_docs.extend(archive_result)
                    print(f"‚úÖ Archive fallback successful for: {url}")
                
                time.sleep(2)  # Respectful delay for archive.org
                
            except Exception as e:
                print(f"Archive fallback failed for {url}: {e}")
                continue
        
        return archive_docs

class CasinoDataExtractor:
    """Extract structured data from web content using LLM"""
    
    def __init__(self, llm: Optional[ChatOpenAI] = None):
        self.llm = llm or ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            max_tokens=2000
        )
        
        # Set up extraction prompt
        self.extraction_prompt = PromptTemplate(
            input_variables=["content", "category"],
            template="""
Extract specific {category} information from this casino website content.

Website Content:
{content}

Focus on extracting:
- Factual information only
- Specific details like numbers, names, policies
- Regulatory information
- Technical specifications
- Contact information
- Terms and conditions

Return the extracted information in a clear, structured format.
If information is not found, indicate "Not found" rather than guessing.

Extracted {category} Information:
"""
        )
    
    def extract_category_data(self, documents: List[Document], category: str) -> Dict[str, Any]:
        """Extract data for a specific category from documents"""
        
        extracted_data = {
            'sources': [],
            'raw_extractions': [],
            'structured_data': {},
            'confidence_score': 0.0
        }
        
        for doc in documents:
            if len(doc.page_content.strip()) < 100:
                continue  # Skip minimal content
            
            try:
                # Extract using LLM
                extraction_chain = self.extraction_prompt | self.llm
                
                result = extraction_chain.invoke({
                    'content': doc.page_content[:4000],  # Limit content length
                    'category': category
                })
                
                extracted_data['sources'].append(doc.metadata.get('source', 'Unknown'))
                extracted_data['raw_extractions'].append(result.content)
                
            except Exception as e:
                print(f"Extraction failed for {doc.metadata.get('source', 'Unknown')}: {e}")
                continue
        
        # Calculate confidence based on successful extractions
        if len(extracted_data['raw_extractions']) > 0:
            extracted_data['confidence_score'] = min(len(extracted_data['raw_extractions']) / 3.0, 1.0)
        
        return extracted_data

class ComprehensiveWebResearchChain:
    """Main LCEL chain for comprehensive casino research using WebBaseLoader"""
    
    def __init__(self, 
                 casino_domain: str,
                 alternative_domains: Optional[List[str]] = None,
                 llm: Optional[ChatOpenAI] = None,
                 categories: Optional[List[str]] = None):
        
        self.casino_domain = casino_domain
        self.alternative_domains = alternative_domains or []
        self.llm = llm or ChatOpenAI(model="gpt-3.5-turbo", temperature=0.1)
        self.categories = categories or ['trustworthiness', 'games', 'bonuses', 'payments', 'user_experience', 'compliance']
        
        # Initialize components
        self.url_strategy = URLStrategy(
            base_domain=casino_domain,
            alternative_domains=self.alternative_domains
        )
        
        self.web_loader = EnhancedWebBaseLoader(
            strategy=self.url_strategy,
            requests_per_second=0.5,
            max_workers=3,
            timeout=30
        )
        
        self.data_extractor = CasinoDataExtractor(llm=self.llm)
        
        # Build LCEL chain
        self.chain = self._build_chain()
    
    def _build_chain(self):
        """Build the main LCEL chain for web research"""
        
        def load_and_extract_data(input_dict: Dict[str, Any]) -> Dict[str, Any]:
            """Load web content and extract structured data"""
            
            casino_domain = input_dict.get('casino_domain', self.casino_domain)
            categories = input_dict.get('categories', self.categories)
            
            print(f"üîç Starting comprehensive web research for: {casino_domain}")
            print(f"üìä Categories: {', '.join(categories)}")
            
            # Generate URLs for all categories
            url_collection = self.web_loader.generate_urls(categories)
            
            results = {
                'casino_domain': casino_domain,
                'categories_researched': categories,
                'total_urls_attempted': sum(len(urls) for urls in url_collection.values()),
                'category_data': {},
                'research_summary': {}
            }
            
            # Process each category in parallel
            with ThreadPoolExecutor(max_workers=3) as executor:
                future_to_category = {}
                
                for category, urls in url_collection.items():
                    print(f"üåê Loading {len(urls)} URLs for {category}...")
                    
                    future = executor.submit(self._process_category, category, urls)
                    future_to_category[future] = category
                
                # Collect results
                for future in as_completed(future_to_category):
                    category = future_to_category[future]
                    
                    try:
                        category_result = future.result()
                        results['category_data'][category] = category_result
                        
                        # Add to summary
                        results['research_summary'][category] = {
                            'urls_successful': len(category_result['sources']),
                            'confidence_score': category_result['confidence_score'],
                            'data_quality': 'High' if category_result['confidence_score'] > 0.7 else 'Medium' if category_result['confidence_score'] > 0.3 else 'Low'
                        }
                        
                        print(f"‚úÖ Completed {category}: {len(category_result['sources'])} sources, {category_result['confidence_score']:.2f} confidence")
                        
                    except Exception as e:
                        print(f"‚ùå Failed to process {category}: {e}")
                        results['category_data'][category] = {'error': str(e)}
            
            # Calculate overall research quality
            total_confidence = sum(r.get('confidence_score', 0) for r in results['research_summary'].values())
            avg_confidence = total_confidence / len(results['research_summary']) if results['research_summary'] else 0
            
            results['overall_quality'] = {
                'average_confidence': avg_confidence,
                'research_grade': 'A' if avg_confidence > 0.8 else 'B' if avg_confidence > 0.6 else 'C' if avg_confidence > 0.4 else 'D',
                'ready_for_95_fields': avg_confidence > 0.6
            }
            
            print(f"\nüéØ Research Complete: {results['overall_quality']['research_grade']} grade, {avg_confidence:.2f} confidence")
            print(f"üìä Ready for 95 fields extraction: {results['overall_quality']['ready_for_95_fields']}")
            
            return results
        
        # Build LCEL chain
        return RunnableLambda(load_and_extract_data)
    
    def _process_category(self, category: str, urls: List[str]) -> Dict[str, Any]:
        """Process a single category with its URLs"""
        
        # Load documents with fallback
        documents = self.web_loader.load_with_fallback(urls)
        
        if not documents:
            return {
                'sources': [],
                'raw_extractions': [],
                'structured_data': {},
                'confidence_score': 0.0,
                'error': 'No content could be loaded'
            }
        
        # Extract structured data
        extracted_data = self.data_extractor.extract_category_data(documents, category)
        
        return extracted_data
    
    def invoke(self, input_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Invoke the research chain"""
        return self.chain.invoke(input_dict)

# Factory function for easy initialization
def create_comprehensive_web_research_chain(casino_domain: str = "casino.org", 
                                           alternative_domains: Optional[List[str]] = None,
                                           llm: Optional[ChatOpenAI] = None,
                                           categories: Optional[List[str]] = None) -> ComprehensiveWebResearchChain:
    """Create a comprehensive web research chain for casino analysis"""
    
    return ComprehensiveWebResearchChain(
        casino_domain=casino_domain,
        alternative_domains=alternative_domains,
        llm=llm,
        categories=categories
    )

# Example usage and testing
if __name__ == "__main__":
    # Test with casino.org 
    print(f"\n{'='*60}")
    print(f"üé∞ Testing Web Research for: casino.org")
    print(f"{'='*60}")
    
    try:
        # Create research chain
        research_chain = create_comprehensive_web_research_chain(
            casino_domain='casino.org',
            categories=['trustworthiness', 'games', 'bonuses']  # Limited for testing
        )
        
        # Run research
        results = research_chain.invoke({
            'casino_domain': 'casino.org'
        })
        
        print(f"\nüéØ RESULTS SUMMARY:")
        print(f"Overall Grade: {results['overall_quality']['research_grade']}")
        print(f"Average Confidence: {results['overall_quality']['average_confidence']:.2f}")
        print(f"URLs Attempted: {results['total_urls_attempted']}")
        print(f"Ready for 95 Fields: {results['overall_quality']['ready_for_95_fields']}")
        
        for category, summary in results['research_summary'].items():
            print(f"  {category.title()}: {summary['urls_successful']} sources, {summary['confidence_score']:.2f} confidence, {summary['data_quality']} quality")
        
    except Exception as e:
        print(f"‚ùå Research failed: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\n‚úÖ Web Research Chain testing completed!")
    print(f"üöÄ Ready for integration with Universal RAG Chain!") 