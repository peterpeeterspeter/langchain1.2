{
  "master": {
    "tasks": [
      {
        "id": 24,
        "title": "Create Project Structure and Initial Files",
        "description": "Set up the directory structure and create initial files for the CacheService extraction project according to the specified file structure, leveraging LangChain's native caching infrastructure with support for multi-cache orchestration and async operations.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "1. Create the following directory structure:\n```\nsrc/\n├── services/\n│   ├── __init__.py\n│   │   ├── cache/\n│   │   │   ├── __init__.py\n│   │   │   ├── casino_cache_orchestrator.py\n│   │   │   ├── cache_factory.py\n│   │   │   ├── semantic_key_generator.py\n│   │   │   ├── ttl_strategy.py\n│   │   │   ├── content_type_config.py\n│   │   │   ├── redis_client_manager.py\n│   │   │   ├── cache_analytics.py\n│   │   │   └── exceptions.py\n│   │   └── ...\n│   ├── chains/\n│   │   └── universal_rag_lcel.py\n│   │   └── async_lcel_integration.py\n│   └── tests/\n│       └── services/\n│           └── cache/\n│               ├── test_casino_cache_orchestrator.py\n│               ├── test_cache_factory.py\n│               ├── test_semantic_key_generator.py\n│               ├── test_ttl_strategy.py\n│               ├── test_content_type_config.py\n│               ├── test_redis_client_manager.py\n│               └── test_cache_analytics.py\n```\n2. Create empty Python files with appropriate module docstrings\n3. Add necessary imports and package initialization\n4. Set up basic class skeletons for each component that extend LangChain's native caching classes\n5. Implement support for multi-cache orchestration with specialized RedisSemanticCache instances\n6. Configure content-type specific settings (news, reviews, regulatory)\n7. Design shared Redis client architecture for efficient connection management\n8. Implement async LCEL integration patterns\n9. Add cache analytics and monitoring capabilities\n10. Ensure TTL is handled in seconds (not timedelta) for compatibility\n11. Support distance threshold optimization per content type",
        "testStrategy": "Verify that all directories and files exist in the correct structure. Ensure that Python files can be imported without errors and contain appropriate docstrings. Verify that the classes properly extend LangChain's native caching classes. Test the multi-cache orchestration capabilities, async operations, and content-type specific configurations. Validate that TTL is properly handled in seconds and that distance thresholds can be optimized per content type.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Directory Structure",
            "description": "Set up the complete directory structure for the CacheService extraction project according to the specified file structure.",
            "status": "done",
            "dependencies": [],
            "details": "Use the `mkdir -p` command or equivalent to create the nested directory structure. Ensure all directories are created with proper permissions. The structure should include: src/services/, src/services/cache/, src/services/chains/, and src/tests/services/cache/. Verify the directory structure is correctly created before proceeding to the next subtask.",
            "testStrategy": "Manually verify that all directories exist and have the correct structure using `ls -la` or equivalent file explorer commands."
          },
          {
            "id": 2,
            "title": "Create Cache Module Files",
            "description": "Create the empty Python files for the cache module with appropriate module docstrings.",
            "status": "done",
            "dependencies": [],
            "details": "Create the following files in the src/services/cache/ directory: __init__.py, casino_cache_orchestrator.py, cache_factory.py, semantic_key_generator.py, ttl_strategy.py, content_type_config.py, redis_client_manager.py, cache_analytics.py, and exceptions.py. Each file should include a module docstring that describes its purpose. For example, casino_cache_orchestrator.py should have a docstring explaining that it orchestrates multiple specialized RedisSemanticCache instances for different content types.",
            "testStrategy": "Verify that all files exist and contain appropriate module docstrings."
          },
          {
            "id": 3,
            "title": "Create Test Files",
            "description": "Create empty test files for each cache module component.",
            "status": "done",
            "dependencies": [],
            "details": "Create the following test files in the src/tests/services/cache/ directory: test_casino_cache_orchestrator.py, test_cache_factory.py, test_semantic_key_generator.py, test_ttl_strategy.py, test_content_type_config.py, test_redis_client_manager.py, and test_cache_analytics.py. Each test file should include imports for the unittest or pytest framework and the corresponding module being tested. Add docstrings explaining the purpose of each test file.",
            "testStrategy": "Verify that all test files exist and contain the necessary imports and docstrings."
          },
          {
            "id": 4,
            "title": "Implement Package Initialization",
            "description": "Add necessary imports and package initialization to __init__.py files.",
            "status": "done",
            "dependencies": [],
            "details": "Update the __init__.py files to properly expose the module's classes and functions. In src/services/__init__.py, include imports for submodules. In src/services/cache/__init__.py, import and expose the main classes (CasinoCacheOrchestrator, CacheFactory, SemanticKeyGenerator, etc.) to make them available when importing from the package. Use relative imports where appropriate.",
            "testStrategy": "Write a simple script that imports the exposed classes from the package to verify that the initialization is working correctly."
          },
          {
            "id": 5,
            "title": "Create Basic Class Skeletons",
            "description": "Implement skeleton class definitions for each component in the cache module that extend LangChain's native caching classes.",
            "status": "done",
            "dependencies": [],
            "details": "For each main Python file, create skeleton class definitions with docstrings, constructor methods, and stub methods for the main functionality. Ensure these classes properly extend or integrate with LangChain's native caching classes:\n- casino_cache_orchestrator.py: Create CasinoCacheOrchestrator class that manages multiple specialized RedisSemanticCache instances\n- cache_factory.py: Implement factory for creating different cache instances\n- semantic_key_generator.py: Create key generator with content-type specific configurations\n- ttl_strategy.py: Implement TTL strategy using seconds (not timedelta)\n- content_type_config.py: Define configurations for different content types (news, reviews, regulatory)\n- redis_client_manager.py: Implement shared Redis client architecture\n- cache_analytics.py: Create analytics and monitoring capabilities\n- exceptions.py: Define custom exception classes\n- chains/universal_rag_lcel.py: Create skeleton for RAG chain implementation\n- chains/async_lcel_integration.py: Implement async LCEL integration patterns\n\nInclude type hints and docstrings for all methods.",
            "testStrategy": "Run a static type checker (like mypy) to verify the class structures and imports are valid. Ensure each file can be imported without runtime errors."
          },
          {
            "id": 6,
            "title": "Implement Multi-Cache Orchestration",
            "description": "Design and implement the CasinoCacheOrchestrator to manage multiple specialized RedisSemanticCache instances.",
            "status": "done",
            "dependencies": [
              5
            ],
            "details": "In casino_cache_orchestrator.py, implement the CasinoCacheOrchestrator class that:\n1. Manages multiple RedisSemanticCache instances for different content types\n2. Provides methods for routing cache operations to the appropriate specialized cache\n3. Supports both synchronous and asynchronous operations\n4. Implements fallback strategies when primary caches fail\n5. Handles cache invalidation across multiple caches\n\nEnsure the orchestrator can be easily configured and extended for different content types.",
            "testStrategy": "Create unit tests that verify the orchestrator correctly routes requests to the appropriate cache instance. Test both sync and async operations, as well as fallback strategies."
          },
          {
            "id": 7,
            "title": "Implement Content-Type Configuration",
            "description": "Create configuration system for content-type specific cache settings.",
            "status": "done",
            "dependencies": [
              5
            ],
            "details": "In content_type_config.py, implement a configuration system that:\n1. Defines settings for different content types (news, reviews, regulatory)\n2. Configures distance thresholds per content type\n3. Sets appropriate TTL values in seconds for each content type\n4. Configures embedding models or parameters per content type\n5. Provides a clean API for retrieving configurations\n\nImplement as a combination of dataclasses/Pydantic models and factory methods.",
            "testStrategy": "Write tests that verify configurations are correctly loaded and applied for different content types. Test that distance thresholds and TTL values are properly set."
          },
          {
            "id": 8,
            "title": "Implement Shared Redis Client Architecture",
            "description": "Create a Redis client manager for efficient connection handling across multiple cache instances.",
            "status": "done",
            "dependencies": [
              5
            ],
            "details": "In redis_client_manager.py, implement a client manager that:\n1. Provides a shared Redis client pool\n2. Handles connection lifecycle (creation, reuse, cleanup)\n3. Supports both synchronous and asynchronous Redis clients\n4. Implements connection retry and error handling\n5. Configures Redis client parameters appropriately for semantic caching\n\nEnsure the implementation is thread-safe and efficient for multi-cache scenarios.",
            "testStrategy": "Create tests that verify connection pooling works correctly. Test connection error handling and recovery. Verify that both sync and async clients can be obtained and used correctly."
          },
          {
            "id": 9,
            "title": "Implement Async LCEL Integration",
            "description": "Create patterns for integrating the cache orchestrator with async LCEL chains.",
            "status": "done",
            "dependencies": [
              5,
              6
            ],
            "details": "In async_lcel_integration.py, implement:\n1. Async-compatible cache middleware for LCEL chains\n2. Patterns for integrating the cache orchestrator with async LCEL components\n3. Examples of how to use the cache with different LCEL chain configurations\n4. Error handling and fallback strategies for async operations\n\nEnsure the implementation follows LangChain's best practices for async LCEL integration.",
            "testStrategy": "Create tests that verify async LCEL chains correctly interact with the cache orchestrator. Test error handling and fallback strategies."
          },
          {
            "id": 10,
            "title": "Implement Cache Analytics and Monitoring",
            "description": "Create a system for tracking cache performance and usage metrics.",
            "status": "done",
            "dependencies": [
              5,
              6
            ],
            "details": "In cache_analytics.py, implement:\n1. Hit/miss rate tracking per content type\n2. Latency measurement for cache operations\n3. Size monitoring for cache instances\n4. Periodic reporting capabilities\n5. Integration with external monitoring systems (optional)\n\nDesign the analytics system to have minimal performance impact on cache operations.",
            "testStrategy": "Write tests that verify metrics are correctly captured and reported. Test that the analytics system correctly tracks different types of cache operations."
          }
        ]
      },
      {
        "id": 25,
        "title": "Implement Cache-Specific Exceptions",
        "description": "Create custom exception classes for cache-related errors to provide clear error handling and messaging for the RedisSemanticCache system.",
        "status": "done",
        "dependencies": [
          24
        ],
        "priority": "medium",
        "details": "In exceptions.py, implement a comprehensive exception hierarchy for the multi-cache orchestration system with both sync and async support:\n\n```python\nclass CacheError(Exception):\n    \"\"\"Base exception for all cache-related errors.\"\"\"\n    pass\n\n# Connection Errors\nclass ConnectionError(CacheError):\n    \"\"\"Raised when there's an error connecting to cache instances.\"\"\"\n    pass\n\nclass MultiCacheConnectionError(ConnectionError):\n    \"\"\"Raised when there are connection failures across multiple cache instances.\"\"\"\n    pass\n\n# Semantic Key Errors\nclass SemanticKeyGenerationError(CacheError):\n    \"\"\"Raised when there's an error generating a semantic cache key.\"\"\"\n    pass\n\n# Content Routing Errors\nclass ContentRoutingError(CacheError):\n    \"\"\"Raised when there's an error routing content to appropriate caches.\"\"\"\n    pass\n\n# Cache Synchronization Errors\nclass CacheSyncError(CacheError):\n    \"\"\"Raised when there are synchronization issues between different cache types.\"\"\"\n    pass\n\n# Casino-specific Errors\nclass CasinoCacheError(CacheError):\n    \"\"\"Raised when there's an error with the casino-specific cache operations.\"\"\"\n    pass\n\nclass CasinoContextExtractionError(CasinoCacheError):\n    \"\"\"Raised when there's an error extracting context from casino data.\"\"\"\n    pass\n\n# TTL Errors\nclass TTLStrategyError(CacheError):\n    \"\"\"Raised when there's an error calculating TTL.\"\"\"\n    pass\n\nclass TTLValidationError(TTLStrategyError):\n    \"\"\"Raised when TTL values are invalid (must be positive integers in seconds).\"\"\"\n    pass\n\n# Distance Threshold Errors\nclass DistanceThresholdError(CacheError):\n    \"\"\"Raised when distance threshold values are invalid (must be between 0.0 and 1.0).\"\"\"\n    pass\n\n# Configuration Errors\nclass CacheConfigurationError(CacheError):\n    \"\"\"Raised when there's an error in cache configuration.\"\"\"\n    pass\n\n# Analytics Errors\nclass CacheAnalyticsError(CacheError):\n    \"\"\"Raised when there's an error with cache analytics operations.\"\"\"\n    pass\n\n# LCEL Integration Errors\nclass LCELIntegrationError(CacheError):\n    \"\"\"Raised when there's an error integrating with LangChain Expression Language.\"\"\"\n    pass\n\n# Async-specific Errors\nclass AsyncCacheError(CacheError):\n    \"\"\"Base exception for async cache operations.\"\"\"\n    pass\n\nclass AsyncOperationTimeout(AsyncCacheError):\n    \"\"\"Raised when an async cache operation times out.\"\"\"\n    pass\n```\n\nEnsure each exception:\n1. Has appropriate docstrings\n2. Inherits from the correct parent exception\n3. Includes helpful error messaging\n4. Supports both synchronous and asynchronous error handling patterns\n5. Provides context about the specific error condition\n\nImplement additional helper methods for error context propagation in async environments.",
        "testStrategy": "Write comprehensive unit tests that:\n\n1. Verify each exception can be raised and caught properly\n2. Test inheritance relationships to ensure that catching a base exception also catches derived exceptions\n3. Test async error handling patterns with asyncio.gather() and try/except blocks\n4. Verify error messages contain useful diagnostic information\n5. Test error propagation across multiple cache instances\n6. Test validation errors with boundary values (e.g., TTL=0, distance threshold=1.1)\n7. Test integration with LCEL error handling\n8. Ensure exceptions work correctly in both sync and async contexts\n\nUse pytest fixtures to set up test environments with multiple cache instances.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement base CacheError class",
            "description": "Create the base CacheError class that all other cache exceptions will inherit from.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement connection-related exceptions",
            "description": "Create ConnectionError and MultiCacheConnectionError classes for handling Redis connection failures across multiple cache instances.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement semantic key and content routing exceptions",
            "description": "Create SemanticKeyGenerationError and ContentRoutingError classes.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement cache synchronization exceptions",
            "description": "Create CacheSyncError for handling synchronization issues between different cache types.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement casino-specific exceptions",
            "description": "Create CasinoCacheError and CasinoContextExtractionError classes.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement TTL and threshold validation exceptions",
            "description": "Create TTLStrategyError, TTLValidationError, and DistanceThresholdError classes with proper validation logic.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement configuration and analytics exceptions",
            "description": "Create CacheConfigurationError and CacheAnalyticsError classes.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement LCEL integration exceptions",
            "description": "Create LCELIntegrationError for handling failures in LangChain Expression Language integration.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement async-specific exceptions",
            "description": "Create AsyncCacheError and AsyncOperationTimeout classes with async-compatible error handling patterns.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Write unit tests for exception hierarchy",
            "description": "Create tests that verify inheritance relationships and proper exception catching.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Write unit tests for async error handling",
            "description": "Create tests that verify exceptions work correctly in async contexts with asyncio.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Write validation error tests",
            "description": "Create tests for TTL and distance threshold validation with boundary values.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 26,
        "title": "Implement TTLStrategy Class",
        "description": "Create the TTLStrategy class to handle dynamic Time-To-Live (TTL) calculations based on query types, confidence scores, and user expertise levels, returning TTL values in seconds for compatibility with RedisSemanticCache.",
        "status": "cancelled",
        "dependencies": [
          25
        ],
        "priority": "high",
        "details": "In ttl_strategy.py, implement the TTLStrategy class:\n\n```python\nfrom typing import Dict, Any, Optional, Union\nfrom datetime import timedelta\nfrom .exceptions import TTLStrategyError\n\nclass TTLStrategy:\n    \"\"\"Provides dynamic TTL calculation strategies for cache entries based on query context.\"\"\"\n    \n    # Default TTL values in seconds for different content types\n    DEFAULT_TTL_MAPPING = {\n        'NEWS_UPDATE': 7200,       # 2 hours\n        'REVIEWS': 172800,         # 2 days\n        'REGULATORY': 604800,      # 7 days\n        'GENERAL_QUERY': 86400,    # 1 day\n    }\n    \n    def __init__(self, default_ttl_seconds: int = 86400):\n        \"\"\"Initialize TTLStrategy with default TTL in seconds.\"\"\"\n        self.default_ttl = default_ttl_seconds\n        \n    def calculate_ttl(self, query_type: Optional[str] = None, \n                      confidence_score: Optional[float] = None,\n                      expertise_level: Optional[str] = None,\n                      **kwargs) -> int:\n        \"\"\"Calculate TTL in seconds based on query type, confidence score, and user expertise.\n        \n        Args:\n            query_type: Type of query (e.g., NEWS_UPDATE, REVIEWS, REGULATORY)\n            confidence_score: Confidence score of the query analysis (0.0-1.0)\n            expertise_level: User expertise level\n            **kwargs: Additional context for TTL calculation\n            \n        Returns:\n            int: The calculated TTL duration in seconds for RedisSemanticCache compatibility\n        \n        Raises:\n            TTLStrategyError: If TTL calculation fails\n        \"\"\"\n        try:\n            # Get base TTL from query type\n            base_ttl = self.DEFAULT_TTL_MAPPING.get(query_type, self.default_ttl)\n            \n            # Adjust TTL based on confidence score (lower confidence = shorter TTL)\n            if confidence_score is not None:\n                confidence_factor = max(0.5, min(1.5, confidence_score * 1.5))\n                base_ttl = base_ttl * confidence_factor\n            \n            # Adjust TTL based on expertise level\n            if expertise_level == 'EXPERT':\n                # Experts might need fresher data\n                base_ttl = base_ttl * 0.8\n            elif expertise_level == 'BEGINNER':\n                # Beginners can use slightly older data\n                base_ttl = base_ttl * 1.2\n            \n            # Apply any additional adjustments from kwargs\n            freshness_factor = kwargs.get('freshness_factor', 1.0)\n            base_ttl = base_ttl * freshness_factor\n            \n            return int(base_ttl)  # Ensure we return an integer value in seconds\n        except Exception as e:\n            raise TTLStrategyError(f\"Failed to calculate TTL: {str(e)}\") from e\n            \n    def from_timedelta(self, delta: timedelta) -> int:\n        \"\"\"Convert a timedelta object to seconds for RedisSemanticCache compatibility.\n        \n        Args:\n            delta: A timedelta object\n            \n        Returns:\n            int: Total seconds as an integer\n        \"\"\"\n        return int(delta.total_seconds())\n        \n    def to_timedelta(self, seconds: int) -> timedelta:\n        \"\"\"Convert seconds to a timedelta object for convenience.\n        \n        Args:\n            seconds: TTL in seconds\n            \n        Returns:\n            timedelta: A timedelta object representing the TTL\n        \"\"\"\n        return timedelta(seconds=seconds)\n```",
        "testStrategy": "Write unit tests that verify TTL calculations for different query types, confidence scores, and expertise levels. Test edge cases like very high/low confidence scores and missing parameters. Ensure TTLStrategyError is raised appropriately for invalid inputs. Verify that the returned values are integers (seconds) compatible with RedisSemanticCache. Test the timedelta conversion methods to ensure proper conversion between seconds and timedelta objects. Verify the predefined TTL values for different content types (News, Reviews, Regulatory, etc.) match the requirements.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement TTLStrategy class with seconds-based TTL",
            "description": "Create the TTLStrategy class with DEFAULT_TTL_MAPPING using seconds instead of hours, and ensure calculate_ttl returns integer seconds.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement timedelta conversion methods",
            "description": "Add from_timedelta and to_timedelta methods to convert between timedelta objects and seconds for flexibility.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Write unit tests for TTLStrategy",
            "description": "Create comprehensive tests for TTL calculations with different content types, confidence scores, and expertise levels. Verify seconds-based output and timedelta conversions.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Test integration with RedisSemanticCache",
            "description": "Verify that the TTLStrategy output is compatible with RedisSemanticCache's TTL parameter requirements.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 27,
        "title": "Implement SemanticKeyGenerator Class",
        "description": "Create the SemanticKeyGenerator class to handle semantic cache key generation with casino name extraction and query analysis integration, leveraging LangChain's semantic caching capabilities and optimizing for RedisSemanticCache compatibility.",
        "status": "cancelled",
        "dependencies": [
          25
        ],
        "priority": "high",
        "details": "In semantic_key_generator.py, implement the SemanticKeyGenerator class:\n\n```python\nimport re\nfrom typing import Dict, Any, Optional, List, Union\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.caches import BaseCache\nfrom .exceptions import SemanticKeyGenerationError\n\nclass SemanticKeyGenerator:\n    \"\"\"Generates semantic cache keys with casino name isolation for LangChain's RedisSemanticCache.\"\"\"\n    \n    # Common casino name patterns to extract from queries\n    CASINO_PATTERNS = [\n        r'\\b(888casino|888\\s+casino)\\b',\n        r'\\b(betway)\\b',\n        # Add other casino patterns from existing implementation\n    ]\n    \n    # Content type patterns for routing decisions\n    CONTENT_TYPES = {\n        'faq': r'\\b(faq|frequently asked|question)\\b',\n        'game_rules': r'\\b(rules|how to play|gameplay)\\b',\n        'bonus_terms': r'\\b(bonus|promotion|offer|terms)\\b'\n    }\n    \n    def __init__(self, embeddings: Optional[Embeddings] = None, namespace: str = 'rag_cache'):\n        \"\"\"Initialize SemanticKeyGenerator with embeddings model and namespace.\n        \n        Args:\n            embeddings: LangChain embeddings model for semantic key generation\n            namespace: Namespace prefix for all keys\n        \"\"\"\n        self.embeddings = embeddings\n        self.namespace = namespace\n        self.casino_regex = re.compile('|'.join(self.CASINO_PATTERNS), re.IGNORECASE)\n        self.content_type_regexes = {k: re.compile(v, re.IGNORECASE) for k, v in self.CONTENT_TYPES.items()}\n    \n    def extract_casino_name(self, query: str) -> Optional[str]:\n        \"\"\"Extract casino name from query text.\n        \n        Args:\n            query: The user query text\n            \n        Returns:\n            Optional[str]: Extracted casino name or None if not found\n        \"\"\"\n        match = self.casino_regex.search(query)\n        if match:\n            return match.group(0).lower().replace(' ', '')\n        return None\n    \n    def detect_content_type(self, query: str) -> Optional[str]:\n        \"\"\"Detect content type from query for routing decisions.\n        \n        Args:\n            query: The user query text\n            \n        Returns:\n            Optional[str]: Detected content type or None if not determined\n        \"\"\"\n        for content_type, regex in self.content_type_regexes.items():\n            if regex.search(query):\n                return content_type\n        return None\n    \n    def generate_redis_prefix(self, query: str, context: Dict[str, Any] = None) -> str:\n        \"\"\"Generate a Redis key prefix for casino-specific isolation.\n        \n        Args:\n            query: The user query text\n            context: Additional context for prefix generation\n            \n        Returns:\n            str: Generated Redis key prefix\n        \"\"\"\n        # Extract casino name for isolation\n        casino = self.extract_casino_name(query)\n        content_type = self.detect_content_type(query)\n        \n        # Create base prefix components\n        prefix_parts = [self.namespace]\n        \n        # Add casino name if found (for isolation)\n        if casino:\n            prefix_parts.append(f\"casino:{casino}\")\n        \n        # Add content type if detected\n        if content_type:\n            prefix_parts.append(f\"type:{content_type}\")\n            \n        # Add any additional context keys that should affect caching\n        if context:\n            for k in ['language', 'expertise_level']:\n                if k in context:\n                    prefix_parts.append(f\"{k}:{context[k]}\")\n        \n        # Join all parts with colon separator\n        return ':'.join(prefix_parts)\n    \n    def prepare_prompt_for_embedding(self, prompt: str) -> str:\n        \"\"\"Prepare prompt for embedding by normalizing and removing casino-specific terms.\n        \n        Args:\n            prompt: The user prompt text\n            \n        Returns:\n            str: Normalized prompt for embedding\n        \"\"\"\n        # Remove casino names to focus on the core query intent\n        normalized_prompt = self.casino_regex.sub('', prompt)\n        # Remove extra whitespace\n        normalized_prompt = ' '.join(normalized_prompt.split())\n        return normalized_prompt\n    \n    def enhance_llm_string(self, llm_string: str, query: str, context: Dict[str, Any] = None) -> str:\n        \"\"\"Enhance the llm_string with casino context for better cache isolation.\n        \n        Args:\n            llm_string: Original LLM string from LangChain\n            query: The user query text\n            context: Additional context\n            \n        Returns:\n            str: Enhanced llm_string with casino context\n        \"\"\"\n        casino = self.extract_casino_name(query)\n        content_type = self.detect_content_type(query)\n        \n        # Create enhanced llm_string with casino and content type context\n        enhanced_parts = [llm_string]\n        \n        if casino:\n            enhanced_parts.append(f\"casino={casino}\")\n        \n        if content_type:\n            enhanced_parts.append(f\"content_type={content_type}\")\n            \n        if context and 'model_name' in context:\n            enhanced_parts.append(f\"model={context['model_name']}\")\n            \n        return \"|\".join(enhanced_parts)\n    \n    def get_embedding_model(self, casino: Optional[str] = None) -> Embeddings:\n        \"\"\"Get the appropriate embedding model, potentially casino-specific.\n        \n        Args:\n            casino: Casino name if available\n            \n        Returns:\n            Embeddings: The embedding model to use\n        \"\"\"\n        # If casino-specific embedding models are configured, return the appropriate one\n        # For now, just return the default embeddings\n        if not self.embeddings:\n            raise SemanticKeyGenerationError(\"No embedding model configured\")\n        return self.embeddings\n    \n    def prepare_for_redis_semantic_cache(self, prompt: str, llm_string: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"Prepare all parameters needed for RedisSemanticCache lookup/update.\n        \n        Args:\n            prompt: The original prompt/query\n            llm_string: The original llm_string\n            context: Additional context\n            \n        Returns:\n            Dict with prepared parameters for RedisSemanticCache\n            \n        Raises:\n            SemanticKeyGenerationError: If preparation fails\n        \"\"\"\n        try:\n            casino = self.extract_casino_name(prompt)\n            \n            return {\n                'prompt': self.prepare_prompt_for_embedding(prompt),\n                'llm_string': self.enhance_llm_string(llm_string, prompt, context),\n                'redis_prefix': self.generate_redis_prefix(prompt, context),\n                'embeddings': self.get_embedding_model(casino),\n                'original_prompt': prompt,\n                'casino': casino,\n                'content_type': self.detect_content_type(prompt)\n            }\n        except Exception as e:\n            raise SemanticKeyGenerationError(f\"Failed to prepare for Redis semantic cache: {str(e)}\") from e\n```",
        "testStrategy": "Write unit tests that verify:\n\n1. Redis prefix generation with different queries, casino names, and context values\n2. Casino name extraction with various casino name formats\n3. Content type detection for different query types\n4. LLM string enhancement with casino and content type context\n5. Prompt preparation for embedding correctly normalizes and removes casino-specific terms\n6. The complete prepare_for_redis_semantic_cache method returns all expected parameters\n7. SemanticKeyGenerationError is raised for invalid inputs\n8. Different queries with the same casino produce different but consistently prefixed Redis keys\n9. The same query for different casinos produces different Redis keys\n10. Test with mock RedisSemanticCache to ensure compatibility with its lookup() and update() methods\n11. Verify that embedding model selection works correctly with and without casino-specific models",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement casino name extraction",
            "description": "Implement the extract_casino_name method to identify and extract casino names from user queries using regex patterns.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement content type detection",
            "description": "Create the detect_content_type method to identify query intent categories (FAQ, game rules, bonus terms, etc.) for routing decisions.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Redis prefix generation",
            "description": "Develop the generate_redis_prefix method to create casino-specific and content-type aware Redis key prefixes for proper cache isolation.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement prompt normalization",
            "description": "Create the prepare_prompt_for_embedding method to normalize queries by removing casino-specific terms for better semantic matching.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement LLM string enhancement",
            "description": "Develop the enhance_llm_string method to add casino and content type context to the LLM string for cache isolation.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement embedding model selection",
            "description": "Create the get_embedding_model method to support potential casino-specific embedding models.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement RedisSemanticCache integration",
            "description": "Develop the prepare_for_redis_semantic_cache method to prepare all parameters needed for RedisSemanticCache lookup/update methods.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Write comprehensive unit tests",
            "description": "Create unit tests for all SemanticKeyGenerator methods, including Redis compatibility tests.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 28,
        "title": "Implement CasinoCache Class",
        "description": "Create a casino-aware cache orchestration system that manages multiple specialized RedisSemanticCache instances with content-type specific configurations and casino isolation.",
        "status": "cancelled",
        "dependencies": [
          26,
          27
        ],
        "priority": "high",
        "details": "In casino_cache.py, implement the CasinoCacheOrchestrator class and related components:\n\n```python\nfrom typing import Dict, Any, Optional, List, Union, cast, TypeVar, Generic, Callable\nimport asyncio\nimport json\n\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.caches.redis_semantic_cache import RedisSemanticCache\nfrom langchain_core.embeddings import Embeddings\n\nfrom .semantic_key_generator import SemanticKeyGenerator\nfrom .exceptions import CasinoCacheError\n\nclass CacheConfig:\n    \"\"\"Configuration for a specific content-type cache.\"\"\"\n    \n    def __init__(self, \n                 content_type: str,\n                 similarity_threshold: float,\n                 ttl_seconds: int,\n                 description: str = \"\"):\n        \"\"\"Initialize cache configuration.\n        \n        Args:\n            content_type: Type of content (e.g., 'news', 'reviews', 'regulatory')\n            similarity_threshold: Threshold for semantic similarity matching\n            ttl_seconds: Time-to-live in seconds\n            description: Human-readable description of this cache\n        \"\"\"\n        self.content_type = content_type\n        self.similarity_threshold = similarity_threshold\n        self.ttl_seconds = ttl_seconds\n        self.description = description\n\nclass CacheAnalytics:\n    \"\"\"Analytics tracker for cache performance.\"\"\"\n    \n    def __init__(self):\n        self.requests = 0\n        self.hits = 0\n        self.misses = 0\n        self.updates = 0\n        self.errors = 0\n        \n    @property\n    def hit_rate(self) -> float:\n        \"\"\"Calculate the cache hit rate.\"\"\"\n        if self.requests == 0:\n            return 0.0\n        return self.hits / self.requests\n    \n    def record_hit(self):\n        \"\"\"Record a cache hit.\"\"\"\n        self.requests += 1\n        self.hits += 1\n        \n    def record_miss(self):\n        \"\"\"Record a cache miss.\"\"\"\n        self.requests += 1\n        self.misses += 1\n        \n    def record_update(self):\n        \"\"\"Record a cache update.\"\"\"\n        self.updates += 1\n        \n    def record_error(self):\n        \"\"\"Record a cache error.\"\"\"\n        self.errors += 1\n\nclass CasinoCacheOrchestrator:\n    \"\"\"Casino-aware cache orchestration system that manages multiple specialized RedisSemanticCache instances.\"\"\"\n    \n    DEFAULT_CONFIGS = {\n        \"news\": CacheConfig(\"news\", 0.1, 7200, \"News content cache\"),  # 2 hours\n        \"reviews\": CacheConfig(\"reviews\", 0.3, 172800, \"Review content cache\"),  # 2 days\n        \"regulatory\": CacheConfig(\"regulatory\", 0.2, 604800, \"Regulatory content cache\"),  # 7 days\n        \"default\": CacheConfig(\"default\", 0.2, 86400, \"Default content cache\")  # 1 day\n    }\n    \n    def __init__(self, \n                 redis_url: str,\n                 embeddings: Embeddings,\n                 casino_id: str,\n                 configs: Optional[Dict[str, CacheConfig]] = None,\n                 key_generator: Optional[SemanticKeyGenerator] = None):\n        \"\"\"Initialize the casino cache orchestrator.\n        \n        Args:\n            redis_url: URL for Redis connection\n            embeddings: Embeddings model to use for semantic caching\n            casino_id: Casino identifier for isolation\n            configs: Custom cache configurations or default if None\n            key_generator: Custom key generator or default if None\n        \"\"\"\n        self.redis_url = redis_url\n        self.embeddings = embeddings\n        self.casino_id = casino_id\n        self.configs = configs or self.DEFAULT_CONFIGS\n        self.key_generator = key_generator or SemanticKeyGenerator()\n        self.analytics = {content_type: CacheAnalytics() for content_type in self.configs.keys()}\n        \n        # Initialize caches with shared Redis client\n        self.caches = {}\n        for content_type, config in self.configs.items():\n            prefix = f\"{casino_id}:{content_type}\"\n            self.caches[content_type] = RedisSemanticCache(\n                redis_url=redis_url,\n                embeddings=embeddings,\n                similarity_threshold=config.similarity_threshold,\n                ttl=config.ttl_seconds,\n                prefix=prefix\n            )\n    \n    def _determine_content_type(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Determine the content type for a given prompt and context.\n        \n        Args:\n            prompt: The prompt string\n            context: Additional context for content type determination\n            \n        Returns:\n            str: The determined content type\n        \"\"\"\n        # Use explicit content type from context if available\n        if context and \"content_type\" in context:\n            content_type = context[\"content_type\"]\n            if content_type in self.configs:\n                return content_type\n        \n        # TODO: Implement more sophisticated content type detection based on prompt analysis\n        # For now, use simple keyword matching\n        prompt_lower = prompt.lower()\n        if \"news\" in prompt_lower or \"article\" in prompt_lower:\n            return \"news\"\n        elif \"review\" in prompt_lower or \"rating\" in prompt_lower:\n            return \"reviews\"\n        elif \"regulation\" in prompt_lower or \"compliance\" in prompt_lower or \"law\" in prompt_lower:\n            return \"regulatory\"\n        \n        return \"default\"\n    \n    def _enhance_llm_string(self, llm_string: str, context: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Enhance the LLM string with casino context for better cache isolation.\n        \n        Args:\n            llm_string: Original LLM string\n            context: Additional context for enhancement\n            \n        Returns:\n            str: Enhanced LLM string\n        \"\"\"\n        # Add casino ID to LLM string for complete isolation\n        enhanced = f\"{llm_string}:casino={self.casino_id}\"\n        \n        # Add relevant context elements that might affect the response\n        if context:\n            if \"user_role\" in context:\n                enhanced += f\":role={context['user_role']}\"\n            if \"language\" in context:\n                enhanced += f\":lang={context['language']}\"\n        \n        return enhanced\n    \n    async def alookup(self, prompt: str, llm_string: str, context: Optional[Dict[str, Any]] = None) -> Optional[str]:\n        \"\"\"Asynchronously look up a cached response based on prompt and LLM string.\n        \n        Args:\n            prompt: The prompt string\n            llm_string: String representation of the LLM\n            context: Additional context for cache lookup\n            \n        Returns:\n            Optional[str]: Cached response if found, None otherwise\n            \n        Raises:\n            CasinoCacheError: If cache lookup fails\n        \"\"\"\n        try:\n            # Determine content type and get appropriate cache\n            content_type = self._determine_content_type(prompt, context)\n            cache = self.caches[content_type]\n            analytics = self.analytics[content_type]\n            \n            # Prepare query and enhance LLM string\n            prepared_prompt = self.key_generator.prepare_query_for_embedding(prompt)\n            enhanced_llm_string = self._enhance_llm_string(llm_string, context)\n            \n            # Lookup in cache\n            result = await cache.alookup(prepared_prompt, enhanced_llm_string)\n            \n            # Record analytics\n            if result is not None:\n                analytics.record_hit()\n            else:\n                analytics.record_miss()\n                \n            return result\n        except Exception as e:\n            self.analytics[content_type].record_error()\n            raise CasinoCacheError(f\"Failed to lookup from cache: {str(e)}\") from e\n    \n    def lookup(self, prompt: str, llm_string: str, context: Optional[Dict[str, Any]] = None) -> Optional[str]:\n        \"\"\"Synchronously look up a cached response based on prompt and LLM string.\n        \n        Args:\n            prompt: The prompt string\n            llm_string: String representation of the LLM\n            context: Additional context for cache lookup\n            \n        Returns:\n            Optional[str]: Cached response if found, None otherwise\n            \n        Raises:\n            CasinoCacheError: If cache lookup fails\n        \"\"\"\n        return asyncio.run(self.alookup(prompt, llm_string, context))\n    \n    async def aupdate(self, prompt: str, llm_string: str, return_val: str, \n                     context: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"Asynchronously update the cache with a new response.\n        \n        Args:\n            prompt: The prompt string\n            llm_string: String representation of the LLM\n            return_val: The response to cache\n            context: Additional context for cache update\n            \n        Raises:\n            CasinoCacheError: If cache update fails\n        \"\"\"\n        try:\n            # Determine content type and get appropriate cache\n            content_type = self._determine_content_type(prompt, context)\n            cache = self.caches[content_type]\n            analytics = self.analytics[content_type]\n            \n            # Prepare query and enhance LLM string\n            prepared_prompt = self.key_generator.prepare_query_for_embedding(prompt)\n            enhanced_llm_string = self._enhance_llm_string(llm_string, context)\n            \n            # Update cache\n            await cache.aupdate(prepared_prompt, enhanced_llm_string, return_val)\n            analytics.record_update()\n        except Exception as e:\n            self.analytics[content_type].record_error()\n            raise CasinoCacheError(f\"Failed to update cache: {str(e)}\") from e\n    \n    def update(self, prompt: str, llm_string: str, return_val: str, \n               context: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"Synchronously update the cache with a new response.\n        \n        Args:\n            prompt: The prompt string\n            llm_string: String representation of the LLM\n            return_val: The response to cache\n            context: Additional context for cache update\n            \n        Raises:\n            CasinoCacheError: If cache update fails\n        \"\"\"\n        asyncio.run(self.aupdate(prompt, llm_string, return_val, context))\n    \n    async def aclear(self, content_type: Optional[str] = None) -> None:\n        \"\"\"Asynchronously clear the cache for a specific content type or all caches.\n        \n        Args:\n            content_type: Content type to clear, or None for all caches\n            \n        Raises:\n            CasinoCacheError: If cache clearing fails\n        \"\"\"\n        try:\n            if content_type is not None and content_type in self.caches:\n                await self.caches[content_type].aclear()\n            else:\n                # Clear all caches\n                for cache in self.caches.values():\n                    await cache.aclear()\n        except Exception as e:\n            raise CasinoCacheError(f\"Failed to clear cache: {str(e)}\") from e\n    \n    def clear(self, content_type: Optional[str] = None) -> None:\n        \"\"\"Synchronously clear the cache for a specific content type or all caches.\n        \n        Args:\n            content_type: Content type to clear, or None for all caches\n            \n        Raises:\n            CasinoCacheError: If cache clearing fails\n        \"\"\"\n        asyncio.run(self.aclear(content_type))\n    \n    def get_analytics(self, content_type: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Get analytics for a specific content type or aggregated across all caches.\n        \n        Args:\n            content_type: Content type to get analytics for, or None for all\n            \n        Returns:\n            Dict[str, Any]: Analytics data\n        \"\"\"\n        if content_type is not None and content_type in self.analytics:\n            analytics = self.analytics[content_type]\n            return {\n                \"content_type\": content_type,\n                \"requests\": analytics.requests,\n                \"hits\": analytics.hits,\n                \"misses\": analytics.misses,\n                \"updates\": analytics.updates,\n                \"errors\": analytics.errors,\n                \"hit_rate\": analytics.hit_rate\n            }\n        else:\n            # Aggregate analytics across all content types\n            total_requests = sum(a.requests for a in self.analytics.values())\n            total_hits = sum(a.hits for a in self.analytics.values())\n            total_misses = sum(a.misses for a in self.analytics.values())\n            total_updates = sum(a.updates for a in self.analytics.values())\n            total_errors = sum(a.errors for a in self.analytics.values())\n            \n            hit_rate = total_hits / total_requests if total_requests > 0 else 0.0\n            \n            return {\n                \"content_type\": \"all\",\n                \"requests\": total_requests,\n                \"hits\": total_hits,\n                \"misses\": total_misses,\n                \"updates\": total_updates,\n                \"errors\": total_errors,\n                \"hit_rate\": hit_rate,\n                \"by_content_type\": {\n                    ct: {\n                        \"requests\": a.requests,\n                        \"hits\": a.hits,\n                        \"hit_rate\": a.hit_rate\n                    } for ct, a in self.analytics.items()\n                }\n            }\n```",
        "testStrategy": "Write comprehensive unit tests for the CasinoCacheOrchestrator:\n\n1. Test async and sync methods for lookup, update, and clear operations\n2. Test with different content types (news, reviews, regulatory) and verify correct cache selection\n3. Test cache isolation between different casino IDs\n4. Test analytics tracking and reporting\n5. Test error handling and exception propagation\n6. Test content type detection logic\n7. Test LLM string enhancement with casino context\n8. Mock Redis client to verify correct TTL and similarity threshold settings\n9. Test performance with large numbers of requests\n10. Test concurrent access patterns\n\nUse pytest-asyncio for testing async methods. Create fixtures for different content types and casino contexts.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement CacheConfig class",
            "description": "Create a configuration class for content-type specific cache settings including similarity threshold and TTL in seconds.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CacheAnalytics class",
            "description": "Create an analytics tracking class that records hits, misses, updates, and calculates hit rates.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement CasinoCacheOrchestrator core functionality",
            "description": "Implement the main orchestrator class that manages multiple RedisSemanticCache instances with shared Redis client.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement content type detection logic",
            "description": "Create the _determine_content_type method to route queries to the appropriate specialized cache.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement LLM string enhancement",
            "description": "Create the _enhance_llm_string method to include casino context in cache keys for better isolation.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement async and sync lookup methods",
            "description": "Implement alookup and lookup methods with analytics tracking and error handling.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement async and sync update methods",
            "description": "Implement aupdate and update methods with analytics tracking and error handling.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement async and sync clear methods",
            "description": "Implement aclear and clear methods with support for clearing specific content types or all caches.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement analytics reporting",
            "description": "Create the get_analytics method to report cache performance metrics by content type or aggregated.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Write unit tests for CasinoCacheOrchestrator",
            "description": "Create comprehensive test suite covering all functionality including async operations, content routing, and analytics.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 29,
        "title": "Implement CachingFactory Class",
        "description": "Create a factory class to simplify the creation and configuration of different types of casino-aware caches based on LangChain's native caching infrastructure, with specialized support for content-specific RedisSemanticCache instances.",
        "status": "cancelled",
        "dependencies": [
          28
        ],
        "priority": "high",
        "details": "In cache_factory.py, implement the CachingFactory class:\n\n```python\nfrom typing import Dict, Any, Optional, Union, Type, Literal\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.caches.redis_cache import RedisCache\nfrom langchain_core.caches.redis_semantic_cache import RedisSemanticCache\nfrom langchain_core.caches.momento_cache import MomentoCache\nfrom langchain_core.caches.momento_semantic_cache import MomentoSemanticCache\nfrom langchain_core.caches.in_memory import InMemoryCache\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.globals import set_llm_cache\nimport redis\n\nfrom .casino_cache import CasinoCache\nfrom .semantic_key_generator import SemanticKeyGenerator\nfrom .ttl_strategy import TTLStrategy\nfrom .exceptions import CacheConfigurationError\n\nContentType = Literal[\"news\", \"reviews\", \"regulatory\", \"general\"]\n\nclass CachingFactory:\n    \"\"\"Factory for creating and configuring different types of casino-aware caches.\"\"\"\n    \n    # Shared Redis client instance\n    _redis_client = None\n    \n    @classmethod\n    def get_redis_client(cls, redis_url: str) -> redis.Redis:\n        \"\"\"Get or create a shared Redis client.\n        \n        Args:\n            redis_url: Redis connection URL\n            \n        Returns:\n            redis.Redis: Shared Redis client instance\n            \n        Raises:\n            CacheConfigurationError: If Redis client creation fails\n        \"\"\"\n        if cls._redis_client is None:\n            try:\n                cls._redis_client = redis.from_url(redis_url)\n            except Exception as e:\n                raise CacheConfigurationError(f\"Failed to create Redis client: {str(e)}\") from e\n        return cls._redis_client\n    \n    @staticmethod\n    def create_memory_cache(ttl_strategy: Optional[TTLStrategy] = None) -> CasinoCache:\n        \"\"\"Create an in-memory casino cache.\n        \n        Args:\n            ttl_strategy: Optional TTL strategy\n            \n        Returns:\n            CasinoCache: Configured in-memory casino cache\n        \"\"\"\n        underlying_cache = InMemoryCache()\n        key_generator = SemanticKeyGenerator()\n        return CasinoCache(\n            underlying_cache=underlying_cache,\n            key_generator=key_generator,\n            ttl_strategy=ttl_strategy or TTLStrategy()\n        )\n    \n    @staticmethod\n    def create_redis_cache(redis_url: str, ttl_strategy: Optional[TTLStrategy] = None) -> CasinoCache:\n        \"\"\"Create a Redis-backed casino cache.\n        \n        Args:\n            redis_url: Redis connection URL\n            ttl_strategy: Optional TTL strategy\n            \n        Returns:\n            CasinoCache: Configured Redis-backed casino cache\n            \n        Raises:\n            CacheConfigurationError: If Redis configuration fails\n        \"\"\"\n        try:\n            underlying_cache = RedisCache(redis_url=redis_url)\n            key_generator = SemanticKeyGenerator()\n            return CasinoCache(\n                underlying_cache=underlying_cache,\n                key_generator=key_generator,\n                ttl_strategy=ttl_strategy or TTLStrategy()\n            )\n        except Exception as e:\n            raise CacheConfigurationError(f\"Failed to configure Redis cache: {str(e)}\") from e\n    \n    @staticmethod\n    def validate_distance_threshold(threshold: float) -> None:\n        \"\"\"Validate that the distance threshold is within acceptable range.\n        \n        Args:\n            threshold: Distance threshold value to validate\n            \n        Raises:\n            CacheConfigurationError: If threshold is not between 0.0 and 1.0\n        \"\"\"\n        if not 0.0 <= threshold <= 1.0:\n            raise CacheConfigurationError(f\"Distance threshold must be between 0.0 and 1.0, got {threshold}\")\n    \n    @staticmethod\n    def validate_ttl(ttl: int) -> None:\n        \"\"\"Validate that the TTL is a positive integer.\n        \n        Args:\n            ttl: TTL value in seconds to validate\n            \n        Raises:\n            CacheConfigurationError: If TTL is not a positive integer\n        \"\"\"\n        if not isinstance(ttl, int) or ttl <= 0:\n            raise CacheConfigurationError(f\"TTL must be a positive integer, got {ttl}\")\n    \n    @classmethod\n    def create_content_specific_semantic_cache(\n        cls,\n        content_type: ContentType,\n        embeddings: Embeddings,\n        redis_url: str,\n        distance_threshold: Optional[float] = None,\n        ttl_seconds: Optional[int] = None,\n        async_mode: bool = False,\n        namespace: Optional[str] = None\n    ) -> RedisSemanticCache:\n        \"\"\"Create a content-specific RedisSemanticCache instance.\n        \n        Args:\n            content_type: Type of content (news, reviews, regulatory, general)\n            embeddings: LangChain embeddings model\n            redis_url: Redis connection URL\n            distance_threshold: Optional override for content-specific distance threshold\n            ttl_seconds: Optional override for content-specific TTL in seconds\n            async_mode: Whether to create an async cache instance\n            namespace: Optional namespace prefix for cache keys\n            \n        Returns:\n            RedisSemanticCache: Configured semantic cache for the specific content type\n            \n        Raises:\n            CacheConfigurationError: If configuration fails or parameters are invalid\n        \"\"\"\n        # Default content-specific configurations\n        content_configs = {\n            \"news\": {\"threshold\": 0.85, \"ttl\": 86400},  # 24 hours for news\n            \"reviews\": {\"threshold\": 0.90, \"ttl\": 604800},  # 7 days for reviews\n            \"regulatory\": {\"threshold\": 0.95, \"ttl\": 2592000},  # 30 days for regulatory\n            \"general\": {\"threshold\": 0.80, \"ttl\": 259200}  # 3 days for general content\n        }\n        \n        # Use provided values or defaults based on content type\n        config = content_configs.get(content_type, content_configs[\"general\"])\n        threshold = distance_threshold if distance_threshold is not None else config[\"threshold\"]\n        ttl = ttl_seconds if ttl_seconds is not None else config[\"ttl\"]\n        \n        # Validate configuration parameters\n        cls.validate_distance_threshold(threshold)\n        cls.validate_ttl(ttl)\n        \n        # Create cache prefix/namespace\n        prefix = f\"casino:{content_type}:\"\n        if namespace:\n            prefix = f\"{namespace}:{prefix}\"\n        \n        try:\n            # Get or create shared Redis client\n            redis_client = cls.get_redis_client(redis_url)\n            \n            # Create the appropriate cache instance\n            if async_mode:\n                from langchain_core.caches.redis_semantic_cache import AsyncRedisSemanticCache\n                return AsyncRedisSemanticCache(\n                    redis_client=redis_client,\n                    embedding=embeddings,\n                    similarity_threshold=threshold,\n                    ttl=ttl,\n                    namespace=prefix\n                )\n            else:\n                return RedisSemanticCache(\n                    redis_client=redis_client,\n                    embedding=embeddings,\n                    similarity_threshold=threshold,\n                    ttl=ttl,\n                    namespace=prefix\n                )\n        except Exception as e:\n            raise CacheConfigurationError(f\"Failed to configure {content_type} semantic cache: {str(e)}\") from e\n    \n    @classmethod\n    def create_news_cache(cls, embeddings: Embeddings, redis_url: str, **kwargs) -> RedisSemanticCache:\n        \"\"\"Create a semantic cache optimized for news content.\n        \n        Args:\n            embeddings: LangChain embeddings model\n            redis_url: Redis connection URL\n            **kwargs: Additional configuration options\n            \n        Returns:\n            RedisSemanticCache: Configured news semantic cache\n        \"\"\"\n        return cls.create_content_specific_semantic_cache(\"news\", embeddings, redis_url, **kwargs)\n    \n    @classmethod\n    def create_reviews_cache(cls, embeddings: Embeddings, redis_url: str, **kwargs) -> RedisSemanticCache:\n        \"\"\"Create a semantic cache optimized for review content.\n        \n        Args:\n            embeddings: LangChain embeddings model\n            redis_url: Redis connection URL\n            **kwargs: Additional configuration options\n            \n        Returns:\n            RedisSemanticCache: Configured reviews semantic cache\n        \"\"\"\n        return cls.create_content_specific_semantic_cache(\"reviews\", embeddings, redis_url, **kwargs)\n    \n    @classmethod\n    def create_regulatory_cache(cls, embeddings: Embeddings, redis_url: str, **kwargs) -> RedisSemanticCache:\n        \"\"\"Create a semantic cache optimized for regulatory content.\n        \n        Args:\n            embeddings: LangChain embeddings model\n            redis_url: Redis connection URL\n            **kwargs: Additional configuration options\n            \n        Returns:\n            RedisSemanticCache: Configured regulatory semantic cache\n        \"\"\"\n        return cls.create_content_specific_semantic_cache(\"regulatory\", embeddings, redis_url, **kwargs)\n    \n    @staticmethod\n    def create_semantic_cache(embeddings: Embeddings, \n                             redis_url: Optional[str] = None,\n                             similarity_threshold: float = 0.8,\n                             ttl_strategy: Optional[TTLStrategy] = None) -> CasinoCache:\n        \"\"\"Create a semantic casino cache with embeddings.\n        \n        Args:\n            embeddings: LangChain embeddings model\n            redis_url: Optional Redis connection URL (uses in-memory if None)\n            similarity_threshold: Threshold for semantic similarity matching\n            ttl_strategy: Optional TTL strategy\n            \n        Returns:\n            CasinoCache: Configured semantic casino cache\n            \n        Raises:\n            CacheConfigurationError: If semantic cache configuration fails\n        \"\"\"\n        try:\n            if redis_url:\n                underlying_cache = RedisSemanticCache(\n                    redis_url=redis_url,\n                    embedding=embeddings,\n                    similarity_threshold=similarity_threshold\n                )\n            else:\n                # Fall back to in-memory if no Redis URL provided\n                underlying_cache = InMemoryCache()\n            \n            key_generator = SemanticKeyGenerator(embeddings=embeddings)\n            return CasinoCache(\n                underlying_cache=underlying_cache,\n                key_generator=key_generator,\n                ttl_strategy=ttl_strategy or TTLStrategy()\n            )\n        except Exception as e:\n            raise CacheConfigurationError(f\"Failed to configure semantic cache: {str(e)}\") from e\n    \n    @staticmethod\n    def setup_global_llm_cache(cache: Union[CasinoCache, BaseCache], llm: Optional[LLM] = None) -> None:\n        \"\"\"Set up global LLM cache using LangChain's set_llm_cache.\n        \n        Args:\n            cache: Cache instance to use globally\n            llm: Optional specific LLM to configure cache for\n            \n        Raises:\n            CacheConfigurationError: If global cache setup fails\n        \"\"\"\n        try:\n            if llm is not None:\n                llm.cache = cache\n            else:\n                set_llm_cache(cache)\n        except Exception as e:\n            raise CacheConfigurationError(f\"Failed to set up global LLM cache: {str(e)}\") from e\n    \n    @classmethod\n    def create_multi_content_cache_system(cls, \n                                         embeddings: Embeddings,\n                                         redis_url: str,\n                                         content_types: Optional[list[ContentType]] = None,\n                                         async_mode: bool = False,\n                                         namespace: Optional[str] = None) -> Dict[ContentType, RedisSemanticCache]:\n        \"\"\"Create a complete cache system with specialized caches for different content types.\n        \n        Args:\n            embeddings: LangChain embeddings model\n            redis_url: Redis connection URL\n            content_types: List of content types to create caches for (defaults to all)\n            async_mode: Whether to create async cache instances\n            namespace: Optional namespace prefix for all cache keys\n            \n        Returns:\n            Dict[ContentType, RedisSemanticCache]: Dictionary of content-specific caches\n            \n        Raises:\n            CacheConfigurationError: If cache system creation fails\n        \"\"\"\n        try:\n            if content_types is None:\n                content_types = [\"news\", \"reviews\", \"regulatory\", \"general\"]\n            \n            cache_system = {}\n            for content_type in content_types:\n                cache_system[content_type] = cls.create_content_specific_semantic_cache(\n                    content_type=content_type,\n                    embeddings=embeddings,\n                    redis_url=redis_url,\n                    async_mode=async_mode,\n                    namespace=namespace\n                )\n            \n            return cache_system\n        except Exception as e:\n            raise CacheConfigurationError(f\"Failed to create multi-content cache system: {str(e)}\") from e\n    \n    @staticmethod\n    def create_multi_level_cache(embeddings: Optional[Embeddings] = None,\n                                redis_url: Optional[str] = None) -> Dict[str, CasinoCache]:\n        \"\"\"Create a multi-level caching strategy with different caches for different levels.\n        \n        Args:\n            embeddings: Optional embeddings model for semantic caching\n            redis_url: Optional Redis connection URL\n            \n        Returns:\n            Dict[str, CasinoCache]: Dictionary of caches for different levels\n            \n        Raises:\n            CacheConfigurationError: If multi-level cache setup fails\n        \"\"\"\n        try:\n            # Create different TTL strategies for different levels\n            llm_ttl = TTLStrategy(default_ttl=48)  # Longer TTL for LLM responses\n            chain_ttl = TTLStrategy(default_ttl=24)  # Medium TTL for chain results\n            rag_ttl = TTLStrategy(default_ttl=12)  # Shorter TTL for RAG results\n            \n            caches = {}\n            \n            # LLM-level cache (exact matching)\n            caches['llm'] = CachingFactory.create_redis_cache(redis_url) if redis_url else CachingFactory.create_memory_cache(llm_ttl)\n            \n            # Chain-level cache\n            caches['chain'] = CachingFactory.create_redis_cache(redis_url) if redis_url else CachingFactory.create_memory_cache(chain_ttl)\n            \n            # RAG-level cache (semantic matching if embeddings provided)\n            if embeddings:\n                caches['rag'] = CachingFactory.create_semantic_cache(embeddings, redis_url, ttl_strategy=rag_ttl)\n            else:\n                caches['rag'] = CachingFactory.create_redis_cache(redis_url) if redis_url else CachingFactory.create_memory_cache(rag_ttl)\n            \n            return caches\n        except Exception as e:\n            raise CacheConfigurationError(f\"Failed to set up multi-level cache: {str(e)}\") from e\n```",
        "testStrategy": "Write unit tests for each factory method with the following coverage:\n\n1. Test creation of basic cache types (memory, Redis, semantic)\n2. Test content-specific cache creation for each content type (news, reviews, regulatory, general)\n3. Verify that the factory correctly configures each cache with the appropriate distance thresholds and TTL values\n4. Test validation methods for distance thresholds and TTL values\n5. Test shared Redis client management\n6. Test both sync and async cache instance creation\n7. Test namespace/prefix configuration\n8. Test the multi-content cache system creation with different configurations\n9. Verify error handling and exception propagation\n10. Test the global LLM cache setup\n11. Test the multi-level cache creation\n\nUse mocking for Redis connections and embedding models to avoid external dependencies during testing.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement content-specific semantic cache factory methods",
            "description": "Create specialized factory methods for news, reviews, and regulatory content with optimized parameters",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement shared Redis client management",
            "description": "Add functionality to create and reuse a shared Redis client across all cache instances",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement configuration validation",
            "description": "Add validation methods for distance thresholds (0.0-1.0) and TTL values (positive integers)",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add support for async cache instances",
            "description": "Extend factory methods to support creation of both synchronous and asynchronous RedisSemanticCache instances",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement cache naming and prefixing strategies",
            "description": "Add support for namespace prefixing to organize cache keys by content type and optional custom namespace",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create multi-content cache system factory method",
            "description": "Implement a method to create a complete system of content-specific caches with a single factory call",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 30,
        "title": "Create LangChain Integration Utilities",
        "description": "Implement utility functions to simplify integration with LangChain's native caching infrastructure and LCEL chains, focusing on seamless integration with CasinoCacheOrchestrator.",
        "status": "cancelled",
        "dependencies": [
          29
        ],
        "priority": "medium",
        "details": "Create a new file `langchain_integration.py` in the cache directory with the following content:\n\n```python\nfrom typing import Dict, Any, Optional, List, Union, Callable, TypeVar, cast, Awaitable\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda, Runnable, RunnableConfig\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.globals import set_llm_cache\nfrom langchain_core.callbacks import CallbackManagerForChainRun\nimport asyncio\nfrom functools import wraps\n\nfrom .casino_cache import CasinoCache\nfrom .cache_factory import CachingFactory\nfrom .cache_orchestrator import CasinoCacheOrchestrator\n\nT = TypeVar('T')\n\ndef setup_llm_with_cache(llm: LLM, cache: Optional[Union[CasinoCache, CasinoCacheOrchestrator]] = None) -> LLM:\n    \"\"\"Configure an LLM to use the casino cache.\n    \n    Args:\n        llm: The LLM to configure\n        cache: The cache to use or create a new one if None\n        \n    Returns:\n        LLM: The configured LLM\n    \"\"\"\n    if cache is None:\n        cache = CachingFactory.create_memory_cache()\n    \n    llm.cache = cache\n    return llm\n\ndef setup_global_cache(cache: Optional[Union[CasinoCache, CasinoCacheOrchestrator]] = None) -> None:\n    \"\"\"Set up global LLM cache using LangChain's native caching.\n    \n    Args:\n        cache: The cache to use globally or create a new one if None\n    \"\"\"\n    if cache is None:\n        cache = CachingFactory.create_orchestrator_cache()\n    \n    set_llm_cache(cache)\n\ndef with_context_extractor() -> RunnableLambda:\n    \"\"\"Create a RunnableLambda that extracts context for caching from the input.\n    \n    Returns:\n        RunnableLambda: A runnable that extracts context\n    \"\"\"\n    def _extract_context(data: Dict[str, Any]) -> Dict[str, Any]:\n        # Extract context for caching\n        context = {}\n        \n        # Extract query type if available\n        if 'query_type' in data:\n            context['query_type'] = data['query_type']\n        \n        # Extract confidence score if available\n        if 'confidence_score' in data:\n            context['confidence_score'] = data['confidence_score']\n        \n        # Extract expertise level if available\n        if 'expertise_level' in data:\n            context['expertise_level'] = data['expertise_level']\n        \n        # Extract language if available\n        if 'language' in data:\n            context['language'] = data['language']\n            \n        # Extract content type for routing\n        if 'content_type' in data:\n            context['content_type'] = data['content_type']\n        \n        # Add context to data\n        return {**data, 'cache_context': context}\n    \n    return RunnableLambda(_extract_context)\n\ndef create_cached_chain(chain: Runnable, cache: Optional[Union[CasinoCache, CasinoCacheOrchestrator]] = None) -> Runnable:\n    \"\"\"Create a chain with LangChain's native caching.\n    \n    Args:\n        chain: The main processing chain\n        cache: The cache to use or create a new one if None\n        \n    Returns:\n        Runnable: A chain with native LangChain caching integrated\n    \"\"\"\n    if cache is None:\n        cache = CachingFactory.create_orchestrator_cache()\n    \n    # Set up global cache for any LLMs in the chain\n    setup_global_cache(cache)\n    \n    # Add context extraction to the chain\n    return with_context_extractor() | chain.with_cache(cache=cache)\n\ndef cache_middleware() -> Callable[[Runnable], Runnable]:\n    \"\"\"Create middleware that checks cache before executing LLM calls.\n    \n    Returns:\n        Callable: A middleware function that can be applied to a runnable\n    \"\"\"\n    def decorator(runnable: Runnable) -> Runnable:\n        @wraps(runnable.invoke)\n        def invoke_with_cache(self, input: Any, config: Optional[RunnableConfig] = None, **kwargs) -> Any:\n            cache = CachingFactory.get_default_orchestrator()\n            cache_key = cache.generate_key(input)\n            cached_result = cache.get(cache_key)\n            \n            if cached_result is not None:\n                return cached_result\n            \n            result = self.invoke(input, config, **kwargs)\n            cache.set(cache_key, result)\n            return result\n            \n        @wraps(runnable.ainvoke)\n        async def ainvoke_with_cache(self, input: Any, config: Optional[RunnableConfig] = None, **kwargs) -> Any:\n            cache = CachingFactory.get_default_orchestrator()\n            cache_key = cache.generate_key(input)\n            cached_result = await cache.aget(cache_key)\n            \n            if cached_result is not None:\n                return cached_result\n            \n            result = await self.ainvoke(input, config, **kwargs)\n            await cache.aset(cache_key, result)\n            return result\n        \n        runnable.invoke = invoke_with_cache.__get__(runnable)\n        runnable.ainvoke = ainvoke_with_cache.__get__(runnable)\n        return runnable\n    \n    return decorator\n\ndef create_fallback_chain(semantic_cache: CasinoCache, exact_cache: CasinoCache, llm: LLM) -> Runnable:\n    \"\"\"Create a chain with fallback strategy: semantic → exact → LLM.\n    \n    Args:\n        semantic_cache: The semantic cache to try first\n        exact_cache: The exact match cache to try second\n        llm: The LLM to use as final fallback\n        \n    Returns:\n        Runnable: A chain with fallback strategy\n    \"\"\"\n    def try_semantic_cache(input: Dict[str, Any]) -> Optional[str]:\n        cache_key = semantic_cache.generate_key(input)\n        return semantic_cache.get(cache_key)\n    \n    def try_exact_cache(input: Dict[str, Any]) -> Optional[str]:\n        cache_key = exact_cache.generate_key(input)\n        return exact_cache.get(cache_key)\n    \n    def process_with_llm(input: Dict[str, Any]) -> str:\n        result = llm.invoke(input)\n        # Cache the result in both caches\n        semantic_key = semantic_cache.generate_key(input)\n        exact_key = exact_cache.generate_key(input)\n        semantic_cache.set(semantic_key, result)\n        exact_cache.set(exact_key, result)\n        return result\n    \n    semantic_chain = RunnableLambda(try_semantic_cache)\n    exact_chain = RunnableLambda(try_exact_cache)\n    llm_chain = RunnableLambda(process_with_llm)\n    \n    return semantic_chain.with_fallbacks([exact_chain, llm_chain])\n\nasync def warm_cache_for_common_queries(queries: List[str], llm: LLM, cache: Optional[CasinoCacheOrchestrator] = None) -> None:\n    \"\"\"Pre-warm cache with common casino queries.\n    \n    Args:\n        queries: List of common queries to cache\n        llm: The LLM to use for generating responses\n        cache: The cache orchestrator to use or create a new one if None\n    \"\"\"\n    if cache is None:\n        cache = CachingFactory.get_default_orchestrator()\n    \n    async def process_query(query: str) -> None:\n        result = llm.invoke(query)\n        cache_key = cache.generate_key(query)\n        await cache.aset(cache_key, result)\n    \n    await asyncio.gather(*[process_query(query) for query in queries])\n\ndef content_type_router() -> RunnableLambda:\n    \"\"\"Create a router that directs queries to appropriate caches based on content type.\n    \n    Returns:\n        RunnableLambda: A runnable that routes based on content type\n    \"\"\"\n    def _route_by_content_type(data: Dict[str, Any]) -> Dict[str, Any]:\n        content_type = data.get('content_type', 'general')\n        orchestrator = CachingFactory.get_default_orchestrator()\n        \n        # Set the appropriate cache based on content type\n        if content_type == 'game_rules':\n            data['cache'] = orchestrator.get_cache('game_rules_cache')\n        elif content_type == 'promotions':\n            data['cache'] = orchestrator.get_cache('promotions_cache')\n        elif content_type == 'customer_service':\n            data['cache'] = orchestrator.get_cache('customer_service_cache')\n        else:\n            data['cache'] = orchestrator.get_cache('general_cache')\n        \n        return data\n    \n    return RunnableLambda(_route_by_content_type)\n\ndef analytics_collector() -> RunnableLambda:\n    \"\"\"Create a collector that records analytics during LCEL execution.\n    \n    Returns:\n        RunnableLambda: A runnable that collects analytics\n    \"\"\"\n    def _collect_analytics(data: Dict[str, Any], run_manager: Optional[CallbackManagerForChainRun] = None) -> Dict[str, Any]:\n        # Record analytics about the query and response\n        analytics = {\n            'timestamp': data.get('timestamp'),\n            'query_type': data.get('query_type', 'unknown'),\n            'cache_hit': data.get('cache_hit', False),\n            'response_time': data.get('response_time'),\n            'content_type': data.get('content_type', 'general'),\n            'language': data.get('language', 'en')\n        }\n        \n        # Log analytics if a run manager is available\n        if run_manager:\n            run_manager.on_text(f\"Analytics: {analytics}\")\n        \n        # Store analytics in data for later use\n        return {**data, 'analytics': analytics}\n    \n    return RunnableLambda(_collect_analytics)\n```",
        "testStrategy": "Write unit tests for each utility function with the following focus areas:\n\n1. Test integration with LangChain LLMs and LCEL chains\n2. Verify that the global cache setup works correctly with CasinoCacheOrchestrator\n3. Test the context extraction functionality with various input types\n4. Verify that the cached chain correctly passes context through the chain\n5. Test the async cache lookup/update in the cache middleware\n6. Verify the fallback strategy works correctly (semantic → exact → LLM)\n7. Test cache warming utilities with common casino queries\n8. Verify analytics collection during LCEL execution\n9. Test content-type based cache routing within chains\n10. Test integration with Universal RAG Chain to ensure transparent caching\n\nCreate both synchronous and asynchronous test cases to verify all functionality works in both contexts.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement basic LLM cache integration",
            "description": "Create functions for setting up LLM with cache and global cache configuration",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement context extraction for LCEL chains",
            "description": "Create a RunnableLambda that extracts relevant context for caching from input data",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement cached chain creation",
            "description": "Create utility to wrap chains with caching functionality using LangChain's native patterns",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement cache middleware",
            "description": "Create middleware for automatic cache checking before LLM calls with async support",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement fallback strategy chain",
            "description": "Create a chain with semantic → exact → LLM fallback strategy",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement cache warming utilities",
            "description": "Create async function to pre-warm cache with common casino queries",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement analytics collection",
            "description": "Create a RunnableLambda that collects analytics during LCEL execution",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement content-type cache routing",
            "description": "Create a router that directs queries to appropriate caches based on content type",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Write comprehensive tests",
            "description": "Create unit tests for all utility functions with both sync and async test cases",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 31,
        "title": "Implement Unit Tests for TTLStrategy",
        "description": "Create comprehensive unit and integration tests for the TTLStrategy class and its integration with RedisSemanticCache and CasinoCacheOrchestrator.",
        "status": "cancelled",
        "dependencies": [
          26
        ],
        "priority": "medium",
        "details": "## Unit Tests for TTLStrategy\n\nIn test_ttl_strategy.py, implement the following tests:\n\n```python\nimport unittest\nfrom datetime import timedelta\nfrom src.services.cache.ttl_strategy import TTLStrategy\nfrom src.services.cache.exceptions import TTLStrategyError\n\nclass TestTTLStrategy(unittest.TestCase):\n    def setUp(self):\n        self.ttl_strategy = TTLStrategy(default_ttl=24)\n    \n    def test_default_ttl(self):\n        \"\"\"Test that default TTL is used when no query type is provided.\"\"\"\n        ttl = self.ttl_strategy.calculate_ttl()\n        self.assertEqual(ttl, timedelta(hours=24))\n        # Verify TTL in seconds for Redis compatibility\n        self.assertEqual(ttl.total_seconds(), 86400)\n    \n    def test_query_type_ttl(self):\n        \"\"\"Test TTL calculation based on query type.\"\"\"\n        # Test NEWS_UPDATE type\n        ttl = self.ttl_strategy.calculate_ttl(query_type='NEWS_UPDATE')\n        self.assertEqual(ttl, timedelta(hours=2))\n        self.assertEqual(ttl.total_seconds(), 7200)\n        \n        # Test REGULATORY type\n        ttl = self.ttl_strategy.calculate_ttl(query_type='REGULATORY')\n        self.assertEqual(ttl, timedelta(hours=168))\n        self.assertEqual(ttl.total_seconds(), 604800)\n    \n    def test_confidence_score_adjustment(self):\n        \"\"\"Test TTL adjustment based on confidence score.\"\"\"\n        # High confidence should increase TTL\n        ttl = self.ttl_strategy.calculate_ttl(query_type='GENERAL_QUERY', confidence_score=0.9)\n        self.assertGreater(ttl, timedelta(hours=24))\n        \n        # Low confidence should decrease TTL\n        ttl = self.ttl_strategy.calculate_ttl(query_type='GENERAL_QUERY', confidence_score=0.3)\n        self.assertLess(ttl, timedelta(hours=24))\n    \n    def test_expertise_level_adjustment(self):\n        \"\"\"Test TTL adjustment based on expertise level.\"\"\"\n        # Expert users get shorter TTL (fresher data)\n        ttl = self.ttl_strategy.calculate_ttl(query_type='GENERAL_QUERY', expertise_level='EXPERT')\n        self.assertLess(ttl, timedelta(hours=24))\n        \n        # Beginner users get longer TTL\n        ttl = self.ttl_strategy.calculate_ttl(query_type='GENERAL_QUERY', expertise_level='BEGINNER')\n        self.assertGreater(ttl, timedelta(hours=24))\n    \n    def test_combined_factors(self):\n        \"\"\"Test TTL calculation with multiple factors.\"\"\"\n        ttl = self.ttl_strategy.calculate_ttl(\n            query_type='NEWS_UPDATE',\n            confidence_score=0.8,\n            expertise_level='EXPERT',\n            freshness_factor=0.9\n        )\n        # Calculate expected: 2h * 1.2 (confidence) * 0.8 (expert) * 0.9 (freshness)\n        expected = timedelta(hours=2 * 1.2 * 0.8 * 0.9)\n        self.assertAlmostEqual(ttl.total_seconds(), expected.total_seconds(), delta=1)\n    \n    def test_error_handling(self):\n        \"\"\"Test error handling in TTL calculation.\"\"\"\n        # Create a subclass with a broken calculate_ttl method\n        class BrokenTTLStrategy(TTLStrategy):\n            def calculate_ttl(self, **kwargs):\n                raise ValueError(\"Simulated error\")\n        \n        broken_strategy = BrokenTTLStrategy()\n        with self.assertRaises(TTLStrategyError):\n            broken_strategy.calculate_ttl()\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n## Integration Tests with RedisSemanticCache\n\nIn test_redis_semantic_cache_integration.py, implement the following tests:\n\n```python\nimport unittest\nimport asyncio\nimport pytest\nfrom unittest.mock import patch, MagicMock\nfrom src.services.cache.ttl_strategy import TTLStrategy\nfrom src.services.cache.redis_semantic_cache import RedisSemanticCache\nfrom src.services.cache.casino_cache_orchestrator import CasinoCacheOrchestrator\nfrom langchain.schema import Generation\nfrom langchain.cache import RedisCache\n\nclass TestRedisSemanticCacheIntegration(unittest.TestCase):\n    @pytest.fixture(autouse=True)\n    def setup_redis(self):\n        # Setup Redis client mock or real test instance\n        self.redis_client = MagicMock()\n        self.ttl_strategy = TTLStrategy(default_ttl=24)\n        \n        # Create multiple cache instances for different content types\n        self.general_cache = RedisSemanticCache(\n            redis_client=self.redis_client,\n            namespace=\"casino1:general\",\n            ttl_strategy=self.ttl_strategy,\n            distance_threshold=0.2\n        )\n        \n        self.regulatory_cache = RedisSemanticCache(\n            redis_client=self.redis_client,\n            namespace=\"casino1:regulatory\",\n            ttl_strategy=self.ttl_strategy,\n            distance_threshold=0.1  # Stricter for regulatory content\n        )\n        \n        # Create orchestrator with multiple caches\n        self.orchestrator = CasinoCacheOrchestrator(\n            casino_id=\"casino1\",\n            caches={\n                \"general\": self.general_cache,\n                \"regulatory\": self.regulatory_cache\n            }\n        )\n    \n    @pytest.mark.asyncio\n    async def test_async_lookup_update_clear(self):\n        \"\"\"Test async operations on RedisSemanticCache.\"\"\"\n        # Mock Redis responses\n        self.redis_client.aexists.return_value = False\n        self.redis_client.aset.return_value = True\n        \n        # Test async lookup (cache miss)\n        result = await self.general_cache.alookup(\"What are the casino hours?\")\n        self.assertIsNone(result)\n        \n        # Test async update\n        gen = Generation(text=\"The casino is open from 9am to 2am daily.\")\n        await self.general_cache.aupdate(\"What are the casino hours?\", gen)\n        \n        # Verify Redis operations were called with correct TTL in seconds\n        self.redis_client.aset.assert_called_once()\n        # Extract the TTL argument and verify it's in seconds\n        args, kwargs = self.redis_client.aset.call_args\n        self.assertIsInstance(kwargs.get('ex', None), int)\n        \n        # Test async clear\n        await self.general_cache.aclear()\n        self.redis_client.adelete.assert_called_once()\n    \n    def test_content_type_routing(self):\n        \"\"\"Test that queries are routed to the correct cache based on content type.\"\"\"\n        # Setup test data\n        general_query = \"What are the casino hours?\"\n        regulatory_query = \"What are the regulatory requirements for jackpot payouts?\"\n        \n        # Mock cache lookup methods\n        with patch.object(self.general_cache, 'lookup') as mock_general_lookup, \\\n             patch.object(self.regulatory_cache, 'lookup') as mock_regulatory_lookup:\n            \n            # Test general query routing\n            self.orchestrator.lookup(general_query, content_type=\"general\")\n            mock_general_lookup.assert_called_once_with(general_query)\n            mock_regulatory_lookup.assert_not_called()\n            \n            # Reset mocks\n            mock_general_lookup.reset_mock()\n            mock_regulatory_lookup.reset_mock()\n            \n            # Test regulatory query routing\n            self.orchestrator.lookup(regulatory_query, content_type=\"regulatory\")\n            mock_regulatory_lookup.assert_called_once_with(regulatory_query)\n            mock_general_lookup.assert_not_called()\n    \n    def test_cache_isolation(self):\n        \"\"\"Test that caches for different casinos and content types are isolated.\"\"\"\n        # Create a second casino cache with the same Redis client\n        casino2_cache = RedisSemanticCache(\n            redis_client=self.redis_client,\n            namespace=\"casino2:general\",\n            ttl_strategy=self.ttl_strategy,\n            distance_threshold=0.2\n        )\n        \n        # Mock Redis operations\n        with patch.object(self.redis_client, 'set') as mock_set, \\\n             patch.object(self.redis_client, 'get') as mock_get:\n            \n            # Update both caches with the same query but different responses\n            query = \"What are the casino hours?\"\n            casino1_response = Generation(text=\"Casino 1 is open 24/7.\")\n            casino2_response = Generation(text=\"Casino 2 is open from 10am to midnight.\")\n            \n            self.general_cache.update(query, casino1_response)\n            casino2_cache.update(query, casino2_response)\n            \n            # Verify different namespaces were used in Redis operations\n            calls = mock_set.call_args_list\n            self.assertEqual(len(calls), 2)\n            self.assertIn(\"casino1:general\", str(calls[0]))\n            self.assertIn(\"casino2:general\", str(calls[1]))\n    \n    @pytest.mark.asyncio\n    async def test_fallback_strategy(self):\n        \"\"\"Test the fallback strategy from semantic → exact → LLM.\"\"\"\n        query = \"What are the casino hours?\"\n        similar_query = \"When is the casino open?\"\n        exact_response = Generation(text=\"The casino is open from 9am to 2am.\")\n        \n        # Setup mocks for the orchestrator's caches\n        with patch.object(self.general_cache, 'lookup') as mock_semantic_lookup, \\\n             patch.object(self.general_cache, 'lookup_exact') as mock_exact_lookup, \\\n             patch.object(self.orchestrator, '_generate_from_llm') as mock_llm_generate:\n            \n            # Scenario 1: Semantic hit\n            mock_semantic_lookup.return_value = exact_response\n            mock_exact_lookup.return_value = None\n            \n            result = self.orchestrator.lookup(similar_query, content_type=\"general\")\n            self.assertEqual(result, exact_response)\n            mock_semantic_lookup.assert_called_once()\n            mock_exact_lookup.assert_not_called()\n            mock_llm_generate.assert_not_called()\n            \n            # Reset mocks\n            mock_semantic_lookup.reset_mock()\n            mock_exact_lookup.reset_mock()\n            mock_llm_generate.reset_mock()\n            \n            # Scenario 2: Semantic miss, exact hit\n            mock_semantic_lookup.return_value = None\n            mock_exact_lookup.return_value = exact_response\n            \n            result = self.orchestrator.lookup(query, content_type=\"general\")\n            self.assertEqual(result, exact_response)\n            mock_semantic_lookup.assert_called_once()\n            mock_exact_lookup.assert_called_once()\n            mock_llm_generate.assert_not_called()\n            \n            # Reset mocks\n            mock_semantic_lookup.reset_mock()\n            mock_exact_lookup.reset_mock()\n            mock_llm_generate.reset_mock()\n            \n            # Scenario 3: Both miss, fallback to LLM\n            mock_semantic_lookup.return_value = None\n            mock_exact_lookup.return_value = None\n            mock_llm_generate.return_value = exact_response\n            \n            result = self.orchestrator.lookup(\"Are there any special events today?\", content_type=\"general\")\n            self.assertEqual(result, exact_response)\n            mock_semantic_lookup.assert_called_once()\n            mock_exact_lookup.assert_called_once()\n            mock_llm_generate.assert_called_once()\n    \n    @pytest.mark.asyncio\n    async def test_error_handling(self):\n        \"\"\"Test error handling with Redis connection failures.\"\"\"\n        # Mock Redis client to raise an exception\n        self.redis_client.aexists.side_effect = Exception(\"Redis connection error\")\n        \n        # Test that the cache gracefully handles Redis errors\n        try:\n            result = await self.general_cache.alookup(\"What are the casino hours?\")\n            # Should return None on error rather than raising exception\n            self.assertIsNone(result)\n        except Exception as e:\n            self.fail(f\"RedisSemanticCache should handle Redis errors gracefully, but raised: {e}\")\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n## LCEL Integration Tests\n\nIn test_lcel_integration.py, implement the following tests:\n\n```python\nimport unittest\nimport pytest\nfrom unittest.mock import patch, MagicMock\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms.fake import FakeListLLM\nfrom src.services.cache.redis_semantic_cache import RedisSemanticCache\nfrom src.services.cache.casino_cache_orchestrator import CasinoCacheOrchestrator\nfrom src.services.cache.ttl_strategy import TTLStrategy\n\nclass TestLCELIntegration(unittest.TestCase):\n    def setUp(self):\n        # Setup Redis client mock\n        self.redis_client = MagicMock()\n        self.ttl_strategy = TTLStrategy(default_ttl=24)\n        \n        # Create cache instances\n        self.cache = RedisSemanticCache(\n            redis_client=self.redis_client,\n            namespace=\"test:lcel\",\n            ttl_strategy=self.ttl_strategy,\n            distance_threshold=0.2\n        )\n        \n        # Create orchestrator\n        self.orchestrator = CasinoCacheOrchestrator(\n            casino_id=\"test_casino\",\n            caches={\"general\": self.cache}\n        )\n        \n        # Create a fake LLM for testing\n        self.responses = [\"This is a cached response.\"]\n        self.llm = FakeListLLM(responses=self.responses)\n        \n        # Create a simple prompt template\n        self.prompt = PromptTemplate(\n            input_variables=[\"question\"],\n            template=\"Question: {question}\\nAnswer:\"\n        )\n    \n    def test_set_llm_cache(self):\n        \"\"\"Test integration with LangChain's set_llm_cache().\"\"\"\n        from langchain.globals import set_llm_cache\n        \n        # Set the cache\n        set_llm_cache(self.cache)\n        \n        # Create a chain\n        chain = LLMChain(llm=self.llm, prompt=self.prompt)\n        \n        # First call should miss cache and use LLM\n        result1 = chain.run(question=\"What is the meaning of life?\")\n        self.assertEqual(result1, self.responses[0])\n        \n        # Mock that the cache now has the result\n        self.cache.lookup = MagicMock(return_value=self.responses[0])\n        \n        # Second call should hit cache\n        result2 = chain.run(question=\"What is the meaning of life?\")\n        self.assertEqual(result2, self.responses[0])\n        self.cache.lookup.assert_called_once()\n    \n    def test_with_cache(self):\n        \"\"\"Test integration with the .with_cache() method in LCEL chains.\"\"\"\n        # Create a chain with cache\n        chain = (\n            self.prompt \n            | self.llm.with_cache(cache=self.cache)\n        )\n        \n        # First call should miss cache and use LLM\n        result1 = chain.invoke({\"question\": \"What is the meaning of life?\"})\n        self.assertEqual(result1, self.responses[0])\n        \n        # Mock that the cache now has the result\n        with patch.object(self.cache, 'lookup', return_value=self.responses[0]) as mock_lookup:\n            # Second call should hit cache\n            result2 = chain.invoke({\"question\": \"What is the meaning of life?\"})\n            self.assertEqual(result2, self.responses[0])\n            mock_lookup.assert_called_once()\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n## Performance Tests\n\nIn test_performance.py, implement the following tests:\n\n```python\nimport unittest\nimport time\nimport asyncio\nimport pytest\nimport redis.asyncio as redis\nfrom src.services.cache.ttl_strategy import TTLStrategy\nfrom src.services.cache.redis_semantic_cache import RedisSemanticCache\nfrom langchain.schema import Generation\n\n@pytest.mark.performance\nclass TestRedisPerformance(unittest.TestCase):\n    @classmethod\n    async def setUpClass(cls):\n        # Connect to a real Redis instance for performance testing\n        # Note: This requires a running Redis server\n        cls.redis_client = await redis.Redis(host='localhost', port=6379, db=0)\n        cls.ttl_strategy = TTLStrategy(default_ttl=24)\n        \n        # Create cache with shared Redis client\n        cls.cache = RedisSemanticCache(\n            redis_client=cls.redis_client,\n            namespace=\"perf_test\",\n            ttl_strategy=cls.ttl_strategy,\n            distance_threshold=0.2\n        )\n        \n        # Clear any existing test data\n        await cls.redis_client.flushdb()\n    \n    @classmethod\n    async def tearDownClass(cls):\n        # Clean up after tests\n        await cls.redis_client.flushdb()\n        await cls.redis_client.close()\n    \n    @pytest.mark.asyncio\n    async def test_cache_performance(self):\n        \"\"\"Test performance of cache operations.\"\"\"\n        # Generate test data\n        num_entries = 100\n        test_data = [\n            (f\"Question {i}\", Generation(text=f\"Answer {i}\"))\n            for i in range(num_entries)\n        ]\n        \n        # Measure time to populate cache\n        start_time = time.time()\n        for question, answer in test_data:\n            await self.cache.aupdate(question, answer)\n        populate_time = time.time() - start_time\n        print(f\"Time to populate {num_entries} entries: {populate_time:.4f} seconds\")\n        \n        # Measure time for cache hits\n        start_time = time.time()\n        for question, _ in test_data:\n            result = await self.cache.alookup(question)\n            self.assertIsNotNone(result)  # Should be a cache hit\n        hit_time = time.time() - start_time\n        print(f\"Time for {num_entries} cache hits: {hit_time:.4f} seconds\")\n        \n        # Measure time for semantic lookups (similar but not exact questions)\n        similar_questions = [f\"Similar to question {i}\" for i in range(num_entries)]\n        start_time = time.time()\n        hit_count = 0\n        for question in similar_questions:\n            result = await self.cache.alookup(question)\n            if result is not None:\n                hit_count += 1\n        semantic_time = time.time() - start_time\n        print(f\"Time for {num_entries} semantic lookups: {semantic_time:.4f} seconds\")\n        print(f\"Semantic hit rate: {hit_count/num_entries:.2%}\")\n        \n        # Test shared Redis client efficiency\n        # Create multiple caches with the same Redis client\n        caches = [\n            RedisSemanticCache(\n                redis_client=self.redis_client,\n                namespace=f\"perf_test_{i}\",\n                ttl_strategy=self.ttl_strategy,\n                distance_threshold=0.2\n            )\n            for i in range(5)\n        ]\n        \n        # Measure time for parallel operations across multiple caches\n        start_time = time.time()\n        tasks = []\n        for i, cache in enumerate(caches):\n            for j in range(20):  # 20 operations per cache\n                question = f\"Cache {i} Question {j}\"\n                answer = Generation(text=f\"Cache {i} Answer {j}\")\n                tasks.append(cache.aupdate(question, answer))\n        \n        await asyncio.gather(*tasks)\n        parallel_time = time.time() - start_time\n        print(f\"Time for 100 parallel operations across 5 caches: {parallel_time:.4f} seconds\")\n\nif __name__ == '__main__':\n    unittest.main()\n```",
        "testStrategy": "1. **Unit Tests**: Run the unit tests for TTLStrategy with pytest to verify TTL calculations and error handling.\n\n2. **Integration Tests**: \n   - Test the integration between TTLStrategy, RedisSemanticCache, and CasinoCacheOrchestrator\n   - Verify async operations (alookup, aupdate, aclear) work correctly\n   - Test content-type routing with different distance thresholds\n   - Validate TTL values are properly converted to seconds for Redis\n   - Verify cache isolation between different casinos and content types\n   - Test fallback strategy (semantic → exact → LLM)\n\n3. **LCEL Integration Tests**:\n   - Verify integration with LangChain's set_llm_cache()\n   - Test the .with_cache() method in LCEL chains\n\n4. **Performance Tests**:\n   - Run with a real Redis instance to measure actual performance\n   - Test shared Redis client efficiency\n   - Measure cache hit/miss rates and response times\n   - Test parallel operations across multiple caches\n\n5. **Error Handling Tests**:\n   - Verify graceful handling of Redis connection failures\n   - Test recovery mechanisms\n\nRun tests with appropriate markers to separate unit, integration, and performance tests:\n```\npytest test_ttl_strategy.py  # Unit tests\npytest test_redis_semantic_cache_integration.py  # Integration tests\npytest test_lcel_integration.py  # LCEL integration tests\npytest test_performance.py -m performance  # Performance tests (requires Redis)\n```",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement TTLStrategy unit tests",
            "description": "Create the basic unit tests for TTLStrategy class to verify TTL calculations and error handling.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add TTL seconds validation tests",
            "description": "Add tests to verify TTL values are properly converted to seconds for Redis compatibility.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement RedisSemanticCache integration tests",
            "description": "Create integration tests for RedisSemanticCache with CasinoCacheOrchestrator, focusing on async operations and content-type routing.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement cache isolation tests",
            "description": "Create tests to verify cache isolation between different casinos and content types.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement fallback strategy tests",
            "description": "Test the fallback strategy from semantic → exact → LLM.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement error handling tests",
            "description": "Create tests for graceful handling of Redis connection failures.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement LCEL integration tests",
            "description": "Create tests for integration with LangChain's set_llm_cache() and .with_cache() method.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement performance tests",
            "description": "Create performance tests with a real Redis instance to measure cache operations efficiency.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 32,
        "title": "Implement Unit Tests for SemanticKeyGenerator",
        "description": "Create comprehensive unit tests for the SemanticKeyGenerator class to ensure correct namespace generation and casino name extraction, with support for RedisSemanticCache integration and LCEL compatibility.",
        "status": "cancelled",
        "dependencies": [
          27
        ],
        "priority": "medium",
        "details": "In test_semantic_key_generator.py, implement the following tests:\n\n```python\nimport unittest\nfrom unittest.mock import MagicMock, patch, AsyncMock\nimport re\nimport pytest\nfrom src.services.cache.semantic_key_generator import SemanticKeyGenerator\nfrom src.services.cache.exceptions import SemanticKeyGenerationError\nfrom src.services.cache.redis_semantic_cache import RedisSemanticCache\n\nclass TestSemanticKeyGenerator(unittest.TestCase):\n    def setUp(self):\n        self.mock_embeddings = MagicMock()\n        self.key_generator = SemanticKeyGenerator(embeddings=self.mock_embeddings, namespace='test_cache')\n    \n    def test_casino_name_extraction(self):\n        \"\"\"Test extraction of casino names from queries.\"\"\"\n        # Test 888 Casino extraction\n        query = \"What are the latest promotions at 888Casino?\"\n        casino = self.key_generator.extract_casino_name(query)\n        self.assertEqual(casino, \"888casino\")\n        \n        # Test with space\n        query = \"Tell me about 888 Casino bonuses\"\n        casino = self.key_generator.extract_casino_name(query)\n        self.assertEqual(casino, \"888casino\")\n        \n        # Test Betway extraction\n        query = \"What games are available at Betway?\"\n        casino = self.key_generator.extract_casino_name(query)\n        self.assertEqual(casino, \"betway\")\n        \n        # Test no casino in query\n        query = \"What are the general gambling regulations?\"\n        casino = self.key_generator.extract_casino_name(query)\n        self.assertIsNone(casino)\n    \n    def test_basic_namespace_generation(self):\n        \"\"\"Test basic namespace generation without context.\"\"\"\n        query = \"What are the latest promotions?\"\n        namespace = self.key_generator.generate_namespace(query)\n        \n        # Namespace should start with prefix\n        self.assertTrue(namespace.startswith('test_cache'))\n        \n        # Namespace should not contain casino name\n        self.assertNotIn('casino=', namespace)\n    \n    def test_namespace_with_casino(self):\n        \"\"\"Test namespace generation with casino name in query.\"\"\"\n        query = \"What are the latest promotions at 888Casino?\"\n        namespace = self.key_generator.generate_namespace(query)\n        \n        # Namespace should contain casino name\n        self.assertIn('casino=888casino', namespace)\n    \n    def test_namespace_with_context(self):\n        \"\"\"Test namespace generation with additional context.\"\"\"\n        query = \"What are the latest promotions?\"\n        context = {\n            'query_type': 'PROMOTION_QUERY',\n            'language': 'en',\n            'expertise_level': 'BEGINNER'\n        }\n        \n        namespace = self.key_generator.generate_namespace(query, context)\n        \n        # Namespace should contain context elements\n        self.assertIn('type=PROMOTION_QUERY', namespace)\n        self.assertIn('language=en', namespace)\n        self.assertIn('expertise_level=BEGINNER', namespace)\n    \n    def test_namespace_uniqueness(self):\n        \"\"\"Test that different queries produce different namespaces.\"\"\"\n        query1 = \"What are the latest promotions at 888Casino?\"\n        query2 = \"What are the latest promotions at Betway?\"\n        \n        namespace1 = self.key_generator.generate_namespace(query1)\n        namespace2 = self.key_generator.generate_namespace(query2)\n        \n        self.assertNotEqual(namespace1, namespace2)\n    \n    def test_prepare_query_for_embedding(self):\n        \"\"\"Test query preparation for embedding.\"\"\"\n        # Query with casino name\n        query = \"What are the latest promotions at 888Casino?\"\n        prepared = self.key_generator.prepare_query_for_embedding(query)\n        \n        # Casino name should be removed\n        self.assertNotIn('888Casino', prepared)\n        self.assertEqual(prepared, \"What are the latest promotions at ?\")\n        \n        # Query with multiple casino names\n        query = \"Compare bonuses between 888Casino and Betway\"\n        prepared = self.key_generator.prepare_query_for_embedding(query)\n        \n        # Both casino names should be removed\n        self.assertNotIn('888Casino', prepared)\n        self.assertNotIn('Betway', prepared)\n        self.assertEqual(prepared, \"Compare bonuses between and\")\n    \n    def test_error_handling(self):\n        \"\"\"Test error handling in namespace generation.\"\"\"\n        # Create a subclass with a broken generate_namespace method\n        class BrokenKeyGenerator(SemanticKeyGenerator):\n            def generate_namespace(self, query, context=None):\n                raise ValueError(\"Simulated error\")\n        \n        broken_generator = BrokenKeyGenerator()\n        with self.assertRaises(SemanticKeyGenerationError):\n            broken_generator.generate_namespace(\"test query\")\n\n    def test_content_type_routing(self):\n        \"\"\"Test routing to different caches based on content type.\"\"\"\n        query = \"What are the latest promotions at 888Casino?\"\n        context = {'content_type': 'promotion'}\n        namespace = self.key_generator.generate_namespace(query, context)\n        self.assertIn('content_type=promotion', namespace)\n        \n        query = \"How do I play blackjack at 888Casino?\"\n        context = {'content_type': 'game_rules'}\n        namespace = self.key_generator.generate_namespace(query, context)\n        self.assertIn('content_type=game_rules', namespace)\n\n    def test_casino_specific_embedding_model_support(self):\n        \"\"\"Test support for casino-specific embedding models.\"\"\"\n        # Setup with casino-specific embedding model\n        casino_specific_embeddings = MagicMock()\n        key_generator = SemanticKeyGenerator(\n            embeddings=casino_specific_embeddings, \n            namespace='test_cache',\n            casino_specific_models={'888casino': 'custom_888_model'}\n        )\n        \n        query = \"What are the latest promotions at 888Casino?\"\n        namespace = key_generator.generate_namespace(query)\n        self.assertIn('model=custom_888_model', namespace)\n\n\n@pytest.mark.asyncio\nclass TestRedisSemanticCacheIntegration:\n    async def setup_async(self):\n        self.mock_embeddings = MagicMock()\n        self.mock_embeddings.embed_query = AsyncMock(return_value=[0.1, 0.2, 0.3])\n        self.key_generator = SemanticKeyGenerator(embeddings=self.mock_embeddings, namespace='test_cache')\n        self.redis_cache = RedisSemanticCache(embeddings=self.mock_embeddings, key_generator=self.key_generator)\n    \n    async def test_alookup(self):\n        \"\"\"Test async lookup in Redis cache.\"\"\"\n        await self.setup_async()\n        self.redis_cache.alookup = AsyncMock(return_value=\"cached_result\")\n        \n        result = await self.redis_cache.alookup(\"What are the latest promotions at 888Casino?\")\n        self.assertEqual(result, \"cached_result\")\n        self.redis_cache.alookup.assert_called_once()\n    \n    async def test_aupdate(self):\n        \"\"\"Test async update in Redis cache.\"\"\"\n        await self.setup_async()\n        self.redis_cache.aupdate = AsyncMock()\n        \n        await self.redis_cache.aupdate(\"What are the latest promotions at 888Casino?\", \"new_result\")\n        self.redis_cache.aupdate.assert_called_once()\n    \n    async def test_aclear(self):\n        \"\"\"Test async clear in Redis cache.\"\"\"\n        await self.setup_async()\n        self.redis_cache.aclear = AsyncMock()\n        \n        await self.redis_cache.aclear()\n        self.redis_cache.aclear.assert_called_once()\n    \n    async def test_fallback_strategy(self):\n        \"\"\"Test fallback strategy (semantic → exact → LLM).\"\"\"\n        await self.setup_async()\n        # Mock semantic lookup to return None (cache miss)\n        self.redis_cache._semantic_lookup = AsyncMock(return_value=None)\n        # Mock exact lookup to return a result\n        self.redis_cache._exact_lookup = AsyncMock(return_value=\"exact_match_result\")\n        \n        result = await self.redis_cache.alookup(\"What are the latest promotions at 888Casino?\")\n        self.assertEqual(result, \"exact_match_result\")\n        self.redis_cache._semantic_lookup.assert_called_once()\n        self.redis_cache._exact_lookup.assert_called_once()\n    \n    async def test_cache_warming(self):\n        \"\"\"Test cache warming for common casino queries.\"\"\"\n        await self.setup_async()\n        self.redis_cache.warm_cache = AsyncMock()\n        common_queries = [\n            \"What are the latest promotions at 888Casino?\",\n            \"How do I withdraw money from Betway?\"\n        ]\n        \n        await self.redis_cache.warm_cache(common_queries)\n        self.redis_cache.warm_cache.assert_called_once_with(common_queries)\n    \n    async def test_lcel_integration(self):\n        \"\"\"Test LCEL integration with set_llm_cache and with_cache.\"\"\"\n        await self.setup_async()\n        \n        # Mock the langchain modules\n        with patch('langchain.globals.set_llm_cache') as mock_set_cache, \\\n             patch('langchain.chains.base.Chain.with_cache') as mock_with_cache:\n            \n            # Test setting global cache\n            self.redis_cache.set_as_global_cache()\n            mock_set_cache.assert_called_once_with(self.redis_cache)\n            \n            # Test with_cache method\n            mock_chain = MagicMock()\n            self.redis_cache.apply_to_chain(mock_chain)\n            mock_with_cache.assert_called_once()\n    \n    async def test_cache_analytics(self):\n        \"\"\"Test cache analytics and monitoring.\"\"\"\n        await self.setup_async()\n        self.redis_cache.get_analytics = MagicMock(return_value={\n            'hit_rate': 0.75,\n            'miss_rate': 0.25,\n            'avg_lookup_time': 0.05,\n            'total_entries': 100\n        })\n        \n        analytics = self.redis_cache.get_analytics()\n        self.assertEqual(analytics['hit_rate'], 0.75)\n        self.assertEqual(analytics['miss_rate'], 0.25)\n        self.assertEqual(analytics['avg_lookup_time'], 0.05)\n        self.assertEqual(analytics['total_entries'], 100)\n\nif __name__ == '__main__':\n    unittest.main()\n```",
        "testStrategy": "Run the unit tests with pytest or unittest, using pytest for the async tests. Verify that all tests pass. Check test coverage to ensure all code paths in SemanticKeyGenerator and RedisSemanticCache are tested. Verify that casino name extraction works correctly for all supported casino patterns. Ensure that namespace generation produces consistent and unique namespaces. Test that query preparation for embedding correctly removes casino-specific terms. Verify the async methods (alookup, aupdate, aclear) work correctly with Redis. Test the LCEL integration with set_llm_cache() and .with_cache(). Verify multi-cache routing based on query analysis. Test cache warming for common casino queries. Verify fallback strategies (semantic → exact → LLM). Check cache analytics and monitoring functionality. Test support for casino-specific embeddings models.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement basic SemanticKeyGenerator tests",
            "description": "Implement tests for casino name extraction, namespace generation, and error handling",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement RedisSemanticCache async method tests",
            "description": "Create tests for alookup, aupdate, and aclear async methods",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement LCEL integration tests",
            "description": "Test integration with LCEL chains using set_llm_cache() and .with_cache()",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement multi-cache routing tests",
            "description": "Test routing to different caches based on content type and query analysis",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement cache warming tests",
            "description": "Test cache warming functionality for common casino queries",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement fallback strategy tests",
            "description": "Test fallback strategies from semantic to exact to LLM",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement cache analytics tests",
            "description": "Test cache analytics and monitoring functionality",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement casino-specific embedding model tests",
            "description": "Test support for casino-specific embeddings models",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 33,
        "title": "Implement Unit Tests for CasinoCache",
        "description": "Create comprehensive unit tests for the CasinoCache class to ensure correct integration with LangChain's native caching infrastructure, including RedisSemanticCache and LCEL chains.",
        "status": "cancelled",
        "dependencies": [
          28
        ],
        "priority": "medium",
        "details": "In test_casino_cache.py, implement the following tests:\n\n```python\nimport unittest\nfrom unittest.mock import MagicMock, patch\nfrom datetime import timedelta\nimport asyncio\n\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.caches.in_memory import InMemoryCache\nfrom langchain_redis import RedisSemanticCache\nfrom langchain_core.language_models import BaseLLM\nfrom langchain_core.runnables import RunnableSequence\n\nfrom src.services.cache.casino_cache import CasinoCache\nfrom src.services.cache.casino_cache_orchestrator import CasinoCacheOrchestrator\nfrom src.services.cache.semantic_key_generator import SemanticKeyGenerator\nfrom src.services.cache.ttl_strategy import TTLStrategy\nfrom src.services.cache.exceptions import CasinoCacheError\nfrom src.chains.universal_rag_chain import UniversalRAGChain\n\nclass TestCasinoCache(unittest.TestCase):\n    def setUp(self):\n        self.mock_underlying_cache = MagicMock(spec=BaseCache)\n        self.mock_key_generator = MagicMock(spec=SemanticKeyGenerator)\n        self.mock_ttl_strategy = MagicMock(spec=TTLStrategy)\n        \n        # Configure mocks\n        self.mock_key_generator.generate_namespace.return_value = 'test_namespace'\n        self.mock_key_generator.prepare_query_for_embedding.return_value = 'prepared_query'\n        self.mock_ttl_strategy.calculate_ttl.return_value = timedelta(hours=24)\n        \n        self.cache = CasinoCache(\n            underlying_cache=self.mock_underlying_cache,\n            key_generator=self.mock_key_generator,\n            ttl_strategy=self.mock_ttl_strategy\n        )\n    \n    def test_lookup(self):\n        \"\"\"Test cache lookup operation.\"\"\"\n        # Configure mock to return a cached result\n        self.mock_underlying_cache.lookup.return_value = \"cached result\"\n        \n        # Perform lookup\n        result = self.cache.lookup(\"test query\", \"test llm\", {\"query_type\": \"TEST\"})\n        \n        # Verify result\n        self.assertEqual(result, \"cached result\")\n        \n        # Verify key generator was called correctly\n        self.mock_key_generator.generate_namespace.assert_called_once_with(\n            \"test query\", {\"query_type\": \"TEST\"}\n        )\n        \n        # Verify underlying cache was called correctly\n        self.mock_underlying_cache.lookup.assert_called_once()\n    \n    def test_lookup_with_semantic_cache(self):\n        \"\"\"Test lookup with a semantic cache.\"\"\"\n        # Create a mock semantic cache with similarity_threshold attribute\n        mock_semantic_cache = MagicMock(spec=BaseCache)\n        mock_semantic_cache.similarity_threshold = 0.8\n        \n        # Create cache with semantic underlying cache\n        cache = CasinoCache(\n            underlying_cache=mock_semantic_cache,\n            key_generator=self.mock_key_generator,\n            ttl_strategy=self.mock_ttl_strategy\n        )\n        \n        # Configure mock to return a cached result\n        mock_semantic_cache.lookup.return_value = \"cached result\"\n        \n        # Perform lookup\n        result = cache.lookup(\"test query\", \"test llm\")\n        \n        # Verify result\n        self.assertEqual(result, \"cached result\")\n        \n        # Verify query was prepared for embedding\n        self.mock_key_generator.prepare_query_for_embedding.assert_called_once_with(\"test query\")\n    \n    def test_update(self):\n        \"\"\"Test cache update operation.\"\"\"\n        # Perform update\n        self.cache.update(\n            \"test query\", \"test llm\", \"test result\", {\"query_type\": \"TEST\"}\n        )\n        \n        # Verify key generator was called correctly\n        self.mock_key_generator.generate_namespace.assert_called_once_with(\n            \"test query\", {\"query_type\": \"TEST\"}\n        )\n        \n        # Verify TTL strategy was called correctly\n        self.mock_ttl_strategy.calculate_ttl.assert_called_once_with(\n            query_type=\"TEST\", confidence_score=None, expertise_level=None\n        )\n        \n        # Verify underlying cache was called correctly\n        self.mock_underlying_cache.update.assert_called_once()\n    \n    def test_update_with_ttl_support(self):\n        \"\"\"Test update with a cache that supports TTL.\"\"\"\n        # Create a mock cache with update_with_ttl method\n        mock_ttl_cache = MagicMock(spec=BaseCache)\n        mock_ttl_cache.update_with_ttl = MagicMock()\n        \n        # Create cache with TTL-supporting underlying cache\n        cache = CasinoCache(\n            underlying_cache=mock_ttl_cache,\n            key_generator=self.mock_key_generator,\n            ttl_strategy=self.mock_ttl_strategy\n        )\n        \n        # Perform update\n        cache.update(\n            \"test query\", \"test llm\", \"test result\", {\"query_type\": \"TEST\"}\n        )\n        \n        # Verify update_with_ttl was called instead of update\n        mock_ttl_cache.update_with_ttl.assert_called_once()\n        mock_ttl_cache.update.assert_not_called()\n    \n    def test_clear(self):\n        \"\"\"Test cache clear operation.\"\"\"\n        # Perform clear\n        self.cache.clear(namespace=\"test\")\n        \n        # Verify underlying cache was called correctly\n        self.mock_underlying_cache.clear.assert_called_once_with(namespace=\"test\")\n    \n    def test_error_handling_lookup(self):\n        \"\"\"Test error handling in lookup operation.\"\"\"\n        # Configure mock to raise an exception\n        self.mock_underlying_cache.lookup.side_effect = ValueError(\"Simulated error\")\n        \n        # Verify exception is caught and wrapped\n        with self.assertRaises(CasinoCacheError):\n            self.cache.lookup(\"test query\", \"test llm\")\n    \n    def test_error_handling_update(self):\n        \"\"\"Test error handling in update operation.\"\"\"\n        # Configure mock to raise an exception\n        self.mock_underlying_cache.update.side_effect = ValueError(\"Simulated error\")\n        \n        # Verify exception is caught and wrapped\n        with self.assertRaises(CasinoCacheError):\n            self.cache.update(\"test query\", \"test llm\", \"test result\")\n    \n    def test_integration_with_real_cache(self):\n        \"\"\"Test integration with a real InMemoryCache.\"\"\"\n        # Create cache with real InMemoryCache\n        real_cache = CasinoCache(\n            underlying_cache=InMemoryCache(),\n            key_generator=SemanticKeyGenerator(),\n            ttl_strategy=TTLStrategy()\n        )\n        \n        # Test update and lookup\n        real_cache.update(\"test query\", \"test llm\", \"test result\")\n        result = real_cache.lookup(\"test query\", \"test llm\")\n        \n        # Verify result\n        self.assertEqual(result, \"test result\")\n\n\nclass TestCasinoCacheOrchestrator(unittest.TestCase):\n    @patch('redis.Redis')\n    def setUp(self, mock_redis):\n        # Set up mock Redis client\n        self.mock_redis = mock_redis.return_value\n        \n        # Create specialized caches for different content types\n        self.general_cache = MagicMock(spec=CasinoCache)\n        self.game_rules_cache = MagicMock(spec=CasinoCache)\n        self.promotions_cache = MagicMock(spec=CasinoCache)\n        \n        # Create orchestrator with specialized caches\n        self.orchestrator = CasinoCacheOrchestrator(\n            default_cache=self.general_cache,\n            specialized_caches={\n                'game_rules': self.game_rules_cache,\n                'promotions': self.promotions_cache\n            }\n        )\n        \n        # Mock LLM for chain testing\n        self.mock_llm = MagicMock(spec=BaseLLM)\n        self.mock_llm.invoke.return_value = \"LLM response\"\n        self.mock_llm.ainvoke = AsyncMock(return_value=\"Async LLM response\")\n        \n        # Create a mock chain for testing\n        self.mock_chain = MagicMock(spec=RunnableSequence)\n        self.mock_chain.invoke.return_value = \"Chain result\"\n        self.mock_chain.ainvoke = AsyncMock(return_value=\"Async chain result\")\n        self.mock_chain.with_cache = MagicMock(return_value=self.mock_chain)\n    \n    def test_content_type_routing(self):\n        \"\"\"Test routing queries to specialized caches based on content type.\"\"\"\n        # Test game rules query\n        self.orchestrator.lookup(\"What are the rules for blackjack?\", \"test_llm\", {\"content_type\": \"game_rules\"})\n        self.game_rules_cache.lookup.assert_called_once()\n        self.general_cache.lookup.assert_not_called()\n        \n        # Reset mocks\n        self.game_rules_cache.lookup.reset_mock()\n        \n        # Test promotions query\n        self.orchestrator.lookup(\"What promotions are available?\", \"test_llm\", {\"content_type\": \"promotions\"})\n        self.promotions_cache.lookup.assert_called_once()\n        \n        # Test default routing\n        self.orchestrator.lookup(\"General question\", \"test_llm\", {})\n        self.general_cache.lookup.assert_called_once()\n    \n    def test_fallback_strategy(self):\n        \"\"\"Test fallback from semantic to exact to LLM.\"\"\"\n        # Configure caches to simulate cache misses\n        self.game_rules_cache.lookup.return_value = None\n        self.general_cache.lookup.return_value = None\n        \n        # Create orchestrator with fallback strategy\n        orchestrator = CasinoCacheOrchestrator(\n            default_cache=self.general_cache,\n            specialized_caches={'game_rules': self.game_rules_cache},\n            enable_fallback=True\n        )\n        \n        # Test fallback\n        result = orchestrator.lookup(\"What are the rules for blackjack?\", \"test_llm\", {\"content_type\": \"game_rules\"})\n        \n        # Verify specialized cache was tried first\n        self.game_rules_cache.lookup.assert_called_once()\n        \n        # Verify fallback to general cache\n        self.general_cache.lookup.assert_called_once()\n        \n        # In a real test, we would verify fallback to LLM if both caches miss\n    \n    @patch('langchain_core.globals.set_llm_cache')\n    def test_lcel_integration_global_cache(self, mock_set_llm_cache):\n        \"\"\"Test integration with LCEL chains using global cache setting.\"\"\"\n        # Set orchestrator as global cache\n        self.orchestrator.set_as_global_cache()\n        \n        # Verify global cache was set\n        mock_set_llm_cache.assert_called_once_with(self.orchestrator)\n        \n        # In a real test, we would run a chain and verify caching behavior\n    \n    def test_lcel_integration_with_cache(self):\n        \"\"\"Test integration with LCEL chains using with_cache().\"\"\"\n        # Apply orchestrator to chain\n        cached_chain = self.mock_chain.with_cache(cache=self.orchestrator)\n        \n        # Verify with_cache was called with orchestrator\n        self.mock_chain.with_cache.assert_called_once_with(cache=self.orchestrator)\n        \n        # In a real test, we would run the cached chain and verify caching behavior\n    \n    @patch('redis.Redis')\n    def test_redis_semantic_cache_integration(self, mock_redis):\n        \"\"\"Test integration with RedisSemanticCache.\"\"\"\n        # Create a real RedisSemanticCache with mocked Redis\n        redis_semantic_cache = RedisSemanticCache(\n            redis_url=\"redis://localhost:6379/0\",\n            embedding=MagicMock(),  # Mock embedding function\n            score_threshold=0.8\n        )\n        \n        # Create CasinoCache with RedisSemanticCache\n        casino_cache = CasinoCache(\n            underlying_cache=redis_semantic_cache,\n            key_generator=SemanticKeyGenerator(),\n            ttl_strategy=TTLStrategy()\n        )\n        \n        # Add to orchestrator\n        orchestrator = CasinoCacheOrchestrator(\n            default_cache=casino_cache,\n            specialized_caches={}\n        )\n        \n        # In a real test, we would verify Redis operations and semantic matching\n    \n    @patch('src.services.analytics.cache_analytics.CacheAnalytics')\n    def test_analytics_collection(self, mock_analytics_class):\n        \"\"\"Test analytics collection during cache operations.\"\"\"\n        # Create mock analytics instance\n        mock_analytics = mock_analytics_class.return_value\n        \n        # Create orchestrator with analytics\n        orchestrator = CasinoCacheOrchestrator(\n            default_cache=self.general_cache,\n            specialized_caches={},\n            analytics=mock_analytics\n        )\n        \n        # Perform lookup\n        orchestrator.lookup(\"test query\", \"test_llm\")\n        \n        # Verify analytics were recorded\n        mock_analytics.record_lookup.assert_called_once()\n        \n        # Perform update\n        orchestrator.update(\"test query\", \"test_llm\", \"test result\")\n        \n        # Verify analytics were recorded\n        mock_analytics.record_update.assert_called_once()\n    \n    def test_cache_warming(self):\n        \"\"\"Test cache warming for common casino queries.\"\"\"\n        # Create orchestrator with cache warming capability\n        orchestrator = CasinoCacheOrchestrator(\n            default_cache=self.general_cache,\n            specialized_caches={},\n            common_queries=[\n                \"What are the rules for blackjack?\",\n                \"How do I place a bet?\",\n                \"What are the casino hours?\"\n            ]\n        )\n        \n        # Trigger cache warming\n        orchestrator.warm_cache(self.mock_llm)\n        \n        # Verify LLM was called for each common query\n        self.assertEqual(self.mock_llm.invoke.call_count, 3)\n        \n        # Verify results were cached\n        self.assertEqual(self.general_cache.update.call_count, 3)\n    \n    @patch('redis.Redis')\n    def test_error_recovery(self, mock_redis):\n        \"\"\"Test error recovery with Redis connection failures.\"\"\"\n        # Configure Redis to raise connection error\n        mock_redis.return_value.get.side_effect = ConnectionError(\"Redis connection failed\")\n        \n        # Create RedisSemanticCache with failing Redis\n        redis_cache = RedisSemanticCache(\n            redis_url=\"redis://localhost:6379/0\",\n            embedding=MagicMock(),\n            score_threshold=0.8\n        )\n        \n        # Create CasinoCache with Redis cache\n        casino_cache = CasinoCache(\n            underlying_cache=redis_cache,\n            key_generator=SemanticKeyGenerator(),\n            ttl_strategy=TTLStrategy()\n        )\n        \n        # Create fallback in-memory cache\n        fallback_cache = CasinoCache(\n            underlying_cache=InMemoryCache(),\n            key_generator=SemanticKeyGenerator(),\n            ttl_strategy=TTLStrategy()\n        )\n        \n        # Create orchestrator with error recovery\n        orchestrator = CasinoCacheOrchestrator(\n            default_cache=casino_cache,\n            specialized_caches={},\n            fallback_cache=fallback_cache\n        )\n        \n        # In a real test, we would verify fallback to in-memory cache on Redis failure\n    \n    async def test_async_operations(self):\n        \"\"\"Test async lookup and update operations.\"\"\"\n        # Configure async mock methods\n        self.general_cache.alookup = AsyncMock(return_value=\"Async cached result\")\n        self.general_cache.aupdate = AsyncMock()\n        \n        # Perform async lookup\n        result = await self.orchestrator.alookup(\"test query\", \"test_llm\")\n        \n        # Verify result and method calls\n        self.assertEqual(result, \"Async cached result\")\n        self.general_cache.alookup.assert_called_once()\n        \n        # Perform async update\n        await self.orchestrator.aupdate(\"test query\", \"test_llm\", \"test result\")\n        \n        # Verify method calls\n        self.general_cache.aupdate.assert_called_once()\n    \n    @patch('src.chains.universal_rag_chain.UniversalRAGChain')\n    def test_universal_rag_chain_integration(self, mock_rag_chain_class):\n        \"\"\"Test integration with Universal RAG Chain.\"\"\"\n        # Create mock RAG chain\n        mock_rag_chain = mock_rag_chain_class.return_value\n        mock_rag_chain.with_cache.return_value = mock_rag_chain\n        \n        # Apply orchestrator to RAG chain\n        cached_rag_chain = mock_rag_chain.with_cache(cache=self.orchestrator)\n        \n        # Verify with_cache was called with orchestrator\n        mock_rag_chain.with_cache.assert_called_once_with(cache=self.orchestrator)\n        \n        # In a real test, we would run the cached RAG chain and verify caching behavior\n\n\nclass AsyncMock(MagicMock):\n    async def __call__(self, *args, **kwargs):\n        return super(AsyncMock, self).__call__(*args, **kwargs)\n\n\nif __name__ == '__main__':\n    unittest.main()\n```",
        "testStrategy": "Run the unit tests with pytest or unittest. Verify that all tests pass. Check test coverage to ensure all code paths in CasinoCache and CasinoCacheOrchestrator are tested. Test the following key aspects:\n\n1. End-to-end testing of multi-cache orchestration with real Redis instance\n2. LCEL chain integration using set_llm_cache() and .with_cache()\n3. Content-type routing accuracy between different specialized caches\n4. Async chain operations with alookup, aupdate performance\n5. Cache warming effectiveness for common casino queries\n6. Fallback strategy testing (semantic → exact → LLM) in real chains\n7. Analytics collection during chain execution\n8. Error recovery with Redis connection failures during chain execution\n9. Memory usage and performance with large cache datasets\n10. Integration with Universal RAG Chain to ensure transparency\n\nUse a real Redis instance for integration tests (or a containerized version for CI/CD). Test with actual LCEL chains to verify the complete system works as it would in production. Monitor memory usage and performance metrics during large dataset tests.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement basic CasinoCache unit tests",
            "description": "Create unit tests for the core CasinoCache functionality including lookup, update, and clear operations.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CasinoCacheOrchestrator tests",
            "description": "Create tests for the orchestrator that manages multiple specialized caches and implements content-type routing.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement RedisSemanticCache integration tests",
            "description": "Test integration with RedisSemanticCache including connection handling, semantic matching, and TTL support.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement LCEL chain integration tests",
            "description": "Test integration with LCEL chains using both global cache setting and with_cache() method.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement async operation tests",
            "description": "Test async lookup and update operations with LCEL chains and measure performance.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement fallback strategy tests",
            "description": "Test the fallback strategy from semantic to exact to LLM when cache misses occur.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement cache warming tests",
            "description": "Test cache warming functionality for common casino queries.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement error recovery tests",
            "description": "Test error recovery mechanisms when Redis connection failures occur during chain execution.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement analytics collection tests",
            "description": "Test analytics collection during cache operations and chain execution.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Implement Universal RAG Chain integration tests",
            "description": "Test integration with Universal RAG Chain to ensure transparency and proper caching behavior.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 34,
        "title": "Implement Unit Tests for CachingFactory",
        "description": "Create comprehensive unit tests for the CachingFactory class to ensure correct creation and configuration of different types of caches, with special focus on RedisSemanticCache integration and end-to-end testing.",
        "status": "cancelled",
        "dependencies": [
          29
        ],
        "priority": "medium",
        "details": "In test_cache_factory.py, implement the following tests:\n\n```python\nimport unittest\nfrom unittest.mock import MagicMock, patch\nimport redis\nimport pytest\n\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.caches.in_memory import InMemoryCache\nfrom langchain_core.caches.redis_cache import RedisCache\nfrom langchain_core.caches.redis_semantic_cache import RedisSemanticCache\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.language_models.llms import LLM\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nfrom src.services.cache.cache_factory import CachingFactory\nfrom src.services.cache.casino_cache import CasinoCache\nfrom src.services.cache.ttl_strategy import TTLStrategy\nfrom src.services.cache.exceptions import CacheConfigurationError\nfrom src.services.orchestration.chain_orchestrator import ChainOrchestrator\n\nclass TestCachingFactory(unittest.TestCase):\n    def test_create_memory_cache(self):\n        \"\"\"Test creation of in-memory cache.\"\"\"\n        # Create cache\n        cache = CachingFactory.create_memory_cache()\n        \n        # Verify cache type\n        self.assertIsInstance(cache, CasinoCache)\n        self.assertIsInstance(cache.cache, InMemoryCache)\n    \n    @patch('src.services.cache.cache_factory.RedisCache')\n    def test_create_redis_cache(self, mock_redis_cache):\n        \"\"\"Test creation of Redis cache.\"\"\"\n        # Configure mock\n        mock_redis_cache.return_value = MagicMock(spec=RedisCache)\n        \n        # Create cache\n        cache = CachingFactory.create_redis_cache(\"redis://localhost:6379\")\n        \n        # Verify cache type\n        self.assertIsInstance(cache, CasinoCache)\n        \n        # Verify Redis was configured correctly\n        mock_redis_cache.assert_called_once_with(redis_url=\"redis://localhost:6379\")\n    \n    def test_create_redis_cache_error(self):\n        \"\"\"Test error handling in Redis cache creation.\"\"\"\n        # Patch RedisCache to raise an exception\n        with patch('src.services.cache.cache_factory.RedisCache', side_effect=ValueError(\"Simulated error\")):\n            # Verify exception is caught and wrapped\n            with self.assertRaises(CacheConfigurationError):\n                CachingFactory.create_redis_cache(\"redis://localhost:6379\")\n    \n    @patch('src.services.cache.cache_factory.RedisSemanticCache')\n    def test_create_semantic_cache(self, mock_redis_semantic_cache):\n        \"\"\"Test creation of semantic cache with Redis.\"\"\"\n        # Configure mock\n        mock_redis_semantic_cache.return_value = MagicMock(spec=RedisSemanticCache)\n        \n        # Create mock embeddings\n        mock_embeddings = MagicMock(spec=Embeddings)\n        \n        # Create cache\n        cache = CachingFactory.create_semantic_cache(\n            embeddings=mock_embeddings,\n            redis_url=\"redis://localhost:6379\",\n            similarity_threshold=0.9\n        )\n        \n        # Verify cache type\n        self.assertIsInstance(cache, CasinoCache)\n        \n        # Verify RedisSemanticCache was configured correctly\n        mock_redis_semantic_cache.assert_called_once_with(\n            redis_url=\"redis://localhost:6379\",\n            embedding=mock_embeddings,\n            similarity_threshold=0.9\n        )\n    \n    def test_create_semantic_cache_fallback(self):\n        \"\"\"Test semantic cache creation falls back to in-memory when no Redis URL.\"\"\"\n        # Create mock embeddings\n        mock_embeddings = MagicMock(spec=Embeddings)\n        \n        # Create cache without Redis URL\n        cache = CachingFactory.create_semantic_cache(embeddings=mock_embeddings)\n        \n        # Verify cache type\n        self.assertIsInstance(cache, CasinoCache)\n        self.assertIsInstance(cache.cache, InMemoryCache)\n    \n    def test_setup_global_llm_cache(self):\n        \"\"\"Test setting up global LLM cache.\"\"\"\n        # Create mock cache and LLM\n        mock_cache = MagicMock(spec=CasinoCache)\n        mock_llm = MagicMock(spec=LLM)\n        \n        # Set up global cache for specific LLM\n        CachingFactory.setup_global_llm_cache(mock_cache, mock_llm)\n        \n        # Verify LLM cache was set\n        self.assertEqual(mock_llm.cache, mock_cache)\n    \n    @patch('src.services.cache.cache_factory.set_llm_cache')\n    def test_setup_global_llm_cache_no_llm(self, mock_set_llm_cache):\n        \"\"\"Test setting up global LLM cache without specific LLM.\"\"\"\n        # Create mock cache\n        mock_cache = MagicMock(spec=CasinoCache)\n        \n        # Set up global cache\n        CachingFactory.setup_global_llm_cache(mock_cache)\n        \n        # Verify global cache was set\n        mock_set_llm_cache.assert_called_once_with(mock_cache)\n    \n    def test_create_multi_level_cache(self):\n        \"\"\"Test creation of multi-level cache.\"\"\"\n        # Create mock embeddings\n        mock_embeddings = MagicMock(spec=Embeddings)\n        \n        # Create multi-level cache\n        caches = CachingFactory.create_multi_level_cache(embeddings=mock_embeddings)\n        \n        # Verify cache structure\n        self.assertIn('llm', caches)\n        self.assertIn('chain', caches)\n        self.assertIn('rag', caches)\n        \n        # Verify all caches are CasinoCache instances\n        for cache_name, cache in caches.items():\n            self.assertIsInstance(cache, CasinoCache)\n        \n        # Verify different TTL strategies\n        llm_ttl = caches['llm'].ttl_strategy.default_ttl\n        chain_ttl = caches['chain'].ttl_strategy.default_ttl\n        rag_ttl = caches['rag'].ttl_strategy.default_ttl\n        \n        # LLM TTL should be longer than chain TTL, which should be longer than RAG TTL\n        self.assertGreater(llm_ttl, chain_ttl)\n        self.assertGreater(chain_ttl, rag_ttl)\n    \n    def test_validate_similarity_threshold(self):\n        \"\"\"Test validation of similarity threshold values.\"\"\"\n        # Valid thresholds\n        self.assertTrue(CachingFactory._validate_similarity_threshold(0.0))\n        self.assertTrue(CachingFactory._validate_similarity_threshold(0.5))\n        self.assertTrue(CachingFactory._validate_similarity_threshold(1.0))\n        \n        # Invalid thresholds\n        with self.assertRaises(CacheConfigurationError):\n            CachingFactory._validate_similarity_threshold(-0.1)\n        with self.assertRaises(CacheConfigurationError):\n            CachingFactory._validate_similarity_threshold(1.1)\n    \n    def test_validate_ttl_seconds(self):\n        \"\"\"Test validation of TTL seconds values.\"\"\"\n        # Valid TTL values\n        self.assertTrue(CachingFactory._validate_ttl_seconds(1))\n        self.assertTrue(CachingFactory._validate_ttl_seconds(3600))\n        \n        # Invalid TTL values\n        with self.assertRaises(CacheConfigurationError):\n            CachingFactory._validate_ttl_seconds(0)\n        with self.assertRaises(CacheConfigurationError):\n            CachingFactory._validate_ttl_seconds(-1)\n\n@pytest.mark.integration\nclass TestCachingFactoryIntegration:\n    @pytest.fixture\n    def redis_client(self):\n        \"\"\"Set up and tear down a Redis client for testing.\"\"\"\n        client = redis.Redis.from_url(\"redis://localhost:6379\")\n        yield client\n        # Clean up after tests\n        client.flushall()\n        client.close()\n    \n    @pytest.fixture\n    def embeddings(self):\n        \"\"\"Create real embeddings for testing.\"\"\"\n        from langchain_openai import OpenAIEmbeddings\n        return OpenAIEmbeddings()\n    \n    @pytest.fixture\n    def llm(self):\n        \"\"\"Create real LLM for testing.\"\"\"\n        from langchain_openai import OpenAI\n        return OpenAI()\n    \n    def test_content_specific_semantic_caches(self, redis_client, embeddings):\n        \"\"\"Test creation of content-specific semantic caches.\"\"\"\n        # Create content-specific caches\n        caches = CachingFactory.create_content_specific_caches(\n            embeddings=embeddings,\n            redis_url=\"redis://localhost:6379\"\n        )\n        \n        # Verify cache structure\n        assert 'news' in caches\n        assert 'reviews' in caches\n        assert 'regulatory' in caches\n        \n        # Verify content-specific configurations\n        assert caches['news'].cache.similarity_threshold == 0.1\n        assert caches['news'].ttl_strategy.default_ttl == 7200\n        \n        assert caches['reviews'].cache.similarity_threshold == 0.3\n        assert caches['reviews'].ttl_strategy.default_ttl == 172800\n        \n        assert caches['regulatory'].cache.similarity_threshold == 0.2\n        assert caches['regulatory'].ttl_strategy.default_ttl == 604800\n        \n        # Verify shared Redis client\n        news_client = caches['news'].cache.redis_client\n        reviews_client = caches['reviews'].cache.redis_client\n        regulatory_client = caches['regulatory'].cache.redis_client\n        \n        # Check that the Redis clients are the same object (shared)\n        assert news_client is reviews_client\n        assert reviews_client is regulatory_client\n    \n    def test_end_to_end_with_lcel_chain(self, redis_client, embeddings, llm):\n        \"\"\"Test end-to-end integration with LCEL chains.\"\"\"\n        # Create cache\n        cache = CachingFactory.create_semantic_cache(\n            embeddings=embeddings,\n            redis_url=\"redis://localhost:6379\",\n            similarity_threshold=0.8\n        )\n        \n        # Set up LLM with cache\n        CachingFactory.setup_global_llm_cache(cache, llm)\n        \n        # Create a simple chain\n        prompt = PromptTemplate.from_template(\"Summarize this: {text}\")\n        chain = prompt | llm\n        \n        # Run the chain twice with similar inputs\n        text1 = \"The quick brown fox jumps over the lazy dog.\"\n        text2 = \"A quick brown fox jumped over a lazy dog.\"\n        \n        # First run should miss cache\n        result1 = chain.invoke({\"text\": text1})\n        \n        # Second run with similar input should hit cache\n        with patch.object(llm, 'invoke', wraps=llm.invoke) as mock_invoke:\n            result2 = chain.invoke({\"text\": text2})\n            # Verify LLM wasn't called again (cache hit)\n            assert mock_invoke.call_count == 0\n    \n    def test_factory_integration_with_orchestrator(self, redis_client, embeddings, llm):\n        \"\"\"Test integration between factory and chain orchestrator.\"\"\"\n        # Create multi-level cache\n        caches = CachingFactory.create_multi_level_cache(\n            embeddings=embeddings,\n            redis_url=\"redis://localhost:6379\"\n        )\n        \n        # Create orchestrator with caches\n        orchestrator = ChainOrchestrator(llm=llm, caches=caches)\n        \n        # Create a chain through the orchestrator\n        prompt = PromptTemplate.from_template(\"Answer this question: {question}\")\n        chain = orchestrator.create_chain(prompt)\n        \n        # Run the chain twice with the same input\n        question = \"What is the capital of France?\"\n        \n        # First run should miss cache\n        result1 = chain.invoke({\"question\": question})\n        \n        # Second run should hit cache\n        with patch.object(llm, 'invoke') as mock_invoke:\n            result2 = chain.invoke({\"question\": question})\n            # Verify LLM wasn't called again (cache hit)\n            mock_invoke.assert_not_called()\n    \n    @pytest.mark.performance\n    def test_concurrent_cache_operations(self, redis_client, embeddings):\n        \"\"\"Test performance with concurrent cache operations.\"\"\"\n        import concurrent.futures\n        import time\n        \n        # Create cache\n        cache = CachingFactory.create_semantic_cache(\n            embeddings=embeddings,\n            redis_url=\"redis://localhost:6379\",\n            similarity_threshold=0.8\n        )\n        \n        # Function to perform cache operations\n        def cache_operation(i):\n            key = f\"test_key_{i}\"\n            value = f\"test_value_{i}\"\n            cache.update(key, value)\n            result = cache.lookup(key)\n            return result == value\n        \n        # Run concurrent operations\n        start_time = time.time()\n        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n            results = list(executor.map(cache_operation, range(100)))\n        end_time = time.time()\n        \n        # Verify all operations succeeded\n        assert all(results)\n        \n        # Verify performance is acceptable (adjust threshold as needed)\n        assert end_time - start_time < 5.0  # Should complete in under 5 seconds\n\nif __name__ == '__main__':\n    unittest.main()\n```",
        "testStrategy": "The testing strategy includes both unit tests and integration tests:\n\n1. **Unit Tests**:\n   - Run with pytest or unittest to verify basic functionality\n   - Test all factory methods for creating different cache types\n   - Verify validation of parameters (similarity thresholds, TTL values)\n   - Test error handling and exception propagation\n   - Verify multi-level cache creation with different configurations\n\n2. **Integration Tests**:\n   - Requires a running Redis instance (localhost:6379)\n   - Test content-specific cache configurations (news, reviews, regulatory)\n   - Verify shared Redis client efficiency across multiple cache instances\n   - Test end-to-end with real LCEL chains and LLMs\n   - Test integration with the ChainOrchestrator\n   - Performance testing with concurrent cache operations\n\n3. **Test Coverage**:\n   - Ensure all code paths in CachingFactory are tested\n   - Test both success and failure scenarios\n   - Verify cache hit/miss behavior in real-world scenarios\n\nRun unit tests with: `pytest test_cache_factory.py -v`\nRun integration tests with: `pytest test_cache_factory.py -v -m integration`\nRun performance tests with: `pytest test_cache_factory.py -v -m performance`",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement basic unit tests for CachingFactory",
            "description": "Create unit tests for basic cache creation methods (memory, Redis, semantic)",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement validation tests for factory parameters",
            "description": "Add tests for similarity threshold (0.0-1.0) and TTL seconds (positive integers) validation",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement content-specific cache configuration tests",
            "description": "Test creation of content-specific caches with different thresholds and TTLs (news=0.1/7200s, reviews=0.3/172800s, regulatory=0.2/604800s)",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement shared Redis client efficiency tests",
            "description": "Verify that multiple cache instances share the same Redis client for efficiency",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement end-to-end tests with LCEL chains",
            "description": "Test integration with LCEL chains to verify cache hits/misses with similar inputs",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement orchestrator integration tests",
            "description": "Test integration between CachingFactory and ChainOrchestrator for complete workflow",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement performance tests for concurrent operations",
            "description": "Test performance with concurrent cache operations to verify memory management and efficiency",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 35,
        "title": "Implement Integration Tests for LangChain Compatibility",
        "description": "Create integration tests to verify that the casino caching system correctly integrates with LangChain's native caching infrastructure and LCEL chains using the CasinoCacheOrchestrator.",
        "status": "cancelled",
        "dependencies": [
          30
        ],
        "priority": "high",
        "details": "Create a new file `test_langchain_integration.py` in the tests/services/cache directory with the following content:\n\n```python\nimport unittest\nfrom unittest.mock import MagicMock, patch\nimport asyncio\n\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\nfrom langchain_core.globals import get_llm_cache, set_llm_cache\n\nfrom src.services.cache.casino_cache import CasinoCache\nfrom src.services.cache.cache_factory import CachingFactory\nfrom src.services.cache.langchain_integration import setup_llm_with_cache, setup_global_cache, with_context_extractor, create_cached_chain\nfrom src.services.cache.orchestrator import CasinoCacheOrchestrator\nfrom src.services.cache.redis_semantic_cache import RedisSemanticCache\n\nclass MockLLM(LLM):\n    \"\"\"Mock LLM for testing.\"\"\"\n    def _call(self, prompt, **kwargs):\n        return f\"Response to: {prompt}\"\n    \n    async def _acall(self, prompt, **kwargs):\n        return f\"Async response to: {prompt}\"\n    \n    @property\n    def _llm_type(self):\n        return \"mock\"\n\nclass TestLangChainIntegration(unittest.TestCase):\n    def setUp(self):\n        # Create a mock cache and orchestrator\n        self.mock_cache = MagicMock(spec=CasinoCache)\n        self.mock_semantic_cache = MagicMock(spec=RedisSemanticCache)\n        self.mock_orchestrator = MagicMock(spec=CasinoCacheOrchestrator)\n        self.mock_orchestrator.semantic_cache = self.mock_semantic_cache\n        self.mock_orchestrator.exact_cache = self.mock_cache\n        \n        # Create a real LLM for testing\n        self.llm = MockLLM()\n    \n    def test_setup_llm_with_cache(self):\n        \"\"\"Test configuring an LLM to use the casino cache.\"\"\"\n        # Configure LLM with cache\n        configured_llm = setup_llm_with_cache(self.llm, self.mock_cache)\n        \n        # Verify LLM cache was set\n        self.assertEqual(configured_llm.cache, self.mock_cache)\n        \n        # Verify LLM is the same instance\n        self.assertIs(configured_llm, self.llm)\n    \n    def test_setup_llm_with_default_cache(self):\n        \"\"\"Test configuring an LLM with default cache.\"\"\"\n        # Configure LLM with default cache\n        configured_llm = setup_llm_with_cache(self.llm)\n        \n        # Verify LLM cache was set to a CasinoCache\n        self.assertIsInstance(configured_llm.cache, CasinoCache)\n    \n    @patch('src.services.cache.langchain_integration.set_llm_cache')\n    def test_setup_global_cache(self, mock_set_llm_cache):\n        \"\"\"Test setting up global LLM cache.\"\"\"\n        # Set up global cache\n        setup_global_cache(self.mock_orchestrator)\n        \n        # Verify global cache was set\n        mock_set_llm_cache.assert_called_once_with(self.mock_orchestrator)\n    \n    def test_with_context_extractor(self):\n        \"\"\"Test context extraction runnable.\"\"\"\n        # Create context extractor\n        extractor = with_context_extractor()\n        \n        # Test with various inputs\n        result1 = extractor.invoke({\n            'query': 'test query',\n            'query_type': 'TEST',\n            'confidence_score': 0.8,\n            'expertise_level': 'EXPERT'\n        })\n        \n        # Verify context was extracted\n        self.assertIn('cache_context', result1)\n        self.assertEqual(result1['cache_context']['query_type'], 'TEST')\n        self.assertEqual(result1['cache_context']['confidence_score'], 0.8)\n        self.assertEqual(result1['cache_context']['expertise_level'], 'EXPERT')\n        \n        # Test with minimal input\n        result2 = extractor.invoke({'query': 'test query'})\n        self.assertIn('cache_context', result2)\n        self.assertEqual(result2['cache_context'], {})\n    \n    def test_create_cached_chain(self):\n        \"\"\"Test creating a chain with LangChain's native caching.\"\"\"\n        # Create a simple chain\n        chain = RunnableLambda(lambda x: {**x, 'result': f\"Processed: {x['query']}\"})\n        \n        # Create cached chain\n        cached_chain = create_cached_chain(chain, self.mock_orchestrator)\n        \n        # Test chain\n        result = cached_chain.invoke({'query': 'test query'})\n        \n        # Verify result\n        self.assertEqual(result['result'], \"Processed: test query\")\n        self.assertIn('cache_context', result)\n    \n    @patch('src.services.cache.langchain_integration.setup_global_cache')\n    def test_create_cached_chain_sets_global_cache(self, mock_setup_global_cache):\n        \"\"\"Test that create_cached_chain sets up global cache.\"\"\"\n        # Create a simple chain\n        chain = RunnableLambda(lambda x: {**x, 'result': f\"Processed: {x['query']}\"})\n        \n        # Create cached chain\n        cached_chain = create_cached_chain(chain, self.mock_orchestrator)\n        \n        # Verify global cache was set\n        mock_setup_global_cache.assert_called_once_with(self.mock_orchestrator)\n    \n    def test_llm_with_cache_integration(self):\n        \"\"\"Test end-to-end integration with LLM caching.\"\"\"\n        # Create a real cache\n        cache = CachingFactory.create_memory_cache()\n        \n        # Configure LLM with cache\n        self.llm.cache = cache\n        \n        # First call should miss cache\n        response1 = self.llm(\"test prompt\")\n        \n        # Mock the _call method to verify cache hit\n        original_call = self.llm._call\n        self.llm._call = MagicMock(side_effect=original_call)\n        \n        # Second call should hit cache\n        response2 = self.llm(\"test prompt\")\n        \n        # Verify responses match\n        self.assertEqual(response1, response2)\n        \n        # Verify _call was not called for second request (cache hit)\n        self.llm._call.assert_not_called()\n        \n        # Restore original _call\n        self.llm._call = original_call\n        \n    async def test_async_lcel_integration(self):\n        \"\"\"Test async LCEL integration with cache operations.\"\"\"\n        # Setup async chain with cache\n        async_chain = RunnableLambda(lambda x: asyncio.sleep(0.1)).then(\n            RunnableLambda(lambda x: {\"result\": \"async result\"})\n        )\n        \n        # Configure with cache\n        cached_chain = async_chain.with_cache(cache=self.mock_orchestrator)\n        \n        # Test async execution\n        result = await cached_chain.ainvoke({\"query\": \"async test\"})\n        self.assertEqual(result[\"result\"], \"async result\")\n        \n        # Verify cache was checked\n        self.mock_orchestrator.get.assert_called_once()\n    \n    def test_cache_middleware_integration(self):\n        \"\"\"Test middleware for automatic cache checking before LLM calls.\"\"\"\n        # Create middleware that checks cache before LLM calls\n        from langchain_core.runnables import RunnableConfig\n        \n        # Setup chain with middleware\n        chain = self.llm.with_config({\"callbacks\": [CachingMiddleware(self.mock_orchestrator)]})\n        \n        # Test chain execution\n        result = chain.invoke(\"test query\")\n        \n        # Verify cache was checked before LLM call\n        self.mock_orchestrator.get.assert_called_once()\n    \n    def test_fallback_strategy(self):\n        \"\"\"Test fallback strategy (semantic → exact → LLM) in LCEL chains.\"\"\"\n        # Mock cache misses\n        self.mock_semantic_cache.get.return_value = None\n        self.mock_cache.get.return_value = None\n        \n        # Create chain with fallback strategy\n        chain = create_cached_chain(self.llm, self.mock_orchestrator)\n        \n        # Execute chain\n        result = chain.invoke({\"query\": \"test query\"})\n        \n        # Verify fallback order\n        call_order = self.mock_orchestrator.method_calls\n        self.assertEqual(call_order[0][0], 'semantic_cache.get')\n        self.assertEqual(call_order[1][0], 'exact_cache.get')\n    \n    def test_cache_warming(self):\n        \"\"\"Test cache warming utilities with common casino queries.\"\"\"\n        # Test cache warming function\n        from src.services.cache.langchain_integration import warm_cache\n        \n        # Execute cache warming\n        warm_cache(self.llm, [\"What are the casino hours?\", \"How do I join the rewards program?\"], self.mock_orchestrator)\n        \n        # Verify cache was populated\n        self.assertEqual(self.mock_orchestrator.set.call_count, 2)\n    \n    def test_analytics_collection(self):\n        \"\"\"Test analytics collection during LCEL execution.\"\"\"\n        # Create chain with analytics\n        chain = create_cached_chain(self.llm, self.mock_orchestrator, collect_analytics=True)\n        \n        # Execute chain\n        result = chain.invoke({\"query\": \"test query\"})\n        \n        # Verify analytics were collected\n        self.mock_orchestrator.record_analytics.assert_called_once()\n    \n    def test_multi_cache_routing(self):\n        \"\"\"Test multi-cache routing within chains based on content type.\"\"\"\n        # Create chain with content-based routing\n        chain = create_cached_chain(self.llm, self.mock_orchestrator, content_based_routing=True)\n        \n        # Test with different content types\n        result1 = chain.invoke({\"query\": \"test query\", \"content_type\": \"FAQ\"})\n        result2 = chain.invoke({\"query\": \"test query\", \"content_type\": \"POLICY\"})\n        \n        # Verify different caches were used based on content type\n        self.mock_orchestrator.get_cache_for_content_type.assert_any_call(\"FAQ\")\n        self.mock_orchestrator.get_cache_for_content_type.assert_any_call(\"POLICY\")\n    \n    def test_error_handling(self):\n        \"\"\"Test error handling during cache operations in LCEL chains.\"\"\"\n        # Make cache operations raise exceptions\n        self.mock_orchestrator.get.side_effect = Exception(\"Cache error\")\n        \n        # Create chain with error handling\n        chain = create_cached_chain(self.llm, self.mock_orchestrator, handle_errors=True)\n        \n        # Execute chain - should not propagate cache errors\n        result = chain.invoke({\"query\": \"test query\"})\n        \n        # Verify chain executed despite cache error\n        self.assertIn(\"result\", result)\n    \n    def test_performance(self):\n        \"\"\"Test performance with async chains and cache operations.\"\"\"\n        import time\n        \n        # Create real orchestrator with in-memory caches\n        orchestrator = CasinoCacheOrchestrator(\n            semantic_cache=CachingFactory.create_memory_semantic_cache(),\n            exact_cache=CachingFactory.create_memory_cache()\n        )\n        \n        # Create async chain\n        async_chain = RunnableLambda(lambda x: asyncio.sleep(0.1)).then(self.llm)\n        cached_chain = async_chain.with_cache(cache=orchestrator)\n        \n        # Measure performance\n        start_time = time.time()\n        result1 = asyncio.run(cached_chain.ainvoke(\"test query\"))\n        first_duration = time.time() - start_time\n        \n        # Second call should be faster (cache hit)\n        start_time = time.time()\n        result2 = asyncio.run(cached_chain.ainvoke(\"test query\"))\n        second_duration = time.time() - start_time\n        \n        # Verify cache improved performance\n        self.assertLess(second_duration, first_duration)\n\n# Mock middleware for testing\nclass CachingMiddleware:\n    def __init__(self, cache):\n        self.cache = cache\n    \n    def on_llm_start(self, serialized, prompts, **kwargs):\n        # Check cache before LLM call\n        for prompt in prompts:\n            cached_result = self.cache.get(prompt)\n            if cached_result:\n                return cached_result\n        return None\n\nif __name__ == '__main__':\n    unittest.main()\n```",
        "testStrategy": "Run the integration tests with pytest or unittest. Verify that all tests pass. Check that the casino caching system correctly integrates with LangChain's native caching infrastructure and LCEL chains. Test the following aspects:\n\n1. Async LCEL RunnableLambda integration with cache operations\n2. Middleware for automatic cache checking before LLM calls\n3. Global cache setup using RedisSemanticCache with set_llm_cache()\n4. Chain-level caching using .with_cache() method with CasinoCacheOrchestrator\n5. Fallback strategy (semantic → exact → LLM) in LCEL chains\n6. Cache warming utilities with common casino queries\n7. Analytics collection during LCEL execution\n8. Multi-cache routing within chains based on content type\n9. Error handling during cache operations in LCEL chains\n10. Performance testing with async chains and cache operations\n\nVerify that the integration makes caching completely transparent to chain users, with no need to explicitly handle cache operations in their chain definitions.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement basic LLM cache integration tests",
            "description": "Create tests for basic LLM cache integration with setup_llm_with_cache and setup_global_cache functions.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement context extraction tests",
            "description": "Create tests for the with_context_extractor function to verify proper extraction of cache context from input data.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement chain-level caching tests",
            "description": "Create tests for create_cached_chain function and verify chain-level caching with the orchestrator.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement async LCEL integration tests",
            "description": "Create tests for async LCEL integration with cache operations using RunnableLambda and ainvoke.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement middleware testing",
            "description": "Create tests for middleware that automatically checks cache before LLM calls.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement fallback strategy tests",
            "description": "Create tests for the fallback strategy (semantic → exact → LLM) in LCEL chains.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement cache warming tests",
            "description": "Create tests for cache warming utilities with common casino queries.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement analytics collection tests",
            "description": "Create tests for analytics collection during LCEL execution.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement multi-cache routing tests",
            "description": "Create tests for multi-cache routing within chains based on content type.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Implement error handling tests",
            "description": "Create tests for error handling during cache operations in LCEL chains.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Implement performance tests",
            "description": "Create performance tests with async chains and cache operations to verify caching improves response times.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 36,
        "title": "Implement Performance Tests",
        "description": "Create performance tests to verify that the casino caching system meets the performance requirements specified in the PRD, with a focus on the RedisSemanticCache implementation and multi-cache orchestration.",
        "status": "cancelled",
        "dependencies": [
          29
        ],
        "priority": "medium",
        "details": "Create a new file `test_performance.py` in the tests/services/cache directory with the following content:\n\n```python\nimport unittest\nimport time\nimport random\nimport string\nimport threading\nimport asyncio\nimport psutil\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.caches.in_memory import InMemoryCache\n\nfrom src.services.cache.casino_cache import CasinoCache\nfrom src.services.cache.cache_factory import CachingFactory\nfrom src.services.cache.semantic_key_generator import SemanticKeyGenerator\nfrom src.services.cache.ttl_strategy import TTLStrategy\nfrom src.services.cache.redis_semantic_cache import RedisSemanticCache\n\nclass MockLLM(LLM):\n    \"\"\"Mock LLM for performance testing.\"\"\"\n    def __init__(self, sleep_time=0.01):\n        super().__init__()\n        self.sleep_time = sleep_time\n    \n    def _call(self, prompt, **kwargs):\n        # Simulate LLM processing time\n        time.sleep(self.sleep_time)\n        return f\"Response to: {prompt}\"\n    \n    @property\n    def _llm_type(self):\n        return \"mock\"\n\nclass TestCachePerformance(unittest.TestCase):\n    def setUp(self):\n        # Create a real cache for performance testing\n        self.memory_cache = CachingFactory.create_memory_cache()\n        self.redis_cache = CachingFactory.create_redis_cache()\n        self.orchestrated_cache = CachingFactory.create_orchestrated_cache()\n        \n        # Create an LLM with different caches for comparison\n        self.llm = MockLLM()\n        self.llm_memory = MockLLM()\n        self.llm_memory.cache = self.memory_cache\n        self.llm_redis = MockLLM()\n        self.llm_redis.cache = self.redis_cache\n        self.llm_orchestrated = MockLLM()\n        self.llm_orchestrated.cache = self.orchestrated_cache\n        \n        # Generate test data with realistic casino queries\n        self.casino_brands = ['888Casino', 'Betway', 'PokerStars', 'Unibet', 'Bet365']\n        self.query_types = [\n            \"What are the bonus terms for {casino}?\",\n            \"How do I withdraw funds from {casino}?\",\n            \"What games are available at {casino}?\",\n            \"Is {casino} licensed in the UK?\",\n            \"How to contact {casino} customer support?\",\n            \"What payment methods does {casino} accept?\",\n            \"Are there any promotions at {casino} this month?\",\n            \"How to verify my account on {casino}?\",\n            \"What are the betting limits at {casino}?\",\n            \"Does {casino} have a mobile app?\"\n        ]\n        \n        # Generate 1000 realistic casino queries\n        self.test_queries = []\n        for i in range(1000):\n            casino = random.choice(self.casino_brands)\n            query_template = random.choice(self.query_types)\n            query = query_template.format(casino=casino)\n            # Add some randomness to create similar but not identical queries\n            if random.random() < 0.3:\n                query = query.replace(\"?\", \"? I'm a new user.\")\n            self.test_queries.append(query)\n    \n    def _random_string(self, length):\n        \"\"\"Generate a random string of fixed length.\"\"\"\n        return ''.join(random.choice(string.ascii_letters) for _ in range(length))\n    \n    def test_cache_hit_performance(self):\n        \"\"\"Test cache hit performance (should be < 10ms).\"\"\"\n        # Test different cache implementations\n        cache_configs = [\n            (\"Memory Cache\", self.memory_cache, self.llm_memory),\n            (\"Redis Cache\", self.redis_cache, self.llm_redis),\n            (\"Orchestrated Cache\", self.orchestrated_cache, self.llm_orchestrated)\n        ]\n        \n        for cache_name, cache, llm in cache_configs:\n            # Populate cache by calling LLM\n            for i in range(100):\n                llm(self.test_queries[i])\n            \n            # Measure cache hit performance\n            total_time = 0\n            iterations = 1000\n            \n            for _ in range(iterations):\n                query_idx = random.randint(0, 99)  # Use only cached queries\n                start_time = time.time()\n                \n                llm(self.test_queries[query_idx])\n                \n                end_time = time.time()\n                total_time += (end_time - start_time)\n            \n            avg_time_ms = (total_time / iterations) * 1000\n            print(f\"{cache_name} - Average cache hit time: {avg_time_ms:.2f}ms\")\n            \n            # Should be < 10ms per PRD\n            self.assertLess(avg_time_ms, 10)\n    \n    def test_cache_miss_performance(self):\n        \"\"\"Test cache miss performance (should be < 50ms).\"\"\"\n        # Test different cache implementations\n        cache_configs = [\n            (\"Memory Cache\", self.memory_cache),\n            (\"Redis Cache\", self.redis_cache),\n            (\"Orchestrated Cache\", self.orchestrated_cache)\n        ]\n        \n        for cache_name, cache in cache_configs:\n            # Create LLM with no processing time for accurate cache miss measurement\n            fast_llm = MockLLM(sleep_time=0)\n            fast_llm.cache = cache\n            \n            # Measure cache miss performance (excluding LLM processing time)\n            total_time = 0\n            iterations = 100\n            \n            for i in range(iterations):\n                # Use a new query each time to ensure cache miss\n                query = f\"uncached query {i} {self._random_string(20)}\"\n                \n                start_time = time.time()\n                \n                # Only measuring the cache lookup time, not the LLM processing time\n                result = cache.lookup(query, str(fast_llm))\n                \n                end_time = time.time()\n                total_time += (end_time - start_time)\n            \n            avg_time_ms = (total_time / iterations) * 1000\n            print(f\"{cache_name} - Average cache miss time: {avg_time_ms:.2f}ms\")\n            \n            # Should be < 50ms per PRD\n            self.assertLess(avg_time_ms, 50)\n    \n    def test_throughput(self):\n        \"\"\"Test cache throughput (should be > 1000 ops/sec).\"\"\"\n        # Test different cache implementations\n        cache_configs = [\n            (\"Memory Cache\", self.memory_cache),\n            (\"Redis Cache\", self.redis_cache),\n            (\"Orchestrated Cache\", self.orchestrated_cache)\n        ]\n        \n        for cache_name, cache in cache_configs:\n            # Prepare mixed workload (50% lookups, 50% updates)\n            operations = []\n            for i in range(5000):  # Prepare 5000 operations\n                query_idx = random.randint(0, 999)\n                query = self.test_queries[query_idx]\n                llm_string = str(self.llm)\n                \n                if i % 2 == 0:\n                    # Lookup operation\n                    operations.append(('lookup', query, llm_string))\n                else:\n                    # Update operation\n                    operations.append(('update', query, llm_string, f\"result {i}\"))\n            \n            # Function to execute in threads\n            def execute_operation(op):\n                if op[0] == 'lookup':\n                    cache.lookup(op[1], op[2])\n                else:  # update\n                    cache.update(op[1], op[2], op[3])\n            \n            # Execute operations and measure time\n            start_time = time.time()\n            \n            with ThreadPoolExecutor(max_workers=10) as executor:\n                executor.map(execute_operation, operations)\n            \n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            \n            ops_per_sec = len(operations) / elapsed_time\n            print(f\"{cache_name} - Throughput: {ops_per_sec:.2f} operations per second\")\n            \n            # Should be > 1000 ops/sec per PRD\n            self.assertGreater(ops_per_sec, 1000)\n    \n    def test_concurrent_access(self):\n        \"\"\"Test concurrent access to the cache.\"\"\"\n        # Test different cache implementations\n        cache_configs = [\n            (\"Memory Cache\", self.memory_cache),\n            (\"Redis Cache\", self.redis_cache),\n            (\"Orchestrated Cache\", self.orchestrated_cache)\n        ]\n        \n        for cache_name, cache in cache_configs:\n            # Number of concurrent threads\n            num_threads = 20\n            # Operations per thread\n            ops_per_thread = 100\n            \n            # Shared counter for successful operations\n            success_count = 0\n            lock = threading.Lock()\n            \n            # Thread function\n            def thread_func():\n                nonlocal success_count\n                local_success = 0\n                llm_string = str(self.llm)\n                \n                for i in range(ops_per_thread):\n                    try:\n                        # Randomly choose lookup or update\n                        if random.random() < 0.5:\n                            # Lookup operation\n                            query_idx = random.randint(0, 999)\n                            cache.lookup(self.test_queries[query_idx], llm_string)\n                        else:\n                            # Update operation\n                            query_idx = random.randint(0, 999)\n                            cache.update(\n                                self.test_queries[query_idx],\n                                llm_string,\n                                f\"result {query_idx}\"\n                            )\n                        local_success += 1\n                    except Exception:\n                        pass\n                \n                # Update shared counter\n                with lock:\n                    success_count += local_success\n            \n            # Create and start threads\n            threads = []\n            for _ in range(num_threads):\n                thread = threading.Thread(target=thread_func)\n                threads.append(thread)\n                thread.start()\n            \n            # Wait for all threads to complete\n            for thread in threads:\n                thread.join()\n            \n            # Verify all operations completed successfully\n            expected_ops = num_threads * ops_per_thread\n            success_rate = success_count / expected_ops\n            print(f\"{cache_name} - Concurrent access success rate: {success_rate:.2%}\")\n            \n            # Success rate should be very high (> 99%)\n            self.assertGreater(success_rate, 0.99)\n\n    def test_memory_usage(self):\n        \"\"\"Test memory usage with large datasets.\"\"\"\n        # Generate a large dataset\n        large_dataset = []\n        for i in range(10000):  # 10,000 unique queries\n            casino = random.choice(self.casino_brands)\n            query_template = random.choice(self.query_types)\n            query = f\"{query_template.format(casino=casino)} {self._random_string(20)}\"\n            result = f\"Detailed response for query {i} about {casino} with {self._random_string(100)}\"\n            large_dataset.append((query, result))\n        \n        # Test different cache implementations\n        cache_configs = [\n            (\"Memory Cache\", self.memory_cache),\n            (\"Redis Cache\", self.redis_cache),\n            (\"Orchestrated Cache\", self.orchestrated_cache)\n        ]\n        \n        for cache_name, cache in cache_configs:\n            # Measure memory before\n            process = psutil.Process()\n            memory_before = process.memory_info().rss / 1024 / 1024  # MB\n            \n            # Populate cache\n            llm_string = str(self.llm)\n            for query, result in large_dataset:\n                cache.update(query, llm_string, result)\n            \n            # Measure memory after\n            memory_after = process.memory_info().rss / 1024 / 1024  # MB\n            memory_used = memory_after - memory_before\n            \n            print(f\"{cache_name} - Memory usage for 10,000 entries: {memory_used:.2f} MB\")\n            \n            # Clear cache for next test\n            if hasattr(cache, 'clear'):\n                cache.clear()\n\n    async def _async_test_helper(self, cache, operations):\n        \"\"\"Helper for async tests.\"\"\"\n        tasks = []\n        for op in operations:\n            if op[0] == 'alookup':\n                tasks.append(cache.alookup(op[1], op[2]))\n            else:  # aupdate\n                tasks.append(cache.aupdate(op[1], op[2], op[3]))\n        \n        return await asyncio.gather(*tasks, return_exceptions=True)\n\n    def test_async_throughput(self):\n        \"\"\"Test async operation throughput.\"\"\"\n        # Skip if not running in async environment\n        if not hasattr(self.redis_cache, 'alookup') or not hasattr(self.redis_cache, 'aupdate'):\n            print(\"Skipping async tests - async methods not available\")\n            return\n        \n        # Test different cache implementations\n        cache_configs = [\n            (\"Redis Cache\", self.redis_cache),\n            (\"Orchestrated Cache\", self.orchestrated_cache)\n        ]\n        \n        for cache_name, cache in cache_configs:\n            # Prepare mixed workload (50% lookups, 50% updates)\n            operations = []\n            for i in range(1000):  # Prepare 1000 operations\n                query_idx = random.randint(0, 999)\n                query = self.test_queries[query_idx]\n                llm_string = str(self.llm)\n                \n                if i % 2 == 0:\n                    # Lookup operation\n                    operations.append(('alookup', query, llm_string))\n                else:\n                    # Update operation\n                    operations.append(('aupdate', query, llm_string, f\"result {i}\"))\n            \n            # Execute operations and measure time\n            start_time = time.time()\n            \n            # Run the async test in the event loop\n            loop = asyncio.get_event_loop()\n            results = loop.run_until_complete(self._async_test_helper(cache, operations))\n            \n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            \n            ops_per_sec = len(operations) / elapsed_time\n            print(f\"{cache_name} - Async throughput: {ops_per_sec:.2f} operations per second\")\n            \n            # Should be > 2000 ops/sec for async operations\n            self.assertGreater(ops_per_sec, 2000)\n\n    def test_semantic_vs_exact_match(self):\n        \"\"\"Compare performance of semantic search vs exact match.\"\"\"\n        # Only test Redis semantic cache\n        if not isinstance(self.redis_cache, RedisSemanticCache):\n            print(\"Skipping semantic comparison - RedisSemanticCache not available\")\n            return\n        \n        # Create variations of the same query\n        base_query = \"What are the bonus terms for 888Casino?\"\n        variations = [\n            \"What bonus conditions does 888Casino have?\",\n            \"Tell me about 888Casino's bonus requirements\",\n            \"888Casino bonus terms and conditions?\",\n            \"What are the requirements for 888Casino bonuses?\",\n            \"Explain 888Casino bonus terms\"\n        ]\n        \n        # Populate cache with base query\n        llm_string = str(self.llm)\n        result = \"The bonus terms for 888Casino include wagering requirements of 30x and a maximum cashout of $500.\"\n        self.redis_cache.update(base_query, llm_string, result)\n        \n        # Test exact match performance\n        exact_start = time.time()\n        for _ in range(100):\n            self.redis_cache.lookup(base_query, llm_string)\n        exact_time = (time.time() - exact_start) * 10  # ms per operation\n        \n        # Test semantic match performance\n        semantic_times = []\n        for variation in variations:\n            start = time.time()\n            for _ in range(20):  # Fewer iterations as semantic is slower\n                self.redis_cache.lookup(variation, llm_string)\n            semantic_times.append((time.time() - start) * 50)  # ms per operation\n        \n        avg_semantic_time = sum(semantic_times) / len(semantic_times)\n        \n        print(f\"Exact match lookup time: {exact_time:.2f}ms per operation\")\n        print(f\"Semantic match lookup time: {avg_semantic_time:.2f}ms per operation\")\n        print(f\"Semantic/Exact ratio: {avg_semantic_time/exact_time:.2f}x\")\n        \n        # Semantic should be within reasonable performance range\n        self.assertLess(avg_semantic_time, 50)  # Should be < 50ms\n\n    def test_ttl_performance(self):\n        \"\"\"Test TTL performance with high-frequency updates.\"\"\"\n        # Create a cache with short TTL\n        ttl_strategy = TTLStrategy(ttl_seconds=1)  # 1 second TTL\n        short_ttl_cache = CachingFactory.create_redis_cache(ttl_strategy=ttl_strategy)\n        \n        # Populate cache\n        llm_string = str(self.llm)\n        query = \"What are today's promotions?\"\n        result = \"Current promotion: 100% deposit match\"\n        \n        # Measure update performance with TTL\n        updates = 1000\n        start_time = time.time()\n        \n        for i in range(updates):\n            short_ttl_cache.update(query, llm_string, f\"Result {i}: {result}\")\n        \n        update_time = time.time() - start_time\n        updates_per_sec = updates / update_time\n        \n        print(f\"TTL cache update rate: {updates_per_sec:.2f} updates per second\")\n        \n        # Should support at least 500 updates per second with TTL\n        self.assertGreater(updates_per_sec, 500)\n        \n        # Test TTL expiration accuracy\n        short_ttl_cache.update(\"expiring_query\", llm_string, \"This should expire\")\n        time.sleep(0.5)  # Wait 0.5 seconds\n        result1 = short_ttl_cache.lookup(\"expiring_query\", llm_string)\n        time.sleep(1.0)  # Wait another 1.0 second (total 1.5s)\n        result2 = short_ttl_cache.lookup(\"expiring_query\", llm_string)\n        \n        self.assertIsNotNone(result1, \"Result should be available before TTL expiration\")\n        self.assertIsNone(result2, \"Result should be None after TTL expiration\")\n\n    def test_distance_threshold_optimization(self):\n        \"\"\"Test cache hit rate with different distance thresholds.\"\"\"\n        # Only test Redis semantic cache\n        if not isinstance(self.redis_cache, RedisSemanticCache):\n            print(\"Skipping threshold test - RedisSemanticCache not available\")\n            return\n        \n        # Create test data with similar queries\n        base_queries = [\n            \"How do I withdraw money from Bet365?\",\n            \"What games are available at PokerStars?\",\n            \"Is 888Casino licensed in the UK?\"\n        ]\n        \n        # Create variations with different similarity levels\n        variations = {}\n        for base in base_queries:\n            variations[base] = [\n                # Very similar (should match with threshold ~0.1)\n                base.replace(\"?\", \"\") + \"?\",\n                # Somewhat similar (should match with threshold ~0.2)\n                base.replace(\"do I\", \"can I\").replace(\"from\", \"at\"),\n                # Less similar (should match with threshold ~0.3)\n                \" \".join(base.split(\" \")[:4]) + \"?\",\n                # Different but related (should match with threshold ~0.5)\n                base.split(\" \")[0] + \" \" + base.split(\" \")[-3] + \" \" + base.split(\" \")[-1],\n                # Very different (should not match with threshold < 0.8)\n                \" \".join(reversed(base.split(\" \")))\n            ]\n        \n        # Test different thresholds\n        thresholds = [0.1, 0.2, 0.3, 0.5, 0.8]\n        llm_string = str(self.llm)\n        \n        results = {}\n        for threshold in thresholds:\n            # Create a new cache with this threshold\n            test_cache = CachingFactory.create_redis_cache(distance_threshold=threshold)\n            \n            # Populate with base queries\n            for base in base_queries:\n                test_cache.update(base, llm_string, f\"Response for {base}\")\n            \n            # Test hit rate\n            total_lookups = 0\n            hits = 0\n            \n            for base in base_queries:\n                # Test base query (should always hit)\n                result = test_cache.lookup(base, llm_string)\n                total_lookups += 1\n                if result is not None:\n                    hits += 1\n                \n                # Test variations\n                for var in variations[base]:\n                    result = test_cache.lookup(var, llm_string)\n                    total_lookups += 1\n                    if result is not None:\n                        hits += 1\n            \n            hit_rate = hits / total_lookups\n            results[threshold] = hit_rate\n            print(f\"Distance threshold {threshold}: {hit_rate:.2%} hit rate\")\n        \n        # Verify that hit rate decreases as threshold gets stricter\n        self.assertGreater(results[0.1], results[0.8], \"Hit rate should decrease with stricter threshold\")\n\n    def test_real_world_simulation(self):\n        \"\"\"Simulate real-world casino query patterns and response times.\"\"\"\n        # Create realistic workload patterns\n        # - 70% read, 30% write\n        # - 80% of reads are for popular queries (20% of query space)\n        # - Response sizes vary from small to large\n        \n        # Generate popular queries (20% of query space)\n        popular_indices = random.sample(range(1000), 200)\n        popular_queries = [self.test_queries[i] for i in popular_indices]\n        \n        # Generate workload\n        workload = []\n        for i in range(5000):  # 5000 operations\n            if random.random() < 0.7:  # 70% reads\n                if random.random() < 0.8:  # 80% of reads are popular\n                    query = random.choice(popular_queries)\n                else:\n                    query = random.choice(self.test_queries)\n                workload.append((\"lookup\", query))\n            else:  # 30% writes\n                query = random.choice(self.test_queries)\n                # Vary response size\n                size = random.choice([\"small\", \"medium\", \"large\"])\n                if size == \"small\":\n                    result = f\"Short answer for {query.split()[0]}\"\n                elif size == \"medium\":\n                    result = f\"Medium length answer with some details about {query}\"\n                else:  # large\n                    result = f\"Very detailed response for {query} with lots of information: \" + self._random_string(500)\n                workload.append((\"update\", query, result))\n        \n        # Test different cache implementations\n        cache_configs = [\n            (\"Memory Cache\", self.memory_cache),\n            (\"Redis Cache\", self.redis_cache),\n            (\"Orchestrated Cache\", self.orchestrated_cache)\n        ]\n        \n        for cache_name, cache in cache_configs:\n            # Pre-populate cache with some data\n            llm_string = str(self.llm)\n            for i in range(100):\n                cache.update(self.test_queries[i], llm_string, f\"Initial result {i}\")\n            \n            # Execute workload and measure time\n            start_time = time.time()\n            \n            for op in workload:\n                if op[0] == \"lookup\":\n                    cache.lookup(op[1], llm_string)\n                else:  # update\n                    cache.update(op[1], llm_string, op[2])\n            \n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            \n            ops_per_sec = len(workload) / elapsed_time\n            print(f\"{cache_name} - Real-world simulation: {ops_per_sec:.2f} operations per second\")\n            \n            # Should maintain good performance under realistic workload\n            self.assertGreater(ops_per_sec, 500)\n\nif __name__ == '__main__':\n    unittest.main()\n```",
        "testStrategy": "Run the performance tests with pytest or unittest. Verify that all tests pass with the following criteria:\n\n1. Cache hit performance: < 10ms for all cache implementations\n2. Cache miss performance: < 50ms for all cache implementations\n3. Throughput: > 1000 ops/sec for all cache implementations\n4. Concurrent access: > 99% success rate for all cache implementations\n5. Memory usage: Monitor and compare memory usage across implementations\n6. Async throughput: > 2000 ops/sec for Redis and orchestrated caches\n7. Semantic vs exact match: Semantic lookup < 50ms per operation\n8. TTL performance: > 500 updates/sec with TTL strategy\n9. Distance threshold optimization: Verify hit rate correlation with threshold settings\n10. Real-world simulation: > 500 ops/sec under realistic workload\n\nTest with both local Redis instance and production-like Redis configuration. Compare performance against LangChain baseline implementations. Document any performance bottlenecks or optimization opportunities.",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Redis test environment",
            "description": "Configure a Redis instance for performance testing, either locally or using a containerized solution.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement basic performance tests",
            "description": "Create tests for cache hit/miss performance, throughput, and concurrent access.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement memory usage profiling",
            "description": "Add tests to measure and compare memory usage across different cache implementations with large datasets.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement async operation tests",
            "description": "Create tests for async throughput using alookup/aupdate operations.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement semantic search performance tests",
            "description": "Add tests comparing semantic search performance against exact match performance.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement TTL performance tests",
            "description": "Create tests for TTL performance with high-frequency updates and expiration accuracy.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement distance threshold optimization tests",
            "description": "Add tests to measure cache hit rates with different semantic distance thresholds.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement real-world simulation tests",
            "description": "Create tests simulating real-world casino query patterns and response times.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Run performance benchmarks and document results",
            "description": "Execute all performance tests and document results, comparing against production requirements and LangChain baseline.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 37,
        "title": "Implement Migration Utilities",
        "description": "Create utilities to facilitate the migration from the old caching system to the new RedisSemanticCache-based multi-cache orchestration system.",
        "status": "cancelled",
        "dependencies": [
          29
        ],
        "priority": "high",
        "details": "Create a new file `migration.py` in the cache directory with the following content:\n\n```python\nfrom typing import Dict, Any, Optional, Callable, TypeVar, cast, List\nimport warnings\nfrom functools import wraps\nimport json\nimport datetime\n\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.globals import set_llm_cache\n\nfrom .casino_cache import CasinoCache\nfrom .cache_factory import CachingFactory\nfrom .redis_semantic_cache import RedisSemanticCache\n\nT = TypeVar('T')\n\ndef deprecated(message: str):\n    \"\"\"Decorator to mark functions as deprecated.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {message}\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\nclass LegacyCacheAdapter:\n    \"\"\"Adapter to provide legacy cache interface using the new RedisSemanticCache-based caching system.\"\"\"\n    \n    def __init__(self, cache: Optional[RedisSemanticCache] = None):\n        \"\"\"Initialize with a RedisSemanticCache instance.\"\"\"\n        self.cache = cache or CachingFactory.create_redis_semantic_cache()\n    \n    @deprecated(\"Use RedisSemanticCache's native methods instead\")\n    def get_from_cache(self, query: str, context: Dict[str, Any] = None) -> Optional[Any]:\n        \"\"\"Legacy-compatible method to get from cache.\"\"\"\n        # Generate a consistent key for the legacy interface\n        return self.cache.lookup(query, \"legacy_cache_adapter\", context)\n    \n    @deprecated(\"Use RedisSemanticCache's native methods instead\")\n    def add_to_cache(self, query: str, result: Any, context: Dict[str, Any] = None) -> None:\n        \"\"\"Legacy-compatible method to add to cache.\"\"\"\n        # Generate a consistent key for the legacy interface\n        self.cache.update(query, \"legacy_cache_adapter\", result, context)\n    \n    @deprecated(\"Use RedisSemanticCache's native methods instead\")\n    def invalidate_cache_entry(self, query: str, context: Dict[str, Any] = None) -> bool:\n        \"\"\"Legacy-compatible method to invalidate cache entry.\"\"\"\n        # Use semantic invalidation if possible\n        return self.cache.invalidate_similar(query, context=context)\n    \n    @deprecated(\"Use RedisSemanticCache's native methods instead\")\n    def clear_all_cache(self) -> None:\n        \"\"\"Legacy-compatible method to clear all cache.\"\"\"\n        self.cache.clear()\n    \n    @deprecated(\"Use RedisSemanticCache's native methods instead\")\n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Legacy-compatible method to get cache stats.\"\"\"\n        # Return stats from the new cache if available\n        return self.cache.get_stats() if hasattr(self.cache, 'get_stats') else {\n            'hits': 0,\n            'misses': 0,\n            'size': 0,\n            'max_size': 0\n        }\n\nclass CacheMigrationTool:\n    \"\"\"Tool for migrating data from old cache system to RedisSemanticCache.\"\"\"\n    \n    def __init__(self, old_cache, new_cache: RedisSemanticCache):\n        self.old_cache = old_cache\n        self.new_cache = new_cache\n        self.migration_log = []\n    \n    def convert_timedelta_to_seconds(self, td):\n        \"\"\"Convert timedelta objects to seconds for Redis compatibility.\"\"\"\n        if isinstance(td, datetime.timedelta):\n            return int(td.total_seconds())\n        return td  # Return as is if not a timedelta\n    \n    def classify_content_type(self, content):\n        \"\"\"Classify the content type of cached data.\"\"\"\n        if isinstance(content, str):\n            return \"text\"\n        elif isinstance(content, dict) or isinstance(content, list):\n            return \"json\"\n        else:\n            return \"binary\"\n    \n    def migrate_entry(self, key, value, metadata=None):\n        \"\"\"Migrate a single cache entry to the new format.\"\"\"\n        try:\n            # Extract query and context from old key format if possible\n            query = key\n            context = {}\n            if metadata and 'context' in metadata:\n                context = metadata['context']\n            \n            # Classify content and prepare for storage\n            content_type = self.classify_content_type(value)\n            \n            # Convert TTL if present\n            ttl = None\n            if metadata and 'ttl' in metadata:\n                ttl = self.convert_timedelta_to_seconds(metadata['ttl'])\n            \n            # Store in new cache\n            self.new_cache.update(\n                query=query,\n                llm_string=\"migrated_entry\",\n                result=value,\n                context=context,\n                ttl=ttl,\n                metadata={\"content_type\": content_type, \"migrated\": True}\n            )\n            \n            self.migration_log.append({\n                \"status\": \"success\",\n                \"key\": key,\n                \"content_type\": content_type\n            })\n            return True\n        except Exception as e:\n            self.migration_log.append({\n                \"status\": \"error\",\n                \"key\": key,\n                \"error\": str(e)\n            })\n            return False\n    \n    def batch_migrate(self, keys, batch_size=100):\n        \"\"\"Migrate data in batches for better performance.\"\"\"\n        results = {\"success\": 0, \"failed\": 0}\n        \n        for i in range(0, len(keys), batch_size):\n            batch = keys[i:i+batch_size]\n            for key in batch:\n                value = self.old_cache.get(key)\n                metadata = self.old_cache.get_metadata(key) if hasattr(self.old_cache, 'get_metadata') else None\n                \n                if self.migrate_entry(key, value, metadata):\n                    results[\"success\"] += 1\n                else:\n                    results[\"failed\"] += 1\n        \n        return results\n    \n    def validate_migration(self, sample_keys=None, sample_size=50):\n        \"\"\"Validate the migration by comparing old and new cache data.\"\"\"\n        if not sample_keys:\n            # Get a sample of keys if not provided\n            all_keys = self.old_cache.get_all_keys() if hasattr(self.old_cache, 'get_all_keys') else []\n            import random\n            sample_keys = random.sample(all_keys, min(sample_size, len(all_keys)))\n        \n        validation_results = []\n        for key in sample_keys:\n            old_value = self.old_cache.get(key)\n            # Attempt to retrieve from new cache\n            new_value = self.new_cache.lookup(key, \"migrated_entry\")\n            \n            match = (old_value == new_value)\n            validation_results.append({\n                \"key\": key,\n                \"match\": match\n            })\n        \n        return {\n            \"total_validated\": len(validation_results),\n            \"successful_matches\": sum(1 for r in validation_results if r[\"match\"]),\n            \"failed_matches\": sum(1 for r in validation_results if not r[\"match\"]),\n            \"details\": validation_results\n        }\n    \n    def rollback_migration(self, keys=None):\n        \"\"\"Roll back migration for specified keys or all migrated keys.\"\"\"\n        if keys is None:\n            # Get all keys with migration metadata\n            keys = self.new_cache.get_keys_with_metadata({\"migrated\": True}) \\\n                   if hasattr(self.new_cache, 'get_keys_with_metadata') else []\n        \n        for key in keys:\n            self.new_cache.invalidate(key)\n        \n        return {\"rolled_back\": len(keys)}\n    \n    def export_migration_log(self, filepath):\n        \"\"\"Export migration log to a file.\"\"\"\n        with open(filepath, 'w') as f:\n            json.dump(self.migration_log, f, indent=2)\n\ndef patch_universal_rag_chain(chain, cache: Optional[RedisSemanticCache] = None):\n    \"\"\"Patch an existing UniversalRAGChain instance to use RedisSemanticCache.\n    \n    Args:\n        chain: The UniversalRAGChain instance to patch\n        cache: The RedisSemanticCache to use or create a new one if None\n        \n    Returns:\n        The patched chain instance\n    \"\"\"\n    # Create cache if not provided\n    if cache is None:\n        cache = CachingFactory.create_redis_semantic_cache()\n    \n    # Set up global LLM cache if using LangChain\n    if hasattr(chain, '_langchain_llm'):\n        set_llm_cache(cache)\n    \n    # Create adapter for legacy methods\n    adapter = LegacyCacheAdapter(cache)\n    \n    # Patch chain methods with deprecated warnings\n    chain.get_from_cache = adapter.get_from_cache\n    chain.add_to_cache = adapter.add_to_cache\n    chain.invalidate_cache_entry = adapter.invalidate_cache_entry\n    chain.clear_all_cache = adapter.clear_all_cache\n    chain.get_cache_stats = adapter.get_cache_stats\n    \n    # Store cache on chain for future reference\n    chain._semantic_cache = cache\n    \n    return chain\n\ndef analyze_cache_performance(old_cache, new_cache, test_queries: List[str]):\n    \"\"\"Compare performance between old and new cache implementations.\"\"\"\n    results = {\n        \"old_cache\": {\"hits\": 0, \"misses\": 0, \"latency\": []},\n        \"new_cache\": {\"hits\": 0, \"misses\": 0, \"latency\": []}\n    }\n    \n    for query in test_queries:\n        # Test old cache\n        import time\n        start = time.time()\n        old_result = old_cache.get_from_cache(query) if hasattr(old_cache, 'get_from_cache') else None\n        old_latency = time.time() - start\n        \n        # Test new cache\n        start = time.time()\n        new_result = new_cache.lookup(query, \"performance_test\")\n        new_latency = time.time() - start\n        \n        # Record results\n        if old_result is not None:\n            results[\"old_cache\"][\"hits\"] += 1\n        else:\n            results[\"old_cache\"][\"misses\"] += 1\n        results[\"old_cache\"][\"latency\"].append(old_latency)\n        \n        if new_result is not None:\n            results[\"new_cache\"][\"hits\"] += 1\n        else:\n            results[\"new_cache\"][\"misses\"] += 1\n        results[\"new_cache\"][\"latency\"].append(new_latency)\n    \n    # Calculate averages\n    results[\"old_cache\"][\"avg_latency\"] = sum(results[\"old_cache\"][\"latency\"]) / len(test_queries)\n    results[\"new_cache\"][\"avg_latency\"] = sum(results[\"new_cache\"][\"latency\"]) / len(test_queries)\n    \n    return results\n```",
        "testStrategy": "Write unit tests that verify the migration utilities correctly bridge between the old caching system and the new RedisSemanticCache-based system. Tests should cover:\n\n1. Test that deprecated warnings are issued when using legacy methods through the LegacyCacheAdapter\n2. Verify that the patch_universal_rag_chain function correctly patches an existing chain instance to use RedisSemanticCache\n3. Test the CacheMigrationTool's ability to:\n   - Convert timedelta objects to seconds for Redis compatibility\n   - Correctly classify content types of cached data\n   - Successfully migrate individual entries and batches\n   - Validate migration accuracy\n   - Roll back migrations when needed\n4. Test the performance analysis tool with mock caches\n5. Integration tests that verify a complete migration workflow:\n   - Create test data in old cache format\n   - Migrate to new RedisSemanticCache format\n   - Validate all data was correctly transferred\n   - Test queries against both systems produce identical results\n   - Verify rollback functionality restores system to pre-migration state\n\nUse mocking to simulate Redis connections and large datasets for performance testing.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement LegacyCacheAdapter class",
            "description": "Create an adapter class that provides backward compatibility with the old caching interface while using the new RedisSemanticCache under the hood.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CacheMigrationTool class",
            "description": "Create a tool for migrating data from the old cache system to RedisSemanticCache, including data mapping, TTL conversion, and content-type classification.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement batch migration functionality",
            "description": "Add batch processing capabilities to the migration tool to optimize performance during large-scale migrations.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement migration validation tools",
            "description": "Create utilities to validate data integrity during and after migration, ensuring all data is correctly transferred.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement rollback functionality",
            "description": "Create utilities to safely roll back migrations if issues are detected, preserving the original data.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement UniversalRAGChain patching",
            "description": "Create a function to patch existing UniversalRAGChain instances to use the new RedisSemanticCache system.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement cache performance analysis tools",
            "description": "Create utilities to compare performance metrics between old and new cache implementations during migration.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Write comprehensive unit tests",
            "description": "Create tests for all migration utilities, including adapter functionality, data conversion, validation, and rollback capabilities.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Create integration tests",
            "description": "Develop tests that verify the complete migration workflow from old cache to RedisSemanticCache with real-world data patterns.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 38,
        "title": "Update Universal RAG Chain to Use LangChain Native Caching",
        "description": "Modify the existing universal_rag_lcel.py file to use LangChain's native caching infrastructure with RedisSemanticCache for simpler and more efficient caching.",
        "status": "done",
        "dependencies": [
          29,
          30,
          37
        ],
        "priority": "high",
        "details": "Update the universal_rag_lcel.py file to integrate with LangChain's native caching using the built-in RedisSemanticCache:\n\n1. Import the necessary cache components:\n```python\nfrom langchain_core.globals import set_llm_cache\nfrom langchain_community.cache import RedisSemanticCache\nfrom langchain_openai import OpenAIEmbeddings\nimport os\nimport warnings\n```\n\n2. Remove the old QueryAwareCache class and related complex methods\n\n3. Update the UniversalRAGChain class to use LangChain's native caching with a simple approach:\n```python\nclass UniversalRAGChain:\n    def __init__(self, config=None):\n        # ... existing initialization code ...\n        \n        # Set up global LLM cache with RedisSemanticCache\n        embedding_model = OpenAIEmbeddings()\n        redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n        \n        # Configure cache with content type support\n        content_type = config.get('content_type', 'default')\n        namespace = f\"casino:{content_type}\"\n        \n        # Set up the cache with appropriate TTL (in seconds)\n        self.cache_ttl = config.get('cache_ttl', 3600)  # Default 1 hour\n        \n        # Initialize the semantic cache\n        semantic_cache = RedisSemanticCache(\n            redis_url=redis_url,\n            embedding=embedding_model,\n            namespace=namespace,\n            ttl=self.cache_ttl,\n            similarity_threshold=0.85  # Configurable threshold\n        )\n        \n        # Set the global LLM cache\n        set_llm_cache(semantic_cache)\n        \n        # Build the chain\n        self._build_chain()\n        \n        # Initialize simple analytics tracking\n        self.cache_analytics = {\n            'hits': 0,\n            'misses': 0\n        }\n    \n    def _build_chain(self):\n        # ... existing chain building code ...\n        \n        # For LCEL chains, apply .with_cache() pattern\n        if hasattr(self, 'lcel_chain'):\n            # The cache is already set globally, but we can also apply it directly\n            self.lcel_chain = self.lcel_chain.with_cache()\n    \n    # Legacy cache methods with deprecation warnings\n    def get_from_cache(self, query, context=None):\n        warnings.warn(\n            \"get_from_cache is deprecated, LangChain's native caching is now used automatically\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        return None  # Let LangChain handle caching automatically\n    \n    def add_to_cache(self, query, result, context=None):\n        warnings.warn(\n            \"add_to_cache is deprecated, LangChain's native caching is now used automatically\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        # No action needed - LangChain handles caching automatically\n    \n    def invalidate_cache_entry(self, query, context=None):\n        warnings.warn(\n            \"invalidate_cache_entry is deprecated, use LangChain's native caching instead\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        return False  # Not directly supported in simplified approach\n    \n    def clear_all_cache(self):\n        warnings.warn(\n            \"clear_all_cache is deprecated, use LangChain's native caching instead\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        # Not directly supported in simplified approach\n    \n    def get_cache_stats(self):\n        warnings.warn(\n            \"get_cache_stats is deprecated, LangChain handles caching internally\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        return self.cache_analytics\n```\n\n4. Add a migration note at the top of the file:\n```python\n# NOTE: This file has been updated to use LangChain's native caching infrastructure\n# with RedisSemanticCache for simple and efficient caching.\n# The old caching methods are deprecated and will be removed in a future version.\n# LangChain now handles caching automatically with set_llm_cache().\n```\n\n5. Add content-type specific configuration:\n```python\ndef configure_cache_for_content_type(self, content_type, similarity_threshold=None):\n    \"\"\"Configure cache specifically for a content type\"\"\"\n    embedding_model = OpenAIEmbeddings()\n    redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n    namespace = f\"casino:{content_type}\"\n    \n    # Use provided threshold or default\n    threshold = similarity_threshold or 0.85\n    \n    # Initialize the semantic cache with content-specific settings\n    semantic_cache = RedisSemanticCache(\n        redis_url=redis_url,\n        embedding=embedding_model,\n        namespace=namespace,\n        ttl=self.cache_ttl,\n        similarity_threshold=threshold\n    )\n    \n    # Set the global LLM cache\n    set_llm_cache(semantic_cache)\n    return self\n```",
        "testStrategy": "Write integration tests that verify the updated UniversalRAGChain correctly uses LangChain's native caching with RedisSemanticCache:\n\n1. Test that set_llm_cache() correctly configures the global LLM cache\n2. Verify that identical queries are properly cached and return the same result\n3. Test that similar queries (above the similarity threshold) are correctly matched\n4. Verify that different content types use different cache namespaces\n5. Test that TTL settings correctly expire cache entries after the specified time\n6. Verify that the .with_cache() pattern works for LCEL chains\n7. Test backward compatibility with existing code that might call the deprecated methods\n8. Verify that different similarity thresholds work as expected for different content types\n\nUse mock Redis instances for testing to avoid external dependencies. Include both unit tests and integration tests to ensure the caching works correctly with the actual LLM calls.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement RedisSemanticCache integration",
            "description": "Integrate the UniversalRAGChain with RedisSemanticCache using LangChain's set_llm_cache() function.",
            "status": "done",
            "dependencies": [],
            "details": "- Import RedisSemanticCache and set_llm_cache\n- Configure RedisSemanticCache with appropriate parameters\n- Call set_llm_cache() in the UniversalRAGChain initialization\n- Remove complex caching code and replace with the simpler approach",
            "testStrategy": "Test that the cache is properly initialized and that set_llm_cache() is called with the correct parameters."
          },
          {
            "id": 2,
            "title": "Add content-type specific cache configurations",
            "description": "Implement content-type specific cache configurations with appropriate namespaces.",
            "status": "done",
            "dependencies": [],
            "details": "- Create a configure_cache_for_content_type method\n- Use namespaces to separate caches for different content types\n- Allow configuration of similarity thresholds per content type\n- Ensure TTL settings are properly applied",
            "testStrategy": "Test that different content types use different namespaces and that configuration parameters are correctly applied."
          },
          {
            "id": 3,
            "title": "Update LCEL chain integration",
            "description": "Update LCEL chains to use the .with_cache() pattern for direct cache integration.",
            "status": "done",
            "dependencies": [],
            "details": "- Apply .with_cache() to LCEL chains in _build_chain()\n- Ensure the cache is properly connected to the chain\n- Test that caching works with LCEL chains",
            "testStrategy": "Verify that LCEL chains correctly use the cache and that results are properly cached."
          },
          {
            "id": 4,
            "title": "Add backward compatibility",
            "description": "Add deprecation warnings and minimal implementations for backward compatibility.",
            "status": "done",
            "dependencies": [],
            "details": "- Add deprecation warnings to old cache methods\n- Provide minimal implementations that don't interfere with the new caching system\n- Ensure old code continues to work with appropriate warnings",
            "testStrategy": "Test that old code using deprecated methods continues to work and that appropriate warnings are displayed."
          },
          {
            "id": 5,
            "title": "Write comprehensive tests",
            "description": "Create unit and integration tests for the simplified caching functionality.",
            "status": "done",
            "dependencies": [],
            "details": "- Test identical query caching\n- Test similar query matching with different thresholds\n- Test TTL expiration\n- Test content-type specific caching\n- Use mock Redis instances for testing",
            "testStrategy": "Create a comprehensive test suite that verifies all aspects of the simplified caching system."
          }
        ]
      },
      {
        "id": 39,
        "title": "Create Simple RedisSemanticCache Example",
        "description": "Create a simple example file demonstrating how to use LangChain's native caching with RedisSemanticCache in just 3 lines of code, focusing on set_llm_cache() and .with_cache() usage patterns.",
        "details": "Create a new file `simple_redis_cache_example.py` in the examples directory with the following content:\n\n```python\nfrom langchain_core.globals import set_llm_cache\nfrom langchain_redis import RedisSemanticCache\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.embeddings import OpenAIEmbeddings\n\n# Example 1: Global LLM cache setup (1 line)\nset_llm_cache(RedisSemanticCache(redis_url=\"redis://localhost:6379\", embedding=OpenAIEmbeddings()))\n\n# Example 2: Model-specific cache with .with_cache() (1 line)\nllm = ChatOpenAI().with_cache(RedisSemanticCache(redis_url=\"redis://localhost:6379\", embedding=OpenAIEmbeddings()))\n\n# Example 3: Simple usage demonstration\ndef demonstrate_caching():\n    # First call - will hit the API and cache the result\n    print(\"First call (cache miss):\")\n    start = time.time()\n    result1 = llm.invoke(\"What is the capital of France?\")\n    print(f\"Time taken: {time.time() - start:.2f}s\")\n    print(f\"Result: {result1.content}\\n\")\n    \n    # Second call - should be retrieved from cache\n    print(\"Second call (cache hit):\")\n    start = time.time()\n    result2 = llm.invoke(\"What is the capital of France?\")\n    print(f\"Time taken: {time.time() - start:.2f}s\")\n    print(f\"Result: {result2.content}\")\n\nif __name__ == \"__main__\":\n    import time\n    demonstrate_caching()\n```\n\nThe example should be minimal and focused on demonstrating the core caching functionality without any custom orchestrators, factories, or middleware. It should clearly show:\n\n1. How to set up a global LLM cache with RedisSemanticCache in a single line\n2. How to apply caching to a specific LLM instance using .with_cache() in a single line\n3. A simple demonstration of the caching in action showing the performance difference between a cache miss and a cache hit\n\nMake sure the example is self-contained and can be run with minimal dependencies (just LangChain, Redis, and OpenAI).",
        "testStrategy": "1. Verify the example runs correctly:\n   - Install required dependencies: `pip install langchain langchain-redis langchain-openai redis`\n   - Ensure Redis is running locally (or update the redis_url to point to your Redis instance)\n   - Run the example: `python simple_redis_cache_example.py`\n   - Confirm the second call is significantly faster than the first call\n\n2. Validate caching behavior:\n   - Check Redis to confirm cache entries are being created:\n     ```\n     redis-cli\n     KEYS *\n     ```\n   - Verify the cache is being used by examining the time difference between calls\n   - Modify the prompt slightly and verify it creates a new cache entry\n\n3. Test with different LLM providers:\n   - Modify the example to use a different LLM provider (e.g., Anthropic)\n   - Verify caching works consistently across different providers\n\n4. Confirm the example meets the requirements:\n   - Ensure the core caching setup is demonstrated in just 3 lines of code\n   - Verify both set_llm_cache() and .with_cache() patterns are shown\n   - Check that no custom orchestrators, factories, or middleware are used",
        "status": "done",
        "dependencies": [
          24
        ],
        "priority": "low",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-16T20:43:05.251Z",
      "updated": "2025-06-27T05:48:47.383Z",
      "description": "Tasks for master context"
    }
  }
}